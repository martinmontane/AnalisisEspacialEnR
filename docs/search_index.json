[
["index.html", "Herramientas de análisis espacial en R Herramientas de análisis espacial en R ¿Qué necesitamos para arrancar?", " Herramientas de análisis espacial en R Martin Montane 2020-07-01 Herramientas de análisis espacial en R Los datos espaciales requieren un tratamiento particular tanto en su representación, su almacenamiento, sus transformaciones, sus visualizaciones y su análisis. En este compendio de notas de clase se introducen las herramientas necesarias para comenzar a aprovechar todas las oportunidades que los datos espaciales nos brindan. ¿Qué necesitamos para arrancar? En estas notas de clase se usa el lenguaje de programación R. Recomiendo utilizar RStudio para que nos ayude con los proyectos y la edición de los códigos en R Descargar instalar estos dos softwares es muy simple ya que son gratuitos. R de hecho es un lenguaje de programación open source, o de código abierto, lo que significa que cualquiera puede colaborar. Haciendo click aquí van a poder descargar la última versión de R para Windows, Mac o Linux. Una vez que lo hayan descargado solo tienen que instalarlo. Ahora descarguen RStudio, también van a poder elegir la versión que corresponde según su sistema operativo. RStudio va a identificar automáticamente la versión de R que ya tienen instalada, por lo que es importante que instalen RStudio luego de haber instalado R. Una vez que tienen todo esto instalado pueden pasar al primer capítulo de este libro "],
["datos-espaciales-en-r.html", "1 Datos espaciales en R 1.1 ¿Qué es un dato espacial? 1.2 ¿Dónde estamos en la Tierra? 1.3 Coordinate Reference Systems 1.4 Manos a la obra ¿Dónde construir el próximo centro de salud? 1.5 Ejercicio", " 1 Datos espaciales en R Al terminar este capítulo ustedes van a poder: - Comprender por qué los datos espaciales son distintos al resto de los datos - Las dificultades de la representación de esos datos y los estándares utilizados - Trabajar con datos espaciales en R: su importación, manipulación e introducción a los gráficos - Identificar los principales tipos de archivos donde suelen compartirse estos datos 1.1 ¿Qué es un dato espacial? Un dato espacial o georreferenciado tiene una característica que lo hace único: posee información sobre su ubicación en la Tierra. No es el único tipo de dato que tiene particularidades, por ejemplo las series de tiempo tienen información sobre un específico período de tiempo donde se registró la información. Esto trae importantes consideraciones al momento de realizar el análisis estadístico, lo que generó el desarrollo de toda una rama de la estadística. No obstante, los datos espaciales no presentan un desafío solo al momento de su análisis, sino que presentan específicidades en la forma de representar su información geográfica y realizar transformaciones en los datos. 1.2 ¿Dónde estamos en la Tierra? La respuesta a esta pregunta puede ser un poco más compleja de lo que uno piensa, al menos si desea realizar un análisis con esta información. La respuesta más fácil en este momento sería decir: Av. Figueroa Alcorta 7350, Ciudad de Buenos Aires, Argentina. Bien, es un primer paso. Ahora: ¿Cómo calculamos la distancia con respecto a Abbey Road 2, Londres, Inglaterra, donde se encuentra el famoso cruce peatonal de la tapa del disco de los Beatles, Abbey Road? Imposible saberlo solo con esa información. Si nosotros introdujéramos esos datos en un GPS (o Google Maps), lo que haría es traducir las direcciones que les pasamos a un sistema de grillas que divide al globo en celdas en base a líneas imaginarias en sentido paralelo a los polos (paralelos) y perpendicular a ellos (meridianos). Nuestra dirección quedaría transformada directamente en un vector con dos posiciones: latitud y longitud. Ahora “Av. Figueroa Alcorta 7350, Ciudad de Buenos Aires, Argentina” se convirtió en (-34.714656,-58.785999) y “Abbey Road 2, Londres, Inglaterra” en (51.532068, -0.177305). Las latitudes y longitudes se expresan en grados, así que ya podemos establecer una diferencia cuantitativa entre nuestras dos posiciones ¡Incluso podemos expresarlo en una medida de distancia como metros o kilómetros! Para esta clase vamos a necesitar varios paquetes, así que los cargamos. Recordemos que si no están instalados hay que usar la función install.packages() library(tidyverse) # Paquete multiuso library(sf) # Paquete clave para manipular datos espaciales library(leaflet) # Uno de los paquetes para Una vez que los cargamos, vamos a crear nuestro dataframe con datos espaciales en base a las coordenadas latitud y longitud que definimos anteriormente: # Creamos un Data Frame con los datos necesarios datos &lt;- data.frame(lat = c(-34.714656, 51.532068), long = c(-58.785999, -0.177305), ubicacion = c(&quot;UTDT&quot;, &quot;Abbey Road&quot;)) # Lo convertimos a un objeto sf puntosEspaciales &lt;- st_as_sf(datos, coords = c(&quot;long&quot;, &quot;lat&quot;), crs = 4326) # st_distance() nos permite encontrar la diferencia en la # unidad que diga el CRS (sistema de coordenadas de # referencia) st_distance(puntosEspaciales) # En metros ## Units: [m] ## [,1] [,2] ## [1,] 0 11131513 ## [2,] 11131513 0 st_distance(puntosEspaciales)/1000 # En kilómetros ## Units: [m] ## [,1] [,2] ## [1,] 0.00 11131.51 ## [2,] 11131.51 0.00 Según estos cálculos, nos separan aproximadamente 11.131 kms de Abbey Road. Perfecto, pudimos definir nuestra ubicación en la tierra e incluso medir la distancia con otro punto. Hay un parámetro que todavía no introdujimos y que resulta clave cuando lidiamos con datos espaciales: CRS, las siglas de Coordinate Reference System. Incluso podemos hacer nuestro primer gráfico interactivo de una manera muy rápida leaflet(puntosEspaciales) %&gt;% addTiles() %&gt;% addMarkers() Vayamos paso por paso. En la siguiente sección veremos los distintos modelos de la tierra que usamos para poder representar estas ubicaciones espaciales. 1.3 Coordinate Reference Systems 1.3.1 Elipsoides, sistemas de coordenadas y datums Representar una ubicación en la superficie de la tierra implica superar diversos obstáculos. Para empezar, la tierra no es una esfera: tiene una forma que suele modelarse como geoide, pero incluso eso es una aproximación. La tierra tiene una forma particular, con diversos accidentes geográficos que la hacen única (y difícil de manipular matemáticamente). Sin embargo, nosotros - y a fines prácticos, todas las personas que trabajan con datos georreferenciados - trabajamos en su versión como geoide, y es en relación a esta modelización de la tierra que se montan los CRS. Definido el geoide, ese modelo de la forma de la tierra, introducimos el primer componente de los CRS: el elipsoide. El elipsoide es una aproximación al geoide con mejores propiedades matemáticas. Para definir un elipsoide necesitamos un par de parámetros que definen su forma. Una vez que contamos con un elipsoide podemos establecer un sistema de grillas tridimensional, como el de latitud y longitud, que lo segmenta según los ángulos que se forman entre la línea imaginaria paralela a los polos (paralelos) y la línea imaginaria perpendicular a los polos (meridiano) en un determinado punto, en relación al paralelo y meridiano de origen. Pero ¿cómo relacionamos al elipsoide con el geoide? Si bien el primero es una aproximación del segundo, para establecer un CRS necesitamos saber como se relacionan entre ellos: tenemos que “fijar” el elipsoide al geoide. Esto es lo que hace el datum: define el origen y la orientación de los ejes de coordenadas. Piensen en el datum como la información necesaria para “dibujar” el sistema de coordenadas en el elipsoide Entonces ya tenemos tres elementos que poseen los CRS: Un elipsoide (un modelo de la tierra, en rigor de un geoide) Un sistema de coordenadas, que nos permite determinar la posición de un punto en relación a otro en base a líneas imaginarias Un datum, que nos permite dibujar ese sistema de coordenadas en el elipsoide de tal manera que represente al ubicaciones específicas en el geoide Si no quedó del todo claro no se preocupen: es un tema complejo que, en la mayoría de los casos, solo basta con saber que estos conceptos existen y qué significan. El objetivo de esta subsección es dar la definición básica de cada elemento porque probablemente se encuentren con esta información en diversos lugares, pero a fines prácticos suele utilizarse siempre el mismo elipsoide, datum y sistema de coordenadas, o variaciones que no tienen grandes efectos a los fines prácticos de nuestros trabajos. El World Geodetic System (WGS84) es un standard en la industria a nivel mundial, y existen algunas variaciones locales (la más famosa, el North American Datum (NAD83)) que no nos traerán mayores problemas al momento de las transformaciones. Piensen en el CRS como las unidades de peso o de distancia: cada observación que veamos de datos espaciales corresponde a un determinado CRS y no corresponde hacer operaciones entre dos observaciones pertenecientes a distintos CRS. 1.3.2 Proyecciones Hasta ahora hemos trabajado en la representación de la Tierra en tres dimensiones. Sin embargo, todos los mapas con los que hemos trabajado desde chicos tienen dos dimensiones ¿Cómo transformamos un objeto de tres dimensiones a uno de dos dimensiones? Debemos realizar proyecciones de ese objeto tridimensional que, como veremos en breve, involucra diversos tradeoffs[^1]. Piensen en la proyección como una tarea de traducción: algo se pierde en el proceso. La proyección hoy en día más famosa es MERCATOR, la proyección que usa, entre otros servicios, Google Maps. Diseñada hace ya varios siglos para la navegación marítima, esta transformación es relativamente buena en lo relativo preservar formas y útil para navegar. En lo que realmente falla este tipo de proyección es en definir el tamaño de las unidades geógraficas: los países que están cerca de los polos aparentan tener un tamaño mucho más grande del que realmente tienen, mientras que lo inverso sucede con los que están cerca de la línea del ecuador. Tal es así que existe una página web (https://thetruesize.com/) que permite experimentar de manera interactiva con los tamaños de los países en diversas partes de la proyección. En la Figura 1 muestro un ejemplo: Groenlandia, Islandia, Noruega, Suecia, Finlandia y Reino Unido combinadas ocupan aproximadamente el 50% de Brasil (Figura 1). Figure 1.1: La proyección MERCATOR distorsiona nuestra percepción de los tamaños La oferta de proyecciones es prácticamente ilimitada. El paquete mapproj en R nos permite transformar el mundo en base a diversas proyecciones, incluyendo algunas que preservan el tamaño de los países. La Figura 2 muestra el mundo desde otra perspectiva: los países del norte son más chicos de lo que parecen en la proyección mercator. Figure 1.2: La proyección MOLLWEIDE mantiene la representación de los tamaños Las proyecciones también forman parte de los CRS, que pueden o no tener una proyección. Sea como sea, lo importante de esta sección es haberlos convencido de que importa conocer en que CRS están expresados los datos espaciales. Las transformaciones entre CRS no hace falta conocerlas, sino que el paquete sf lo hará por nosotros. Insisto: lo importante es saber que los datos espaciales SIEMPRE tienen un CRS, aun si no está definido explícitamente en nuestro archivo. Volvamos al ejemplo de los inmuebles de las propiedades de la introducción de este libro para ver un qué formato de archivos tienen los datos espaciales y un ejemplo sobre transformación de CRS. 1.4 Manos a la obra ¿Dónde construir el próximo centro de salud? Mostremos algunas de las funciones de datos espaciales de R con un problema muy concreto. Pónganse en la piel de una funcionara pública que debe decidir en qué manzana específica de la Ciudad de Buenos Aires debe abrir un nuevo centro de salud. Existen múltiples maneras de lidiar con este problema, pero supongamos por un segundo que esta funcionaria sabe como trabajar con GIS y, específicamente, con R. Antes de escuchar las demandas de los habitantes, prefiere conocer, en base a los datos, en qué manzanas hay una mayor necesidad de construir un nuevo centro de salud. Para eso, va a utilizar distintos conjuntos de datos y herramientas de R. El objetivo va a ser generar un Índice de Demanda de Salud (IDS) que, para cada manzana de la Ciudad de Buenos Aires, nos va a indicar qué tan necesaria es la construcción de un centro de salud. Este indicador se va a basar en distintas variables, a saber: La densidad poblacional en la zona La cantidad de hogares con Necesidades Básicas Insatisfechas (NBI) La atención preexistente por el sistema de salud La distancia con la avenida más cercana Veamos de dónde podemos conseguir estos datos y cómo podemos leerlos en R 1.4.1 Cargando los datos espaciales Los datos espaciales, a diferencia de otros tipos de datos, tienen formatos específicos para su almacenamiento. No es el objetivo de este libro introducir a todos los formatos, que son efectivamente muchos. Vamos a leer datos de tres formatos distintos: Geojson: Se trata de una forma de representación de datos como listas, siguiendo el formato de json, pero adaptado para almacenar datos espaciales. Por default, el sistema de coordenada de referencias es el EPSG número 4325, que utiliza el WGS 84, el standard más utilizado en el mundo. El hecho de que lo guarde por default con este CRS es muy importante: no hace falta información de contexto sobre en qué CRS está, porque solo puede estar en ese- Shapefiles: Los shapefiles son un formato antiguo para almacenar datos espaciales, y es propiedad de la empresa ESRI (los creadores de ArcGIS). Los shapefiles están siempre compuestos múltiples archivos, cada uno cumpliendo una función. Los datos específicos de las coordenadas se encuentran en un archivo .shp, pero la proyección se encuentra en .proj, y no siempre viene incluida en nuestros datos. csv: los famosos archivos separados por comas pueden tener información espacial dentro dellos, particularmente cunado se trata de puntos. Suelen especificarse coordenadas como columnas “X” e “Y” o “lat” y “lon” El GCBA ofrece un dataset espacial con todas las vías de circulación de autos en la Ciudad de Buenos Aires (en este link)[https://data.buenosaires.gob.ar/dataset/calles]. Les va a dar la opción de descargar los datos en shapefiles o geojson. Descarguen el shapefile (estará comprimido en un .zip) y extraiganlo en la carpeta del proyecto. Para leer a este y al resto de los archivos espaciales - con excepción de csv - podemos usar read_sf(). Acá estoy suponiendo que tienen a todos los archivos de este shapefile dentro de la carpeta callejero-rar/ # Cargamos la librería de SF library(sf) calles &lt;- read_sf(&quot;callejero-rar/callejero.shp&quot;) Si lograron hacerlo, entonces deberían tener un objeto que se llama calles. Lo primero que tenemos que entender cuando leemos datos espaciales es el sistema de coordenadas de referencia en el que están representados. Esto podemos hacerlo con st_crs() st_crs(calles) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] En este caso nuestro dataset de calles está expresado en el EPSG 4326 ¿Qué significa esto? El EPSG es un sistema con índices de sistema de coordenadas de referencia. Cada uno de estos números representa a cada combinación de parámetros que determinan un sistema de coordenadas de referencia. Pueden buscar estos números en https://spatialreference.org/. Aquí pueden ver específicamente información sobre el 4326. Ahora carguemos información sobre la población y la cantidad de hogares con necesidades básicas insatisfechas - una forma de medir la pobreza - por radio censal según el censo 2010, siendo los radios censales la mínima unidad de medida del censo. Nuevamente vamos a poder descargar estos datos desde aquí. Esta vez descarguen la opción de geojson y, nuevamente, guárdenlo en la carpeta de su proyecto radiosCensales &lt;- read_sf(&quot;caba_radios_censales.geojson&quot;) En este caso no necesitamos saber cuál el es el sistema de coordenada de referencias… siempre será el WSG84. Veamos qué variables trae glimpse(radiosCensales) ## Rows: 3,554 ## Columns: 9 ## $ RADIO_ID &lt;chr&gt; &quot;1_1_1&quot;, &quot;1_12_1&quot;, &quot;1_12_10&quot;, &quot;1_12_11&quot;, &quot;1_12_2&quot;, &quot;1_12_3&quot;, &quot;1_12_4&quot;, &quot;1_12_5&quot;, &quot;1_12_6&quot;, &quot;1_12_7&quot;, &quot;1_12_... ## $ BARRIO &lt;chr&gt; &quot;RETIRO&quot;, &quot;SAN NICOLAS&quot;, &quot;SAN NICOLAS&quot;, &quot;SAN NICOLAS&quot;, &quot;SAN NICOLAS&quot;, &quot;SAN NICOLAS&quot;, &quot;SAN NICOLAS&quot;, &quot;SAN NI... ## $ COMUNA &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1... ## $ POBLACION &lt;dbl&gt; 336, 341, 296, 528, 229, 723, 393, 600, 472, 786, 329, 1356, 586, 725, 358, 426, 614, 292, 622, 431, 522, 5... ## $ VIVIENDAS &lt;dbl&gt; 82, 365, 629, 375, 445, 744, 341, 505, 504, 546, 275, 895, 404, 641, 266, 252, 641, 382, 538, 341, 412, 309... ## $ HOGARES &lt;dbl&gt; 65, 116, 101, 136, 129, 314, 209, 275, 202, 347, 129, 342, 163, 136, 104, 157, 235, 142, 247, 220, 253, 217... ## $ HOGARES_NBI &lt;dbl&gt; 19, 25, 1, 7, 16, 104, 110, 32, 49, 89, 15, 57, 1, 1, 2, 2, 5, 1, 74, 3, 31, 43, 74, 48, 0, 123, 81, 1, 111... ## $ AREA_KM2 &lt;dbl&gt; 1.79899705, 0.01856469, 0.04438025, 0.36634000, 0.01836301, 0.03672540, 0.01671179, 0.03332892, 0.03546841,... ## $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-58.37189 -..., MULTIPOLYGON (((-58.38593 -..., MULTIPOLYGON (((-58.37879 -...... ¿Ven esa columna geometry? Es donde están almacenados nuestros datos espaciales, en este caso son polígonos. Es una forma muy prolija de guardar la información espacial, al mismo tiempo que el resto de las variables pueden trabajarse como si fuera un data.frame normal. Si están atentos y atentas se van a dar cuenta de que no tenemos una medida de densidad… todavía, ya lo vamos a solucionar. Finalmente, nos falta información sobre la oferta actual de cobertura de salud, para esto vamos a usar solamente a los hospitales de la Ciudad de Buenos Aires, siendo conscientes de que existen otras formas de brindar atención de salud, como los Centros de Salud y Acción Comunitaria (CeSACs). El dataset de hospitales se encuentra en un csv al que le hice pequeñas modificaciones y pueden descargar haciendo click acá hospitales &lt;- read_delim(&quot;Hospitales.csv&quot;,delim = &quot;;&quot;) Todavía esto no es un dato espacial… al menos no para R: para eso tenemos que convertirlo en un objeto SF, que tenga a las coordenadas en la columna geometry. Veamos qué columnas tiene glimpse(hospitales) ## Rows: 36 ## Columns: 33 ## $ long &lt;dbl&gt; -58.37755, -58.41207, -58.40273, -58.38516, -58.39131, -58.43494, -58.37584, -58.38233, -58.382... ## $ lat &lt;dbl&gt; -34.62885, -34.59419, -34.58453, -34.63940, -34.63415, -34.60847, -34.63021, -34.63570, -34.636... ## $ nombre &lt;chr&gt; &quot;HOSPITAL GENERAL DE NIÑOS PEDRO DE ELIZALDE&quot;, &quot;HOSPITAL GENERAL DE NIÑOS RICARDO GUTIERREZ&quot;, &quot;... ## $ nom_map &lt;chr&gt; &quot;HOSP. DE ELIZALDE&quot;, &quot;HOSP. GUTIERREZ&quot;, &quot;HOSP. ODONTOLOGICO CARRILLO&quot;, &quot;HOSP. MOYANO&quot;, &quot;HOSP. U... ## $ objeto &lt;chr&gt; &quot;HOSPITAL&quot;, &quot;HOSPITAL&quot;, &quot;HOSPITAL&quot;, &quot;HOSPITAL&quot;, &quot;HOSPITAL&quot;, &quot;HOSPITAL&quot;, &quot;HOSPITAL&quot;, &quot;HOSPITAL&quot;,... ## $ calle_nombre &lt;chr&gt; &quot;MANUEL A. MONTES DE OCA&quot;, &quot;GALLO&quot;, &quot;SANCHEZ DE BUSTAMANTE&quot;, &quot;BRANDSEN&quot;, &quot;CASEROS&quot;, &quot;DIAZ VELEZ... ## $ calle_altura &lt;dbl&gt; 40, 1330, 2529, 2570, 2061, 4821, 849, 315, 375, 1795, 955, 2151, 4151, 15, 150, 369, 2021, 194... ## $ calle_altura_1 &lt;chr&gt; &quot;MANUEL A. MONTES DE OCA 40&quot;, &quot;GALLO 1330&quot;, &quot;SANCHEZ DE BUSTAMANTE 2529&quot;, &quot;BRANDSEN 2570&quot;, &quot;CAS... ## $ dom_geo &lt;chr&gt; &quot;40 MONTES DE OCA, MANUEL AV.&quot;, &quot;1330 GALLO&quot;, &quot;2529 SANCHEZ DE BUSTAMANTE&quot;, &quot;2570 BRANDSEN&quot;, &quot;2... ## $ telefono &lt;chr&gt; &quot;4307-5842 / 5844&quot;, &quot;4962-9247 / 9248 / 9280&quot;, &quot;4805-5521 / 7533&quot;, &quot;4301-3655 / 3659&quot;, &quot;4306-46... ## $ guardia &lt;chr&gt; &quot;4307-5442 / 4300-1700&quot;, &quot;4962-9232&quot;, &quot;4805-6407&quot;, &quot;4301-4522&quot;, &quot;4306-4641 al 49 (int. 125)&quot;, &quot;... ## $ fax &lt;chr&gt; &quot;4302-7400&quot;, &quot;4962-3762&quot;, &quot;4805-7533&quot;, &quot;4303-3655/59&quot;, &quot;4306-3013&quot;, &quot;4983-7300&quot;, &quot;4307-2567&quot;, &quot;... ## $ web &lt;chr&gt; &quot;www.elizalde.gov.ar&quot;, &quot;www.guti.gov.ar&quot;, NA, &quot;www.moyano.org.ar&quot;, NA, NA, NA, NA, NA, NA, NA, ... ## $ tipo &lt;chr&gt; &quot;Hospital de niños&quot;, &quot;Hospital de niños&quot;, &quot;Hospital especializado&quot;, &quot;Hospital especializado&quot;, &quot;... ## $ tipo_espec &lt;chr&gt; &quot;PEDIATRIA&quot;, &quot;PEDIATRIA&quot;, &quot;ODONTOLOGIA&quot;, &quot;SALUD MENTAL&quot;, &quot;ENF. APAR. DIGESTIVO&quot;, &quot;ZOONOSIS&quot;, &quot;N... ## $ mod_at_1 &lt;chr&gt; &quot;AT. AMB./INTERNACION&quot;, &quot;AT. AMB./INTERNACION&quot;, &quot;AT. AMBULATORIA&quot;, &quot;AT. AMB./INTERNACION&quot;, &quot;AT.... ## $ mod_at_2 &lt;chr&gt; &quot;DIAG./TRATAM.&quot;, &quot;DIAG./TRATAM.&quot;, &quot;DIAG./TRATAM.&quot;, &quot;DIAG./TRATAM.&quot;, &quot;DIAG./TRATAM.&quot;, &quot;DIAG./TRA... ## $ depend_adm &lt;chr&gt; &quot;Ministerio de Salud GCBA&quot;, &quot;Ministerio de Salud GCBA&quot;, &quot;Ministerio de Salud GCBA&quot;, &quot;Ministerio... ## $ director &lt;chr&gt; &quot;Dr. Javier Indart&quot;, &quot;Dra. María Cristina Galoppo&quot;, &quot;Dra. Susana Rita Lisanti&quot;, &quot;Dra. Norma Der... ## $ dom_norma &lt;chr&gt; &quot;MONTES DE OCA, MANUEL AV. 40&quot;, &quot;GALLO 1330&quot;, &quot;SANCHEZ DE BUSTAMANTE 2529&quot;, &quot;BRANDSEN 2570&quot;, &quot;C... ## $ calle_altura_2 &lt;dbl&gt; 40, 1330, 2529, 2570, 2061, 4821, 849, 315, 375, 1795, 955, 2151, 4151, 15, 150, 369, 2021, 194... ## $ calle_altura_3 &lt;chr&gt; &quot;MANUEL A. MONTES DE OCA 40&quot;, &quot;GALLO 1330&quot;, &quot;SANCHEZ DE BUSTAMANTE 2529&quot;, &quot;BRANDSEN 2570&quot;, &quot;CAS... ## $ barrio &lt;chr&gt; &quot;Barracas&quot;, &quot;Recoleta&quot;, &quot;Recoleta&quot;, &quot;Barracas&quot;, &quot;Parque Patricios&quot;, &quot;Caballito&quot;, &quot;Barracas&quot;, &quot;B... ## $ comuna &lt;chr&gt; &quot;Comuna 4&quot;, &quot;Comuna 2&quot;, &quot;Comuna 2&quot;, &quot;Comuna 4&quot;, &quot;Comuna 4&quot;, &quot;Comuna 6&quot;, &quot;Comuna 4&quot;, &quot;Comuna 4&quot;,... ## $ codigo_postal &lt;dbl&gt; 1270, 1425, 1425, 1287, 1264, 1405, 1272, 1275, 1275, 1169, 1428, 1246, 1416, 1212, 1405, 1424,... ## $ codigo_postal_argentino &lt;chr&gt; &quot;C1270AAN&quot;, &quot;C1425EFD&quot;, &quot;C1425DUY&quot;, &quot;C1287ABJ&quot;, &quot;C1264AAA&quot;, &quot;C1405DCD&quot;, &quot;C1272AAA&quot;, &quot;C1275AHG&quot;,... ## $ camas_medicina &lt;dbl&gt; 7.336986, 169.665753, NA, 865.950685, 38.893151, NA, 10.575342, NA, 678.027397, NA, 44.175342, ... ## $ camas_cirugía &lt;dbl&gt; NA, 119.347945, NA, 32.000000, 33.079452, NA, NA, NA, 9.972603, NA, 26.405479, NA, 22.000000, N... ## $ camas_pediatría &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 64.00000, NA, NA, NA, 149.89041, NA, NA, NA, 18.01096, NA, NA, 30.1... ## $ camas_tocoginecología &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 96.57260, NA, NA, NA, NA, NA, NA, NA, NA, 42.18904,... ## $ camas_urgencia &lt;dbl&gt; NA, 24.619178, NA, NA, 6.082192, NA, 7.652055, NA, NA, NA, NA, 2.002740, NA, NA, NA, 14.019178,... ## $ camas_otra &lt;dbl&gt; 210.92055, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ... ## $ camas_totales &lt;dbl&gt; 218.25753, 313.63288, NA, 897.95068, 78.05479, NA, 18.22740, 64.00000, 688.00000, NA, 70.58082,... Entre todas las variables, long y lat tienen las variables espaciales. Podemos convertir este data.frame en un objeto sf, es decir espacial, de la siguiente manera: hospitales &lt;- st_as_sf(hospitales,coords=c(&quot;long&quot;,&quot;lat&quot;), crs=4326) Nos falta el último de nuestros datos: las manzanas. Pueden descargar el geojson de acá manzanas &lt;- read_sf(&quot;manzanas.geojson&quot;) Ahora sí ya podemos aplicar las distintas herramientas de análisis espacial para elegir las mejores ubicaciones para el nuevo centro de salud. 1.4.2 Detectando la cobertura actual. Una de las variables que dijimos que ibamos a tomar para detectar un espacio para un nuevo centro de salud iba a ser que la zona no estuviera atendida por un hospital. Existen distintas maneras de cubrir esta atención existente, pero vamos a usar una herramienta en particular: st_buffer(). Lo que hace esta función de sf es generar un polígono a una distancia fija desde cualquiera de los puntos de nuestros objetos espaciales. En el caso de puntos - como nuestros hospitales - esto significa un círculo del radio que nosotros querramos. Una recomendación cuando trabajemos con funciones que requieren medir distancias y relaciones entre distintos objetos espaciales es que trabajemos con CRS que se encuentren proyectados en dos dimensiones, en lugar del ESPG 4326 que se encuentra no proyectado. Usemos la proyección oficial del GCBA, cuya definición podemos encontrar acá https://spatialreference.org/ref/sr-org/8333/. Vamos a usar la función st_transform(), que es la que nos permite transformar entre sistemas de coordenadas de referncia. No vamos a usar los códigos de EPSG, sino la representación proj4string, que la pueden ver en el link que puse anteriormente. hospitales &lt;- st_transform(hospitales, crs=&quot;+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs&quot;) Si usan st_crs(hospitales) van a ver que es distinto ¿Ven la parte de +units=m? Bueno, eso implica que ahora todo lo que hagamos en términos espaciales será tomado en metros, por lo cual podemos encontrar círculos a una distancia fija, por ejemplo, 1000 metros, de cada hospital coberturaHospitales &lt;- st_buffer(hospitales,dist = 1000) Como alguno de los circulos se van a solapar, mejor juntemos todo en un gran polígono que diga que en esa zona hay cobertura, con herramientas de tidyverse coberturaHospitales &lt;- coberturaHospitales %&gt;% summarise(cobertura=TRUE) Dejemos por este dataset cargado en nuestra sesión de R por un rato, ya vamos a volver a él 1.4.3 Distancia contra la avenida más cercana Definitivamente es importante tener una buena conexión con la red de transporte para poder mover mejor a los eventuales pacientes. Teniendo nuestras calles, podemos seleccionar solo las avenidas y despues usar st_distance() para estimar la distancia entre cada una de nuestras manzanas y las avenidas. avenidas &lt;- calles %&gt;% filter(tipo_c == &quot;AVENIDA&quot;) Antes de usar las avenidas, lo que vamos a hacer es quedarnos con los centroides de las manzanas. Los centroides son el punto central dentro de un polígono. Hacemos esto especialmente por las manzanas que son muy grandes, para tener un “promedio” en lugar de uno solo de sus puntos externos. Es muy fácil tener los centroides, utlizando st_centroid(). Antes de hacerlo, vamos a convertir a manzanas al crs de hospitales, es decir el de la ciudad de buenos aires: manzanas &lt;- st_transform(manzanas,crs = st_crs(hospitales)) Ahora vamosa extraer los centroides de cada una de las manzanas y hacemos un gráfico para verlo en detalle: manzanasCentroides &lt;- st_centroid(manzanas) ggplot() + geom_sf(data = manzanas) + geom_sf(data = manzanasCentroides,color=&quot;red&quot;, size= 0.001) Si no se distinguen los puntos en este gráfico, hagánlo en su computadoras y van a poder cambiar el zoom para ver cómo cada punto está en el centro de cada manzana. Ahora midamos la distancia entre cada uno de estos puntos y todas las avenidas, pero tienen que estar en el mismo Sistema de Coordenadas de Referencia: avenidas &lt;- st_transform(avenidas,crs = st_crs(hospitales)) Ahora sí, solo hay que esperar un poco, puede tardar unos minutos: distanciaAvenidas &lt;- st_distance(manzanasCentroides,avenidas) Lo que acaba de hacer es computar la distancia, en metros, entre cada uno de los centroides y las avenidas, con dim podemos ver que efectivamente esto es lo que sucedió # 12.520 filas (centroides de manzanas) y 6,758 columnas (tramos de avenidas) dim(distanciaAvenidas) ## [1] 12520 6758 Sin embargo, nuestro objetivo es tener la distancia mínima por lo cual podemos hacer uso de apply. Es una función muy poderosa que recorre nuestra matriz en filas o culmnas haciendo algo que queremos. Por ejemplo, podemos recorrer todas las filas y tomar el valor mínimo, con lo cual nos quedaríamos con el valor mínimo para cada uno de los centroides de manzanas. # 1 significa filas, 2 columnas. functon(x) min(x) significa que para cada fila devuelva el valor mínimo avenidaMasCercana &lt;- apply(distanciaAvenidas,1,function(x) min(x)) # Rendondeamos avenidaMasCercana &lt;- round(avenidaMasCercana,0) Agregamos esta distancia a cada una de las manzanas manzanas &lt;- manzanas %&gt;% mutate(distanciaAvenida=avenidaMasCercana) 1.4.4 Incorporando el resto de las variables Ahora ya podemos agregar a los centroides, y luego a las manzanas, la información que nos falta para la construcción del índice. Para empezar, podemos hacer lo más fácil, que es marcar si el centroide está dentro de los radios de cobertura que dijimos anteriormente. Para esto, usamos st_join(), es decir la herramienta de unión espacial. Lo que hace es unir la información de dos datasets espaciales distintos en base a algún criterio espacial. De manera predeterminada, utiliza el criterio st_intersects(), es decir si existe algún tipo de intersección entre cada una de los puntos, lineas o polígonos que están en cada uno de nuestros objetos sf. manzanasCentroides &lt;- st_join(manzanasCentroides,coberturaHospitales) Igual que el left_join(), cuando no encuentra ningun matcheo entre las unidades devuelve NA, por lo que todos los valores que están en cobertura como NA en rigor no están cubiertos por la oferta de hospitales actual. Dejamos eso en claro: manzanasCentroides &lt;- manzanasCentroides %&gt;% mutate(cobertura=ifelse(is.na(cobertura),FALSE,cobertura)) Ahora tenemos que hacer algo muy similar con los datos del censo 2010, que lo tenemos en el objeto radiosCensales. radiosCensales &lt;- radiosCensales %&gt;% mutate(densidadPob=POBLACION/AREA_KM2) radiosCensales &lt;- radiosCensales %&gt;% st_transform(radiosCensales,crs=st_crs(hospitales)) manzanasCentroides &lt;- st_join(manzanasCentroides,radiosCensales) Nos queda agregar toda esta información a los polígonos de las manzanas y ya podemos ponernos a construir, finalmente,nuestro índice # Elegimos solo las variables que queremos unir, antes lo convertimos en data frame para poder perder la columna geometry manzanasCentroides &lt;- manzanasCentroides %&gt;% as.data.frame() %&gt;% select(SM,densidadPob,HOGARES_NBI,cobertura) manzanas &lt;- left_join(manzanas,manzanasCentroides,by=&quot;SM&quot;) 1.4.5 Creando nuestro índice de demanda de salud Nuestro índice de demanda de salud va a estar compuesto por un promedio ponderado de variables que construimos, pero anteriormente deberíamos normalizarlas. Lo que vamos a hacer es ponerle un valor del 1 al 5 si está entre el 0%-20%, 20%-40%, 40%-60%, 60%-80% u 80%-100% de esa variable. Esto lo podemos hacer fácilmente con la función quantile(), que calcula justamente estos quiebres, veamos: # Queremos que nos muestre en que porcentaje de estos está cada observación... quiebres &lt;- c(0,0.2,0.4,0.6,0.8,1) manzanas &lt;- manzanas %&gt;% mutate(cat_densidad=cut(densidadPob,breaks = quantile(densidadPob,quiebres,na.rm = TRUE ),include.lowest = TRUE), cat_NBI=cut(HOGARES_NBI,breaks = quantile(HOGARES_NBI,quiebres,na.rm = TRUE ),include.lowest = TRUE), cat_distanciaAv=cut(-distanciaAvenida,breaks = quantile(-distanciaAvenida,quiebres,na.rm = TRUE ),include.lowest = TRUE)) Vamos parte por parte. La función cut() corta a una variable númerica por los cortes que nosotros le indiquemos en el parámetro breaks. Por otro lado, la función quantile() nos devuelve los valores de una variable numérica que alcanza a un determinado porcentaje cuando se ordena de mayor a menor (es decir, calcula el percentil). Como buscamos los puntos que acumulan 0%, 20%, 40%, 60%, 80% y 100%, por los valores que le ponemos al vector quiebres, entonces nos va a devolver esos valores. na.rm=TRUE dentro de quantile() es para que ignore los casos donde hay NA en la variable, mientras que include.lowest = TRUE es para que cut tome al primer valor, 0, y no lo excluya de la nueva variable categórica. Además, ponemos -distanciaAvenida porque es un truco para que nos quede como valor más alto (5) cuando la distancia es menor a la avenida y más bajo (1) cuando la distancia es mayor a la avenida. Si todavía no quedó del todo claro, un gráfico habla más que mil palabras ggplot() + geom_sf(data=manzanas %&gt;% filter(!is.na(cat_NBI)) ,aes(fill=cat_NBI), color=NA) + scale_fill_viridis_d() + theme_minimal() + coord_sf(datum=NA) Bien, ahora una última vuelta de tuerca: podemos convertir estas categorías a números con la función as.numeric(), y estarán ordenados de 1, menor valor, a 5, mayor valor ¡Exactamente lo que queríamos! Esto es por cómo funcionan los factores en R # En el caso de cobertura convertira 0 cuando era FALSE y 1 cuando era TRUE manzanas &lt;- manzanas %&gt;% mutate(cat_densidad=as.numeric(cat_densidad), cat_NBI=as.numeric(cat_NBI), cat_distanciaAv=as.numeric(cat_distanciaAv), cat_cobertura=as.numeric(!cobertura)) Ya podemos crear nuestro índice con las ponderaciones que queramos. Usemos un 10% para la densidad, un 30% para los NBI, un 10% para la distancia con la avenida más cercana y un 50% sobre si en el lugar falta o no cobertura. manzanas &lt;- manzanas %&gt;% mutate(IDS=cat_densidad*0.1+cat_NBI*0.3+cat_distanciaAv*0.1+cat_cobertura*0.5, IDS=ifelse(IDS&gt;quantile(IDS,probs = 0.9,na.rm = TRUE),TRUE,FALSE)) Hagamos nuestro gráfico: # Podemos agrupar a los radioscensales por barrio para que nos queden los polígonos de los barrios. # Tambien es posible bajarlos directamente desde la página del GCBA barrios &lt;- radiosCensales %&gt;% group_by(BARRIO) %&gt;% summarise(n()) ## `summarise()` ungrouping output (override with `.groups` argument) ggplot() + geom_sf(data=barrios) + geom_sf(data=manzanas %&gt;% filter(!is.na(IDS)) ,aes(fill=IDS), color=NA) + scale_fill_manual(values = c(NA,&quot;#f03b20&quot;)) + guides(fill=FALSE) + theme_minimal() + coord_sf(datum=NA) + labs(title=&quot;Índice de Demanda de Salud&quot;, subtitle=&quot;Ciudad de Buenos Aires&quot;) 1.5 Ejercicio La Ciudad de Buenos Aires cuenta con otras alternativas a los hospitales, por ejemplo los CESAC. Descargá los datos de los CESAC y agregalos al gráfico donde se encuentran las áreas más acuciantes del IDS ¿Coinciden las zonas? ¿No? ¿Dónde construirían el próximo CESAC, ahora que saben dónde están emplazados los actuales? "],
["geocoding-de-la-representación-humana-al-sistema-de-coordenadas.html", "2 Geocoding: de la representación humana al sistema de coordenadas 2.1 ¿Qué es la geocodificación o geocoding? 2.2 API: Interfaz de programación de aplicaciones 2.3 Servicio de Normalización de Datos Geográficos de Argentina 2.4 Google geocode API 2.5 Diferencias entre las dos APIs 2.6 Una tercera alternativa: hereR 2.7 Comparando todos los métodos 2.8 Ejercicios", " 2 Geocoding: de la representación humana al sistema de coordenadas En capítulos anteriores ya descubrimos la particularidad de la representación de los datos espaciales: los ubicamos en base a un modelo de la tierra en lo que se conoce como Coordinate Reference System (CRS). Esta forma de representación es muy distinta a la que tenemos en nuestras cabezas cuando nos dicen que tenemos que ir a cursar a Figueroa Alcorta 7350. En diversas circunstancias vamos a necesitar convertir esta direcciones a puntos espaciales para poder agregar información relevante para nuestros análisis y encontrar nuevos patrones en nuestros datos. Veamos cómo podemos hacerlo usando servicios del Estado de Argentina y también de Google. 2.1 ¿Qué es la geocodificación o geocoding? Geocoding no es otra cosa que la transformación de una ubicación en el formato que manejamos a diario hacia una coordenada en un sistema de coordenadas de referencia (CRS, en inglés). Esta simple operación es sumamente útil para muchos de nuestros objetivos. Imaginemos que queremos tener alguna medida de la cobertura de atención para la salud en la Ciudad de Buenos Aires. Tenemos la dirección de distintos hospitales y centros de salud, pero no conocemos cómo se distribuyen en el espacio. Carguemos primero estos datos desde aquí library(tidyverse) salud &lt;- read_csv(&quot;https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/HospitalesYCentrosSalud.csv&quot;) Y veamos un poco las variables que tenemos glimpse(salud) ## Rows: 115 ## Columns: 4 ## $ Establecimiento &lt;chr&gt; &quot;Hospital General de Agudos Dr. T. Alvarez&quot;, &quot;Hospital General de Agudos Dr. C. Argerich&quot;, &quot;Hospital Ge... ## $ Dirección &lt;chr&gt; &quot;Doctor Juan Felipe Aranguren 2701&quot;, &quot;Pi y Margal 750&quot;, &quot;Diaz Velez 5044&quot;, &quot;Cerviño 3356&quot;, &quot;Pedro Chutr... ## $ Barrio &lt;chr&gt; &quot;Flores&quot;, &quot;La Boca&quot;, &quot;Caballito&quot;, &quot;Palermo&quot;, &quot;Parque Patricios&quot;, &quot;Flores&quot;, &quot;Coghlan&quot;, &quot;Balvanera&quot;, &quot;Rec... ## $ Tipo &lt;chr&gt; &quot;Hospital General de Agudos&quot;, &quot;Hospital General de Agudos&quot;, &quot;Hospital General de Agudos&quot;, &quot;Hospital Gen... Geocodificemos con la ayuda del paquete wrapar # Si no lo tenés insalado # require(devtools) # install_github(&quot;martinmontane/wrapar&quot;) library(wrapar) # Agregamos una variable de ID y una columna que indique la provincia salud &lt;- salud %&gt;% mutate(id=row_number(), provincia=&quot;Ciudad de Buenos Aires&quot;) saludGeoreferenciado &lt;- geocodeDirecciones(datos = salud, col_id = &quot;id&quot;, col_direccion = &quot;Dirección&quot;, col_provincia = &quot;provincia&quot;) # Seleccionamos las que tuvimos algún match saludGeoreferenciado &lt;- saludGeoreferenciado %&gt;% filter(nMatchAPI %in% 1) # Le agregamos información que estaba en el anterior data.frame saludGeoreferenciado &lt;- left_join(saludGeoreferenciado, salud, by=c(&quot;id&quot;)) library(sf) # Convertimos a objeto sf saludSf &lt;- st_as_sf(saludGeoreferenciado, coords=c(&quot;ubicacion.lon&quot;,&quot;ubicacion.lat&quot;), crs=4326) Con la ayuda de leaflet hagamos un simple mapa interactivo library(leaflet) leaflet(saludSf) %&gt;% addTiles() %&gt;% addMarkers(label = ~ Establecimiento, popup = ~ Tipo) Nada mal, no? No se preocupen si no entienden lo que hicimos, la única idea de esta introducción es mostrarle lo que van a ser capaces de hacer, nada más ni nada menos. Vamos a ir explicando cómo funciona todo esto. 2.2 API: Interfaz de programación de aplicaciones Sin saberlo, en el ejemplo anterior usamos la API de georreferenciación de Argentina a través de la función geocodeDirecciones() del paquete wrapar ¿Qué es una API? Una API es un conjunto de reglas preestablecidas que nos permiten comunicarnos con servicios que están escritos en diferente lenguaje y con un conjunto de procedimientos específicos. Imaginenlo como que nuestro código de R es español, y la API de geolocalización del Gobierno está en francés, la API podría ser un idioma intermedio, como el inglés, para comunicarnos. Si quieren aprender como comunicarse sin hacerlo por intermedio de wrapar [pueden aprenderlo leyendo los documentos del desarrollo del gobierno]. La idea de wrapar es no tener que aprender otro idioma y hacer todo desde R. Las APIs exceden a este desarrollo particular del gobierno y casi cualquier servicio de cualquier empresa tiene una API para que distintos usuarios puedan hacer consultas sin tener que conocer específicamente cómo es que el servicio trabaja por detrás, es muy útil y eficiente. En este capítulo vamos a usar dos APIs: la del Gobierno, que ya fue presentada, y la de Google. Veamos las funcionalidades y ventajas y desventajas que cada una tiene. 2.3 Servicio de Normalización de Datos Geográficos de Argentina Podemos comunicarnos con el servicio de normalización de datos geográficos de Argentina mediante la función geocodeDirecciones() del paquete wrapar. La función necesita que le pasemos algunos parámetros para poder hacer bien su trabajo. datos: En este parámtro simplemente hay que poner el data.frame que tiene la información que querés georeferencair col_id: es el nombre de la columna que tiene los códigos identificadores únicos de cada uno de los puntos. La función lo hace obligatorio porque va a ser la columna que después va a ser útil para incorporar el resultado de la geocodificación. col_direccion: es el nombre de la columna donde se encuentra la dirección, sin incluid información sobre la localidad, provincia, pais, etc col_provincia: es el nombre de la columna que tiene el nombre de la provincia Con estos cuatro parámetros, cómo hicimos anteriormente, la función pasa las dirección a la API y la API nos devuelve un data.frame con las siguientes variables: id: la columna que identifica a cada uno de los puntos nMatchAPI: nos dice cuántos resultados encontró la API para esa dirección (puede ser más de uno) codigoAPI: Un código que dice “Exito” cuando se pudo comunicar con la API o “Error” cuando hubo algún problema en la comunicación calle.nombre, departamento.nombre, localidad_censal.nombre, y nomenclatura: variables donde tenemos más información sobre la dirección que encontró la API ubicacion.lat y ubicacion.lon las coordenadas de latitud y longitud en el sistema de coordenads EPSG 4326 Ahora que ya sabemos cómo funciona, relean el código que estaba en la parte de arriba ¿Cuántos puntos pudo geolocalizar? 97. En principio, no sabemos si están bien o mal (la API puede haber devuelto simplemente un punto que no correspondía), pero podemos estar seguros que para 18 de nuestros 115 casos no encontró ninguna respuesta. Esto suele pasar cuando la API no es capaz de mapear la dirección que le pasamos con otra que si reconozca. Usando otras herramientas vistas anteriormente, hagamos un mapa de distancia mínima entre cada una de las manzanas de la Ciudad de Buenos Aires y los puntos de los establecimientos de salud. Lo primero que tenemos que hacer para medir las distancias es tener proyectados a los CRS para que la distancia midan metros y no diferencias entre coordenadas latitud y longitud. manzanas &lt;- read_sf(&quot;http://cdn.buenosaires.gob.ar/datosabiertos/datasets/manzanas/manzanas.geojson&quot;) # El objeto manzanas está representado en el CRS que corresponde al ESPG 4326 st_crs(manzanas) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] # El objeto de saludSf no tiene información sobre el CRS de las coordenadas, pero sabemos que es 4326 st_crs(saludSf) &lt;- 4326 # Convertimos a los dos a la proyección que usa el GCBA manzanas &lt;- st_transform(manzanas, crs=&quot;+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs &quot;) saludSf &lt;- st_transform(saludSf, crs=&quot;+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs &quot;) Fìjense que usamos un texto largo en lugar de un código para especificar el CRS de la Ciudad de Buenos Aires. Esto suele pasar cuando una proyección no se encuentra correctamente indexada en el catálogo de EPSG. Lo único que hace EPSG es ponerle un número a cada conjunto de parámetros que determinan un CRS (como los que vemos en el texto que usamos dentro de st_transform). Para buscar los CRS cuando no saben cual usar, les recomiendo https://spatialreference.org/ Ya podemos calcular las distancias. # Tomamos las distancias distanciaManzanas &lt;-st_distance(x = manzanas, y=saludSf) # Nos quedamos con el valor mínimo de distancia entre cada punto y cada manzana distanciaManzanas &lt;- apply(distanciaManzanas,1,min) # Agregamos estos datos al dataset de manzanas manzanas &lt;- manzanas %&gt;% mutate(distMinima=distanciaManzanas) Quizás la parte más rara o con la que están menos familiares es la que dice apply(distanciaManzanas,1,min). La función apply() sirve para iterar, es decir, para realizar el mismo procedimiento para todas las filas o columnas de una matriz. Cuando ponemos 1 luego de pasar a la matriz, va a hacer todo fila por fila, si dice 2 lo hará columna por columna. distanciaManzanas tenía la distancia en metros de cada uno de los centroides de las manzanas contra cada uno de los puntos, por lo cual lo que hace ahí es ir fila por fila y tomar el valor mìnimo (fijense que le pedimos que aplique la función min en el ùltimo argumento). Con esto, ya podemos hacer nuestro gráfico de la accesibilidad de las manzanas a establecimientos de salud: manzanas &lt;- manzanas %&gt;% mutate(distCat=cut(distMinima, breaks = c(0,400,600,900,max(manzanas$distMinima)), labels = c(&quot;Hasta 400 metros&quot;, &quot;Entre 400 y 600 metros&quot;, &quot;Entre 600 y 900 metros&quot;,&quot;Más de 900 metros&quot;), include.lowest = TRUE)) ggplot(manzanas) + geom_sf(aes(fill=distCat),color=NA) + scale_fill_viridis_d(direction = -1,name=&quot;Distancia&quot;) + theme_minimal() + coord_sf(datum = NA) Se observa una importante parte de la Ciudad de Buenos Aires cubierta por al menos un hospital/centro de salud a menos de 400m, con algunas manchas azules en algunas zonas, que puede o no ser explicada por los 18 puntos que nos faltaron georeferenciar. Repliquemos lo mismo, pero ahora usando el servicio de google. 2.4 Google geocode API Google ofrece una amplia gama de servicios para lo que ellos llaman “desarrolladors”, que excede largamente el uso de la API de Google Maps. Sin embargo, la forma para acceder a todos estos servicios tiene como origen una misma cuenta. Desde esta cuenta se pueden ir agregando diversos modulos con funcionalidades distintas. Veamos como registrar la cuenta y habilitar los servicios que necesitamos. Vayan a https://cloud.google.com/ e ingresen con una cuenta de Google (no es necesario que usen una cuenta personal, pueden crear una nueva en caso de ser necesario). Una vez que esten logueados, van a ver un botón en la esquina derecha que dicen “Consola”. Ingresen ahí y desde el menú de navegación en la parte izquierda de la pantalla vayan a APIs y servicios. Ahí van a ver que les va a pedir que creen un proyecto: para usar una API es necesario asociar a un “proyecto” en la plataforma de Google, al que le pueden poner el nombre que ustedes quieran. Lo que nosotros necesitamos ahora es activar las APIs que queremos usar. Google ofrece una multitud de APIs, por lo que más simple es buscarlas en la barra que nos ofrece. Vamos a activar dos APIs: Geocoding API y Distance Matrix API. Primero habilitemos Geocoding API y veamos qué pasa. Si todo funcionó bien, deberían estar en la página de su proyecto nuevo, pero ahora en el submenú de API deberían ver que dentro de API habilitadas tienen una que dice “Geocoding API” ¡Muy bien! Abajo de ese título van a ver que les recomienda otras API en API adicionales. Elijan Distance Matrix API. Habilítenla y listo, ya tenemos las dos APIs con las que vamos a trabajar en el curso. Ahora solo nos queda crear una especie de “contraseña” con la que vamos a vincular nuestra cuenta de Google desde R. Esto es necesario porque las API de Google son servicios pagos. Dan USD 300 de crédito inicial, pero luego hay que ingresar una tarjeta de crédito para continuar usando el servicio, aunque dan USD 200 de crédito todos los meses, el equivalente aproximadamente a 40.000 búsquedas de geocoding. Esta “contraseña” se llama credencial o “key”. Pueden activarla desde el menú de la izquierda en “API y servicios” y luego “credenciales”. Allí pueden crear una credencial, que es básicamente una clave de muchos caracteres que será necesaria para lo que sigue. Importante: Para que efectivamente funcione lo que sigue en esta clase tienen que asociar una tarjeta de crédito a la cuenta de Google. Si bien “regala”\" USD 300 en el primer uso, y USD 200 todos los meses, hay que tener cuidado una vez que la tarjeta ha sido ingresada. Si quieren probar el uso de esta herramienta pongan quotas desde su cuenta de Google. Hechas las presentaciones formales, veamos cómo podemos hacer lo mismo con la funcion geocodeGoogle(). Esta función requiere parámetros similares a la de geocodeDirecciones(), veamos: datos: En este parámtro simplemente hay que poner el data.frame que tiene la información que querés georeferencair col_id: es el nombre de la columna que tiene los códigos identificadores únicos de cada uno de los puntos. La función lo hace obligatorio porque va a ser la columna que después va a ser útil para incorporar el resultado de la geocodificación. cols_query: En esta columna hay que pasar un vector character con las variables que queremos que se incluyan en la consulta col_key: nombre de la columna que tiene los datos de la “key” o “credencial” de google col_region: Este parámetro es opcional y se puede delimitar la búsqueda de google a un área. Por default, region no tiene ningún valor por lo cual busca en todo el mundo. Veamosla en función. Este código, así como está, no debería funcionar porque la key es incorrecta. Deben colocar una propia para que funcione # Mismas operaciones que anteriormente, pero esta vez agregamos una columna con la key y de region AR salud &lt;- read_csv(&quot;https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/HospitalesYCentrosSalud.csv&quot;) salud &lt;- salud %&gt;% mutate(id=row_number(), provincia=&quot;CABA&quot;, key=&quot;aknfadgnadoigdagoida&quot;, region=&quot;AR&quot;) Ahora ya podemos hacer el geocoding con google ! saludGoogle &lt;- geocodeGoogle(datos = salud, col_id = &quot;id&quot;, cols_query =c(&quot;Dirección&quot;,&quot;provincia&quot;,&quot;Barrio&quot;), col_key = &quot;key&quot;, col_region = &quot;region&quot;) Carguemos los datos de la geolocalización, si hubieran ejecutado el código de Google load(file = url(&quot;https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/geocodingSaludExample.RData&quot;)) Un mapa rápido en leaflet para ver qué encontró: saludGoogle &lt;- st_as_sf(x = saludGoogle, coords=c(&quot;results.geometry.location.lng&quot;,&quot;results.geometry.location.lat&quot;), crs=4326) leaflet(saludGoogle) %&gt;% addTiles() %&gt;% addMarkers() Parece bastante bien, veamos cuáles son las varaibles que nos devolvió glimpse(saludGoogle) ## Rows: 118 ## Columns: 19 ## $ results.address_components &lt;list&gt; [&lt;data.frame[7 x 3]&gt;, &lt;data.frame[6 x 3]&gt;, &lt;data.frame[8 x 3]&gt;, &lt;data.frame[7 ... ## $ results.formatted_address &lt;chr&gt; &quot;Dr. Juan Felipe Aranguren 2701, C1406 FWY, Buenos Aires, Argentina&quot;, &quot;Pi y Mar... ## $ results.geometry.bounds.northeast.lat &lt;dbl&gt; -34.62389, -34.62763, NA, -34.58102, -34.64273, NA, NA, NA, NA, NA, NA, -34.624... ## $ results.geometry.bounds.northeast.lng &lt;dbl&gt; -58.46902, -58.36497, NA, -58.40647, -58.40930, NA, NA, NA, NA, NA, NA, -58.507... ## $ results.geometry.bounds.southwest.lat &lt;dbl&gt; -34.62439, -34.62853, NA, -34.58183, -34.64408, NA, NA, NA, NA, NA, NA, -34.625... ## $ results.geometry.bounds.southwest.lng &lt;dbl&gt; -58.46994, -58.36607, NA, -58.40746, -58.41131, NA, NA, NA, NA, NA, NA, -58.508... ## $ results.geometry.location_type &lt;chr&gt; &quot;ROOFTOP&quot;, &quot;ROOFTOP&quot;, &quot;ROOFTOP&quot;, &quot;ROOFTOP&quot;, &quot;GEOMETRIC_CENTER&quot;, &quot;ROOFTOP&quot;, &quot;ROO... ## $ results.geometry.viewport.northeast.lat &lt;dbl&gt; -34.62279, -34.62673, -34.60807, -34.58008, -34.64205, -34.64232, -34.56350, -3... ## $ results.geometry.viewport.northeast.lng &lt;dbl&gt; -58.46813, -58.36417, -58.43656, -58.40561, -58.40896, -58.45280, -58.46989, -5... ## $ results.geometry.viewport.southwest.lat &lt;dbl&gt; -34.62549, -34.62943, -34.61077, -34.58277, -34.64475, -34.64501, -34.56620, -3... ## $ results.geometry.viewport.southwest.lng &lt;dbl&gt; -58.47083, -58.36687, -58.43926, -58.40831, -58.41166, -58.45550, -58.47259, -5... ## $ results.place_id &lt;chr&gt; &quot;ChIJr_Z5l4zJvJURY2wsvn1SkM8&quot;, &quot;ChIJd9naU8k0o5URZ2n7ZEl68fA&quot;, &quot;ChIJbROcemvKvJUR... ## $ results.types &lt;list&gt; [&quot;premise&quot;, &quot;premise&quot;, &quot;street_address&quot;, &quot;premise&quot;, &lt;&quot;establishment&quot;, &quot;health&quot;... ## $ status &lt;chr&gt; &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;OK&quot;, &quot;... ## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, ... ## $ results.plus_code.compound_code &lt;chr&gt; NA, NA, &quot;9HR6+6R Buenos Aires, Argentina&quot;, NA, NA, &quot;9G4W+G8 Buenos Aires, Argen... ## $ results.plus_code.global_code &lt;chr&gt; NA, NA, &quot;48Q39HR6+6R&quot;, NA, NA, &quot;48Q39G4W+G8&quot;, &quot;48Q3CGPH+3G&quot;, &quot;48Q39HJR+W5&quot;, &quot;48... ## $ results.partial_match &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ geometry &lt;POINT [°]&gt; POINT (-58.46949 -34.62417), POINT (-58.3655 -34.62803), POINT (-58.43791... Wow, muchas variables. La primera columna tiene, adentro, un data.frame con información que está en el resto de las columnas, así que vamos a ignorarla. Luego tenemos la formatted_address, que nos permite ver cuál es la calle que efectivamente nos devolvió Google. Luego tenemos cuatro coordenadas bounds, que sirven solo cuando nos devolvió un polígono en lugar de un punto (no le vamos a prestar atención, vamos a trabajar con los puntos que haya devuelto). Luego en location.lat y location.lng tenemos específicamente la latitud y longitud de nuestra dirección. Existen otras columnas, pero con estas y id y partial_match ya podemos seguir. Si están atentos/as a lo que estamos haciendo, van a ver que saludGoogle devolvió 118 resultados cuando mandamos 115 ! Qué fue lo que pasó? Siempre que pasa esto es porque google no encontró un resultado “perfecto” para nuestra consulta y nos devolvió más de uno. Cómo nos damos cuenta? De distintas maneras, pero partial_match, status y mirar a los duplicados nos va a servir mucho. Para empezar, hagamos esto último: veamos cuáles son los que están duplicados # Buscamos los duplicados idsDuplicados &lt;- saludGoogle %&gt;% group_by(id) %&gt;% summarise(conteo=n()) %&gt;% filter(conteo&gt;1) %&gt;% pull(id) # Los vemos con View() View(saludGoogle[saludGoogle$id %in% idsDuplicados,]) En “results.types” van a ver que los ids 78 y 79 son rutas en lugar de puntos, por lo cual no nos sirve: no pudo encontrar un punto que se parezca a la dirección que le pasamos. Vamos a tener que filtrar esos casos. Finalmente, el id 62 nos devuelve dos casos para la misma dirección, uno pareciera ser el hospital y otro la policía (en la misma dirección): eso es algo que no nos interesa. Entonces lo que vamos a hacer es sacar a 78 y 79 y quedarnos solo con uno de los dos del 62. Lo hacemos así: # Filtramos los ids y cualquier caso duplicado en id saludGoogle &lt;- saludGoogle %&gt;% filter(!id %in% c(78,79)) %&gt;% filter(!duplicated(id)) La función duplicated nos elimina a todas las filas que aparecen por segunda vez en el data.frame con el mismo id. En este caso, elimina la segunda vez que aparece el número 62, por lo que funcionó bien. Ahora tenemos 113 puntos georeferenciados, solo perdimos el 78 y el 79. Hagamos el mismo gráfico que hicimos anteriormente: # El objeto de saludGoogle no tiene información sobre el CRS de las coordenadas, pero sabemos que es 4326 st_crs(saludGoogle) &lt;- 4326 # Convertimos a los dos a la proyección que usa el GCBA manzanas &lt;- st_transform(manzanas, crs=&quot;+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs&quot;) saludGoogle &lt;- st_transform(saludGoogle, crs=&quot;+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs&quot;) # Tomamos las distancias distanciaManzanas &lt;- st_distance(x = manzanas, y=saludGoogle) # Nos quedamos con el valor mínimo de distancia entre cada punto y cada manzana distanciaManzanas &lt;- apply(distanciaManzanas,1,min) # Agregamos estos datos al dataset de manzanas manzanas &lt;- manzanas %&gt;% mutate(distMinima=distanciaManzanas) manzanas &lt;- manzanas %&gt;% mutate(distCat=cut(distMinima, breaks = c(0,400,600,900,max(manzanas$distMinima)), labels = c(&quot;Hasta 400 metros&quot;, &quot;Entre 400 y 600 metros&quot;, &quot;Entre 600 y 900 metros&quot;,&quot;Más de 900 metros&quot;), include.lowest = TRUE)) ggplot(manzanas) + geom_sf(aes(fill=distCat),color=NA) + scale_fill_viridis_d(direction = -1,name=&quot;Distancia&quot;) + theme_minimal() + coord_sf(datum = NA) Bastante parecida a la geolocalización de la API del gobierno, a simple vista, pero con una mayor cobertura. Veamos, ahora, en cuanto difieren los cálculos de uno y otro dataset 2.5 Diferencias entre las dos APIs Podemos ver las diferencias de muchas maneras, pero poniendo los puntos con un color de cada uno no sería lo mejor, ya que no podemos ver entre cual par de puntos fue la diferencia. Hagamos algo más simple: la distancia entre cada uno de los puntos. distancias &lt;- st_distance(x = saludGoogle[saludGoogle$id %in% saludSf$id,], saludSf[saludSf$id %in% saludGoogle$id,], by_element = TRUE) distancias &lt;- data.frame(distancia=as.numeric(distancias)) ggplot(distancias) + geom_histogram(aes(x=distancia)) + theme_minimal() + labs(x=&quot;Discrepancia (metros)&quot;,y=&quot;Cantidad de casos&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. La diferencia es baja en casi todos los casos, menos uno que está en aproximadamente 900 metros. Si aceptamos una tolerancia de 200 metros de diferencia, la coincidencia es casi total, nada mal. Google tiene una ventaja, igual: devolvió todos menos dos puntos. 2.6 Una tercera alternativa: hereR En general, la tarea de geocodificación requiere cierta prueba y error. No todos los servicios de geocodificación saben interpretar de la misma manera a las direcciones, y muchas veces las asignan a distintas coordenadas (este problema puede ser especialmente importante al trabajar con Google Maps, que siempre intenta devolver algún resultado). El paquete hereR nos brinda una alternativa muy interesante para geocodificar direcciones. El servicio de Here Maps permite hasta 250.000 consultas mensuales de manera gratuita, lo cual puede ser suficiente para muchos de nuestros proyectos. Lo único que precisan para poder hacer uso del sistema de mapas de HERE Maps es simplemente instalar el paquete hereR, y vincular a nuestra sesión de R con la API Key correspondiente a nuestra cuenta de HERE Maps. library(hereR) set_key(&quot;&lt;YOUR API KEY&gt;&quot;) salud &lt;- salud %&gt;% mutate(consultaHERE=paste(Dirección,&quot;, Ciudad de Buenos Aires, Argentina&quot;,sep=&quot;&quot;)) saludHERE &lt;- geocode(salud %&gt;% pull(consultaHERE)) A diferencia de lo que hicimos con wrapar, en este caso ya nos devolvió un objeto sf, podemos usar leaflet para hacer un control de que, al menos, nuestros puntos están en la Ciudad de Buenos Aires leaflet(saludHERE) %&gt;% addTiles() %&gt;% addMarkers() 2.7 Comparando todos los métodos Llegó el momento de comparar todos los métodos que usamos en la clase. Una primera aproximación para evaluar qué tan efectivos han sido para geolocalizar las direcciones sin ningún tipo de normalización es analizar la cantidad de respuestas a las consultas que le envíamos. Si revisamos la cantidad de respuestas, HERE devolvió una posición para cada una de las direcciones, google también - aunque luego de hacer una limpieza nos quedamos con 113 -, mientras que la API de georreferenciación de Argentina devolvió 97. Ahora bien, estas diferencias en la cantidad de respuestas solo miden la cobertura, no la precisión. Para testear esto vamos a aprovechar que el GCBA ya geolocalizó estas direcciones. Vamos a cargar un dataset que tiene la longitud y latitud según el Gobierno de la Ciudad de Buenos Aires de 88 de estos puntos de salud load(url(&quot;https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/saludGCBA.RData&quot;)) El objeto saludGCBA cuenta con esta información, pero si se fijan la clase verán que no es un objeto espacial. Vamos a hacer eso y, también a proyectarlo en el CRS que estuvimos trabajando para poder medir las distancias. # Acá va el código para poder cargar los datos del GCBA saludGCBA &lt;- st_as_sf(saludGCBA,coords=c(&quot;longGCBA&quot;,&quot;latGCBA&quot;),crs=4326) saludGCBA &lt;- st_transform(saludGCBA,crs = st_crs(saludSf)) Nos queda convertir el dataset de georeferenciación de HERE Maps también al sistema de coordenadas proyectado que usamos para el resto de los datos. Una vez que tenemos todo esto, ya podemos agregar como una columna la distancia - en metros - entre cada una de las metodologías que estuvimos aplicando y la ubicación asignada por el GCBA # Proyectamos los datos de HERE Maps saludHERE &lt;- saludHERE %&gt;% st_transform(st_crs(saludSf)) # Agregamos los datos de distancia para cada uno de los 88 puntos de salud geocodificados por el GCBA # Creamos data frames que tengan el ID y la distancia usando st_distance() distanciaHERE &lt;- data.frame(id=saludGCBA %&gt;% filter(saludGCBA$id %in% saludHERE$id) %&gt;% pull(id), distanciaHERE = st_distance(saludGCBA %&gt;% filter(saludGCBA$id %in% saludHERE$id), saludHERE %&gt;% filter(saludHERE$id %in% saludGCBA$id), by_element = TRUE) %&gt;% as.numeric()) distanciaARG &lt;- data.frame(id=saludGCBA %&gt;% filter(saludGCBA$id %in% saludSf$id) %&gt;% pull(id), distanciaARG = st_distance(saludGCBA %&gt;% filter(saludGCBA$id %in% saludSf$id), saludSf %&gt;% filter(saludSf$id %in% saludGCBA$id), by_element = TRUE) %&gt;% as.numeric()) distanciaGoogle &lt;- data.frame(id=saludGCBA %&gt;% filter(saludGCBA$id %in% saludGoogle$id) %&gt;% pull(id), distanciaGoogle = st_distance(saludGCBA %&gt;% filter(saludGCBA$id %in% saludGoogle$id), saludGoogle %&gt;% filter(saludGoogle$id %in% saludGCBA$id), by_element = TRUE) %&gt;% as.numeric()) # Unimos todas las distancias en un solo data frame distancias &lt;- left_join(distanciaHERE,distanciaARG) ## Joining, by = &quot;id&quot; distancias &lt;- left_join(distancias,distanciaGoogle) ## Joining, by = &quot;id&quot; # Incorporamos esta información al dataset de saludGCBA saludGCBA &lt;- left_join(saludGCBA,distancias) ## Joining, by = &quot;id&quot; # Eliminamos los objetos que ya no necesitamos más rm(distanciaARG,distanciaGoogle,distanciaHERE) # Nos quedamos solo con los casos donde hay información de distancia con todos los métodos saludGCBA &lt;- saludGCBA %&gt;% filter(!is.na(distanciaARG) &amp; !is.na(distanciaGoogle) &amp; !is.na(distanciaHERE)) Finalmente, saludGCBA ahora cuenta con información sobre la ubicación de 72 puntos que ya habían sido geolocalizadas por el GCBA y que pudimos georeferrenciar mediante los tres métodos de este capítulo. Veamos cuales son las diferencias más comunes al observar los cuartiles de la distribución de cada uno de los métodos. Presten atención a st_set_geometry(NULL), una función que nos permite eliminar la columna de geometría para hacer cálculos que no necesita de ellos. saludGCBA %&gt;% st_set_geometry(NULL) %&gt;% summarise_at(c(&quot;distanciaARG&quot;,&quot;distanciaGoogle&quot;,&quot;distanciaHERE&quot;),function(x) quantile(x,probs = c(0,0.25,0.5,0.75,1))) ## distanciaARG distanciaGoogle distanciaHERE ## 1 10.52787 0.599979 6.94956 ## 2 17.70740 5.824472 14.96824 ## 3 20.06838 14.473270 20.19695 ## 4 30.82211 38.104298 28.66071 ## 5 649.08766 954.737526 689.78994 2.8 Ejercicios Busquen un conjunto de direcciones que les interese y calculen las distancias entre ellas usando la aplicación de geolocalización de Argentina y la función de distancia del paquete sf. "],
["tiempos-de-viaje-y-análisis-de-accesibilidad.html", "3 Tiempos de viaje y análisis de accesibilidad 3.1 La distancia espacial y la distancia de viaje 3.2 Los paquetes que vamos a utilizar 3.3 Los datos: repositorio de datos del GCBA 3.4 Transformando nuestros datos 3.5 ¿Cuánto tardo en llegar al monumental? 3.6 Midiendo la cobertura de los parques 3.7 Redondeando: caracterizando la oferta de los espacios verdes en CABA 3.8 Ejercicios", " 3 Tiempos de viaje y análisis de accesibilidad Nuestros datos espaciales pueden ser enriquecidos de distintas maneras, no solo a través de la relación con otras entidades espaciales, tal como se mostró en el ejercicio de ubicación óptima para uno nuevo centro de salud anteriormente. Es posible agregar información sobre tiempos de desplazamiento en distintos tipos de desplazamiento o aproximar un polígono que, para un determinado punto, muestra cuáles son los lugares a los que se pueden acceder en u determinado tiempo. Yendo un paso más atrás, incluso podemos lograr encontrar la coordenadas a partir del texto de una dirección. Vamos a aplicar todos estos conceptos y verlos en acción en un tema muy interesante: el acceso a los espacios verdes en la Ciudad de Buenos Aires. 3.1 La distancia espacial y la distancia de viaje Las personas que vivimos en ciudades entendemos a la perfección que la distancia entre punto A y B puede medirse de distintas maneras. A veces, caminar 1000 metros suele tardar mucho menos que hacerlo en auto, y a veces los tiempos de transporte público pueden variar mucho según la proximidad con distintos medios de transporte disponibles. En distintas circunstancias puede ser muy importante entender cuáles son las condiciones de acceso de cada una de las zonas de una ciudad para distintas razones: trabajo, recreación, salud, entre otras variables. En esta clase vamos a medir el acceso a la los espacios verdes de la Ciudad de Buenos Aires, pero tan solo por una cuestión de acceso a datos. Si tuvieramos, por ejemplo, información sobre la ubicación de los puntos donde las empresas están ubicadas, podríamos estimar la accesibilidad al mercado de trabajo de cada uno de los lugares. Sea como fuere, basta de preámbulos y veamos cómo podemos procesar los datos espaciales para tener una medida espacial del acceso a espacios verdes en la Ciudad de Buenos Aires. Para esto vamos a hacer lo siguiente: Medir la cobertura de los espacios verdes de la CABA, midiendo desde qué lugares se puede llegar a 15 minutos caminando Cruzar estos datos con las manzanas y establecer que aquellas manzanas que no se encuentran en este espacio de cobertura están “mal atendidas” 3.2 Los paquetes que vamos a utilizar Los capítulos de este libro en general no suelen introducir los paquetes que tienen las herramientas que vamos a utilizar antes de que sean necesarias para resolver un problema en particular. Sin embargo, para este capítulo hacemos una pequeña excepción, ya que vamos a cargar un conjunto de paquetes en los que vamos a tener que hacer zoom tanto en lo que ofrecen como en qué pasos adicionales tenemos que hacer para utilizarlas. Vamos paso por paso: sf: Este paquete ya lo conocemos, es el que nos permite trabajar con datos espaciales en R. tidyverse: Colección de paquetes que nos permite cumplir muchas de las tareas necesarias en un proyecto de ciencia de datos. En esta clase vamos a investigar una función del paquete purrr, parte de tidyverse, y que nos permite leaflet: Herramienta muy poderosa para generar mapas interactivos. En este capítulo lo vamos a utilizar para que resolver uno de los potenciales problemas que puede tener R para actuar como GIS: la falta de modificaciones o exploraciones “manuales” de los datos espaciales hereR: Si bien existen distintas alternativas para medir los tiempos de desplazamiento, en este caso vamos a usar el servicio de HERE maps. La API de here puede usarse utilizando el lenguaje de R gracias a las personas que desarrollaron hereR. 3.2.1 Cómo usar los servicios de HERE HERE Maps es una empresa que nos brinda distintas herramientas de geolocalización, medidas de tiempo de viaje entre distintos puntos. Piensen en ella como una caja de herramientas donde podemos elegir entre ellas para poder resolver problemas particulares. A diferencia de otras alternativas, como Google Maps, HERE nos permite realizar hasta 250.000 consultas gratuitas por mes sin tener que poner nuestra tarjeta de crédito como garantía. Solía ser de esta manera con Google Maps en el pasado, pero cambiaron las condiciones de un tiempo para acá, por lo cual hay que buscar alternativas y HERE nos puede ser útil. Para usar HERE Maps en R, además de instalar el paquete hereR, debemos tener una key, que no es otra cosa que una contraseña única que nos pide HERE Maps para poder vincular el uso que le damos a la cuenta desde R con sus registros internos. Para esto, primero hay que crear una cuenta en https://developer.here.com/. Una vez que hayan creado la cuenta - gratuita - tienen que ir a los “projects” que tiene y hacer click en create API key donde dice REST. Una vez que la creen, van a ver que les aparece una tabla con “API KEY” y un botón que dice “COPY”. Hagan click ahí y peguénlo por algún notepad o similar, ya van a ver cómo vamos a poder utilizarlo. 3.3 Los datos: repositorio de datos del GCBA Nuestra materia prima para la introducción a estas herramientas estatales será el dataset espacial de espacios verdes de la Ciudad de Buenos Aires y la información sobre el trazado urbano de la Ciudad, también provisto por el GCBA. Estos datos pueden reemplazarse por otros datasets en caso que quieran replicar este análisis para otras ciudades. En particular para Argentina, es posible usar los radios censales, la mínima unidad espacial para un censo en Argentina, para reemplazar las manzanas. Por otro lado, los datasets de espacios verdes puede ser un poco más difíciles de encontrar, pero siempre existen esfuerzos para construir estos datasets que pueden encontrar haciendo búsquedas por Google Los datos de las manzanas se pueden descargar desde https://data.buenosaires.gob.ar/dataset/manzanas, mientras que los de espacios verdes desde https://data.buenosaires.gob.ar/dataset/espacios-verdes. En mi caso, yo descargué los geojson y los guardé en una carpeta que se llama data, pero pueden hacer lo que ustedes crean necesario! library(tidyverse) library(sf) library(hereR) library(leaflet) manzanas &lt;- st_read(&quot;data/manzanas.geojson&quot;) espaciosVerdes &lt;- st_read(&quot;data/espaciosVerdes.geojson&quot;) Veamos rápidamente qué es lo que cargamos con la ayuda de leaflet: leaflet(espaciosVerdes) %&gt;% addProviderTiles(providers$CartoDB.Positron) %&gt;% addPolygons() Parece que tenemos un conjunto de espacios urbanos de la ciudad de buenos aires, que es exactamente lo que queríamos. Igualmente vamos a hacer algunos cambios a este dataset, ya que vamos a quedarnos con espacios que sean lo suficientemente grande, de una manera muy arbitraria, como para poder ser aprovechados recreativamente: vamos a imponer que el espacio verde tenga al menos una superficie de 10.000 metros cuadrados, es decir una manzana. 3.4 Transformando nuestros datos Ahora que tenemos ya cargados nuestros datos espaciales, podemos transformarlos como para poder medir la cobertura de la oferta de espacio verde en la CABA. Para empezar, vamos a restringir espacios verdes a aquellos espacios verdes que tengan más de 1km2 de superficie total. Esto lo hacemos porque queremos medir de alguna manera el “uso” que le pueden dar a esos espacios, no solo si existe o no un lugar con zonas verdes. Es un criterio discutible, pero acá viene la mejor parte: cuando terminen este capítulo van a poder ir cambiando estos criterios. ¿Cómo podemos saber el tamaño de los polígonos de los espacios verdes? Esto es posible hacer siempre que tengamos nuestros datos espaciales cargados como un objeto sf. Para esto, primero vamos a proyectar en 2 dimensiones a los datos que cargamos, usando la proyección adaptada a la Ciudad de Buenos Aires: espaciosVerdes &lt;- st_transform(espaciosVerdes, crs = &quot;+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs&quot;) Una vez que hicimos esto, entonces ya estamos en condiciones de poder conocer el área de cada uno de los polígonos. La función que usamos es st_area(), pero debemos usar un punto, que hace referencia a “todo el dataset” dentro de tidyverse. Es decir, cuando usemos mutate, por ejemplo, el “.” hará referencia a todas las filas, es decir, a todos los polígonos de nuestro dataset. La función st_area() nos devuelve un tipo particular de datos: units, es decir que son datos con unidad (en este caso, metros cuadrados). Como esto ya lo sabemos, lo convertimos en un vector clásico con as.numeric() como para poder luego usar filter() y quedarnos solos con los que queríamos espaciosVerdes &lt;- espaciosVerdes %&gt;% mutate(area=st_area(.)) %&gt;% mutate(area=as.numeric(area)) %&gt;% filter(area&gt;10000) Probemos nuevamente con qué logramos quedaros: leaflet(espaciosVerdes %&gt;% st_transform(4326)) %&gt;% addProviderTiles(providers$CartoDB.Positron) %&gt;% addPolygons() Muy bien, ahora pensemos en un problema un poco más específico. Para medir la distancia entre dos puntos necesitamos… ¡puntos! Y nuestros dos datasets, manzanas y espacios verdes, son polígonos, tenemos que resolver esto de alguna manera. Podríamos tomar centroides de ambos casos pero tendríamos un problema muy importante: cuando las figuras son largas y/o grandes, el centroide no puede ser un buen indicador de punto de partida o llegada. La solución propuesta es solo medir el tiempo de viaje caminando desde los espacios verdes, pero desde un conjunto de puntos al azar que estén dentro de los polígonos, como para poder hacer más representativo el hecho de que al espacio verde se puede acceder desde distintos puntos. Si esto no queda del todo claro, vamos con un ejemplo bien sencillo. Enfoquémonos un segundo en el Parque Chacabuco. Para eso vamos a filtrarlo y convertirlo a ESPG 4326, que es lo que necesita leaflet para graficar parqueChacabuco &lt;- espaciosVerdes %&gt;% filter(nombre == &quot;Parque Chacabuco&quot;) %&gt;% st_transform(4326) leaflet(parqueChacabuco) %&gt;% addProviderTiles(providers$CartoDB.Positron) %&gt;% addPolygons() El parque es grande, si tomáramos una esquina, probablemente caminando 10 o 15 minutos estaríamos todavía dentro del mismo parque, y quizás arrancando desde el centro también… Retomemos este problema mientras aprendemos a pedir tiempo de viajes. 3.5 ¿Cuánto tardo en llegar al monumental? El equipo más grande de la Argentina tiene su estadio en Avenida Presidente Figueroa Alcorta 7597, en la Ciudad de Buenos Aires. Yo vivo por alguna zona de Palermo - vamos a mantener un poco la anonimidad - digamos Charcas 3591. Solo tengo esta información, pero me gustaría saber cuánto tiempo puedo tardar caminando, en auto o en bicicleta ¿Puedo hacerlo? Claró, puedo entrar a Google Maps y buscar las opciones. Pero acá vamos a hacerlo desde R y con el servicio de HERE Maps, lo cual después nos va a permitir llevar esto un paso más adelante y responder nuestro problema inicial. Lo primero que tenemos que hacer es generar un data frame con la información que sí tenemos: direcciones &lt;- data.frame(lugar=c(&quot;Casa&quot;,&quot;Monumental&quot;), direccion=c(&quot;Charcas 3591, Ciudad de Buenos Aires, Argentina&quot;,&quot;Avenida Presidente Figueroa Alcorta 7597, Ciudad de Buenos Aires, Argentina&quot;)) Luego, usamos la función de geocode() de hereR, que solo nos pide un vector con las direcciones a geolocalizar. Para esto, antes deben usar set_key() para que here maps sea capaz de asociar las consultas que hacemos desde R con nuestra cuenta en HERE Maps. # Esta key es falsa, por obvias razones. Reemplacen este valor por uno que funcione set_key(&quot;&lt;YOUR API KEY&gt;&quot;) ubicaciones &lt;- geocode(direcciones %&gt;% pull(direccion)) Si todo salió bien, ahora deberían tener un objeto ubicaciones con información espacial sobre nuestros datos… veamos si están bien georreferenciados leaflet(ubicaciones) %&gt;% addTiles() %&gt;% addMarkers() Si exploran un poco, van a ver que todo salió relativamente bien. Ahora lo que vamos a hacer es usar la función route(), la que se encarga de consultarle a HERE Maps cuánto tardamos entre el punto A y el B. En principio, solo necesita un origen, un destino y un modo de transporte, que en este caso va a ser “pedestrian”, es decir, cuánto tardamos caminando entre los dos puntos viaje &lt;- route(origin = ubicaciones[1,], destination = ubicaciones[2,], mode = &quot;pedestrian&quot;) glimpse(viaje) ## Rows: 1 ## Columns: 12 ## $ id &lt;dbl&gt; 1 ## $ departure &lt;dttm&gt; 2020-01-17 18:00:00 ## $ origin &lt;chr&gt; &quot;Charcas&quot; ## $ arrival &lt;dttm&gt; 2020-01-17 19:48:00 ## $ destination &lt;chr&gt; &quot;Avenida Presidente Figueroa Alcorta&quot; ## $ mode &lt;chr&gt; &quot;pedestrian&quot; ## $ traffic &lt;chr&gt; &quot;disabled&quot; ## $ distance &lt;int&gt; 6389 ## $ baseTime &lt;int&gt; 6480 ## $ travelTime &lt;int&gt; 6480 ## $ co2Emission &lt;dbl&gt; 0.928 ## $ geometry &lt;LINESTRING [°]&gt; LINESTRING (-58.41548 -34.5... El tiempo lo devuelve en la variable travelTime en segundos, por lo que dice que tardariamos 6480/60 = 108 minutos (una hora y 48 minutos) en hacer 6.389 metros (vean “distance”). Muy bien, ahora probablemente les haya aparecido un horario distinto al que aparece en la salida de este libro… y esto es porque por default hereR consulta por cuanto tardaría saliendo en el mismo instante de la consulta. No se preocupen, acá les explico como replicar lo que hice recién, y también puede serles útiles para medir otro tiempo de distancia. Solo tienen que cambiar la fecha y el horario y les va a funcionar correctamente viaje &lt;- route(origin = ubicaciones[1,], destination = ubicaciones[2,], mode = &quot;pedestrian&quot;, datetime = as.POSIXct(x = &quot;17/01/2020 18:00:00&quot;,format=&quot;%d/%m/%Y %H:%M:%S&quot;)) Medir tiempo de viaje en auto es muy similar, solo tienen que cambiar “pedestrian” por “car”, de la siguiente manera: viaje &lt;- route(origin = ubicaciones[1,], destination = ubicaciones[2,], mode = &quot;car&quot;, datetime = as.POSIXct(x = &quot;17/01/2020 18:00:00&quot;,format=&quot;%d/%m/%Y %H:%M:%S&quot;)) Bien, ahora estamos más cerca de la herramienta que finalmente vamos a usar: las isocronas. Esto que suena horrible es simplemente un polígono que delimita el espacio al cual se puede llegar en un tiempo fijo. Se trata de una aproximación, ya que nunca puede saberse exactamente cuál es este polígono, pero hagamos la prueba desde mi supuesto hogar: cuál es el polígono que podemos alcanzar caminando solo 15 minutos viajeCaminando &lt;- isoline(ubicaciones[1,],mode = &quot;pedestrian&quot;,range = 60*15) Ahora veamos qué es lo que nos devolvió: leaflet(viajeCaminando) %&gt;% addTiles() %&gt;% addPolygons() %&gt;% addMarkers(data=ubicaciones[1,]) Ese polígono nos muestra todos los lugares a los que podemos acceder en una caminata de 15 minutos desde Charcas 3591. Ahora bien, recuerden nuestro punto inicial: queremos medir la cobertura de los espacios verdes en la Ciudad de Buenos Aires. Algunos son muy grandes, entonces nos conviene conseguir esta isocrona para más de un punto dentro del mismo parque, de tal manera de poder capturar este efecto. Veamoslo, de nuevo, con el el ejemplo del Parque Chacabuco. 3.6 Midiendo la cobertura de los parques Si siguieron este capítulo correctamente, deberían tener un objeto parqueChacabuco con sus polígonos. Vamos a medir la cobertura mediante dos métodos alternativos: usando el centroide de los polígonos y tomando al azar 4 puntos del parque. Comparemos los resultados. Lo primero que tenemos que hacer, es reproyectar esos polígonos para poder tomar puntos al azar y también para tomar los centroides. En general, cuando realizamos esta clase de operaciones conviene tener a los datos espaciales proyectados en dos dimensiones parqueChacabuco &lt;- parqueChacabuco %&gt;% st_transform(&quot;+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs&quot;) Ahora ya podemos tomar el centroide, con la función st_centroid centroideChacabuco &lt;- st_centroid(parqueChacabuco) ## Warning in st_centroid.sf(parqueChacabuco): st_centroid assumes attributes are constant over geometries of x Y también cuatro puntos al azar, con la función st_sample set.seed(1) puntosChacabuco &lt;- st_sample(parqueChacabuco,size = 4) Como siempre, podemos ver muy fácilmente que es lo que acabamos de hacer, con la ayuda de leaflet leaflet() %&gt;% addTiles %&gt;% addPolygons(data=parqueChacabuco %&gt;% st_transform(4326)) %&gt;% addCircleMarkers(data=puntosChacabuco %&gt;% st_transform(4326), color=&#39;red&#39;) %&gt;% addCircleMarkers(data=centroideChacabuco %&gt;% st_transform(4326),color =&#39;yellow&#39;) Ahora midamos la isolinea de 15 minutos para esta forma de identificar al parque. Antes de so, tenemos que hacer algo con puntosChacabuco, porque no es un objeto sf ! miren class(puntosChacabuco) ## [1] &quot;sfc_POINT&quot; &quot;sfc&quot; Lo que tenemos que usar es st_as_sf(), que es la forma de decirle a R que queremos que ese objeto sea uno sf: puntosChacabucoSF &lt;- st_as_sf(puntosChacabuco) class(puntosChacabucoSF) ## [1] &quot;sf&quot; &quot;data.frame&quot; Ahora sí, ya estamos en condiciones de hacer lo que queríamos. Para el centroide es muy simple, usamos de nuevo isoline() y no tenemos mayores inconvenientes isoCronaCentroide &lt;- isoline(centroideChacabuco,mode = &quot;pedestrian&quot;,range = 60*15) Ahora bien, lamentablemente la función isoline() no hace automáticamente la operación para todas las filas. Eso ciertamente nos haría el trabajo más simple, pero no vamos a impedir que eso nos deje terminar el trabajo. Lo que vamos a usar es la función map() del paquete purrr. Lo que hace es hacer una función, la que queramos, para un conjunto de objetos que le digamos. Es muy general, por lo cual veamosla en funcionamiento: map(c(1:5),function(x) x+5) ## [[1]] ## [1] 6 ## ## [[2]] ## [1] 7 ## ## [[3]] ## [1] 8 ## ## [[4]] ## [1] 9 ## ## [[5]] ## [1] 10 Todas las funciones de map() tienen las dos cosas que les dije. En primer lugar, un conjunto de elementos a los cuales queremos hacerles una función en particular. En este caso, le pasamos los números que van desde el 1 al 5. Después, creamos una función que toma el el valor x y le suma 5. x en ese contexto significa cada uno de los valores que decíamos antes. Lo que devuelve es una lista con todos los valores que queríamos. Si no quedó del todo claro, no importa: ya va a quedar más claro con la experiencia. Apliquemoslo a este ejemplo entonces: isocronaPuntos &lt;- map(1:nrow(puntosChacabucoSF), function(x) { isoline(puntosChacabucoSF[x,],mode = &quot;pedestrian&quot;,range = 60*15) }) Fijense que lo que nos devolvió es una lista con 4 isolineas basadas en cada uno de los puntos que tomamos aleatoriamente anteriormente. Ahora nos queda juntarlos con rbind. Podemos hacerlo uno por uno, pero también podemos apoyarnos en do.call() que es muy similar a map(). La principal diferencia en este caso es que tenemos que pasarle la función que queremos que haga entre comillas, y luego la lista sobre la que queremos que lo haga. Nosotros queremos qu use rbind para todos los elementos de isocronaPuntos. isocronaPuntos &lt;- do.call(&quot;rbind&quot;,isocronaPuntos) Vemos que es lo que tenemos hasta ahora: leaflet(isocronaPuntos) %&gt;% addTiles() %&gt;% addPolygons() Podemos ver que las curvas se solapan quizás demasiado, y eso no es lo que buscamos. Entonces directamente lo que vamos a hacer es unir todo en un solo gran polígono, para hacer las cosas más fáciles. Esto se hace con st_union() isocronaPuntos &lt;- st_union(isocronaPuntos) Finalmente podemos comparar la cobertura con ambas metodologías: centroides y puntos. Veamos la diferencia: ggplot() + geom_sf(data=parqueChacabuco) + geom_sf(data=isoCronaCentroide, fill=&quot;red&quot;,alpha=0.1) + geom_sf(data=isocronaPuntos, fill=&quot;orange&quot;, alpha=0.2) Otra forma de verlo es sumar el área de cobertura de ambas alternativas: st_area(isoCronaCentroide) ## 1628300 [m^2] st_area(isocronaPuntos) ## 2702583 [m^2] Una diferencia más que importante en cobertura ! Ahora ya estamos en condiciones de hacer el ejercicio por el que veníamos: caracterizar la cobertura de los espacios verdes en la Ciudad de Buenos Aires 3.7 Redondeando: caracterizando la oferta de los espacios verdes en CABA Toda la larga discusión que tiene este capítulo puede resumirse en tan pocas líneas como las que siguen: # Para cada uno de los espacios verdes agarramos 4 puntos al azar puntosEspaciosVerdes &lt;- map(1:nrow(espaciosVerdes),function(x){ st_sample(espaciosVerdes[x,],size=4) }) # Juntamos todos los puntos en un objeto puntosEspaciosVerdes &lt;- do.call(&quot;c&quot;,puntosEspaciosVerdes) # Lo convertimos a un objeto SF puntosEspaciosVerdesSF &lt;- st_as_sf(puntosEspaciosVerdes) # Hacemos una transformación para que esté en el WSG84, que es lo que puede procesar isoline() puntosEspaciosVerdesSF &lt;- st_transform(puntosEspaciosVerdesSF,crs=4326) # Efectivamente calculamos las isocronas para cada uno de los puntos isocronosEspaciosVerdes &lt;- map(1:nrow(puntosEspaciosVerdesSF),function(x) { # Esta línea es solo para que nos vaya avisando qué está haciendo cat(&quot;Procesando: &quot;,x,&quot;\\r&quot;) isoline(puntosEspaciosVerdesSF[x,], mode = &quot;pedestrian&quot;,range_type = &quot;time&quot;,range = 60*15) }) # Los juntamos en el mismo data frame isocronosEspaciosVerdesJuntas &lt;- do.call(rbind,isocronosEspaciosVerdes) # Hacemos un gran poligono isocronosEspaciosVerdesJuntasUnion &lt;- st_union(isocronosEspaciosVerdesJuntas) Va a tardar un poco porque son 588 puntos para los cuales tiene que encontrar las icoronas… Pero si lo dejan correr, va a terminar de procesarlo. Podemos ver la cobertura que estimamos en un mapa de leaflet leaflet(isocronosEspaciosVerdesJuntasUnion %&gt;% st_transform(4326)) %&gt;% addTiles() %&gt;% addPolygons() Ahora simplemente tenemos que hacer un spatial join con st_join(). Recuerden que lo que hace esta función es unir a dos datasets según algún criterio de unión, siendo por default si se intersectan o no. En los casos que se intersecten, entonces va a agregar la información que se encuentra en el segundo dataset al primero. En caso de que no existe unión, ese valor para ese punto/polígono en particular será NA. Para hacer el spatial join vamos a proyectar a los dos datasets, convertimos como objeto sf a todo el espacio que identifica la cobertura de espacios verdes, y creamos una variable que identifique eso, y generamos efectivamente el spatial join. # Transformamos la proyección de las manzanas manzanas &lt;- manzanas %&gt;% st_transform(&quot;+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs&quot;) # Transformamos la proyección de la cobertura de espacios verdes isocronosEspaciosVerdesJuntasUnion &lt;- isocronosEspaciosVerdesJuntasUnion %&gt;% st_transform(&quot;+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs&quot;) # Lo convertimos en un objeto sf y creamos una columna, cobertura, que tendrá valor TRUE siempre isocronosEspaciosVerdesJuntasUnion &lt;- st_as_sf(isocronosEspaciosVerdesJuntasUnion) %&gt;% mutate(cobertura=TRUE) # Spatial join manzanas &lt;- st_join(manzanas,st_as_sf(isocronosEspaciosVerdesJuntasUnion)) # Completamos los datos para los casos en los cuales no hubo ningún resultadoe en el match manzanas &lt;- manzanas %&gt;% mutate(cobertura=ifelse(is.na(cobertura),FALSE,TRUE)) Hacemos el gráfico con ggplot, listo para exportar en caso que sea necesario con ggsave() ggplot(manzanas) + geom_sf(aes(fill=cobertura), color=NA) + theme_minimal() + coord_sf(datum=NA) + scale_fill_manual(values = c(&quot;#377eb8&quot;,&quot;#e41a1c&quot;), breaks = c(TRUE,FALSE), labels=c(&quot;Menos de 15 minutos&quot;,&quot;Más de 15 minutos&quot;)) Se puede mejorar y hacer el siguiente gráfico si usan ggmap() para agregar un mapa de base. Usamos getbb para que nos de la Bounding Box, es decir cuatro puntos, que definen todo un rectángulo dónde se puede ver la Ciudad de Buenos Aires library(osmdata) library(ggmap) bbCABA &lt;-getbb(&quot;Ciudad de Buenos Aires, Argentina&quot;) cabaBaseMap &lt;- get_stamenmap(bbCABA,maptype = &quot;toner-lite&quot;,zoom=12) ggmap(cabaBaseMap,extent = &quot;device&quot;) + geom_sf(data=manzanas %&gt;% st_transform(4326),aes(fill=cobertura), color=NA, inherit.aes=FALSE, alpha=0.7) + theme_minimal() + coord_sf(datum=NA) + labs(x=&quot;&quot;,y=&quot;&quot;) + scale_fill_manual(values = c(&quot;#377eb8&quot;,&quot;#e41a1c&quot;), breaks = c(TRUE,FALSE), labels=c(&quot;Menos de 15 minutos&quot;,&quot;Más de 15 minutos&quot;), name=&quot;&quot;)+ theme(legend.position = &quot;bottom&quot;) 3.8 Ejercicios ¿Cuánto tiempo tardás en llegar desde tu casa hasta la oficina de trabajo en auto? Usá hereR en R para poder sacar esa conclusión ¿Qué camino te sugirió? Podés descubrirlo haciendo un gráfico del objeto que devuelve la función route() Estimar la cobertura de espacios verdes en la ciudad de buenos aires, pero en lugar de usar 15 minutos como en el capítulo, usar 30 minutos ¿Qué zonas no están cubiertas con un parque a 30 minutos de caminata? El GCBA ofrece información sobre la ubicación geográfica de las (comisarias en la Ciudad)[!https://data.buenosaires.gob.ar/dataset/comisarias-policia-ciudad] ¿Hay zonas de la Ciudad en las cuales no hay acceso a una comisaria en menos de 10 minutos en auto, según HERE Maps? "],
["el-ritual-del-aprendizaje-automático-parte-1.html", "4 El ritual del aprendizaje automático (Parte 1) 4.1 Paso 1: Carga de los datos 4.2 Paso 2: análisis exploratorio y corrección de errores 4.3 Paso 3: crear nuevas variables (o feature engineering) 4.4 Paso 4: criterio de selección y optimización de los parámetros", " 4 El ritual del aprendizaje automático (Parte 1) En las notas de clases de Ciencia de Datos para Curiosos van a poder encontrar un capítulo dedicado a la introducción a las herramientas de Machine Learning, en particular los árboles de decisión, los árboles de regresión y sus implementciones en R. El objetivo de este capítulo es mostrar cómo pueden aplicarse estos conocimientos para una tarea de predicción asociada a un problema clásico: el precio de los inmuebles. Pero lo haremos de una manera secuencial, con el propósito de mostrar cuáles son los pasos más comunes y necesarios al momento de elegir el modelo, optimizar sus parámetros y comprender su capacidad predictiva. 4.1 Paso 1: Carga de los datos Puede sonar obvio y repetitivo, pero todo lo que hagan ustedes en análisis cuantitativos comienza con la carga de datos. Es importante que se sientan cómodos y cómodas como para poder leer los datos en R. En esta oportunidad vamos a usar los datos que provee la gente de Properati, disponible mediante consultas al servicio de Big Query de Google. Vamos a leer un archivo .RData que tiene 94.257 anuncios de inmuebles para la provincia de Córdoba entre el 2015 y 2020. Los archivos .RData son objetos de R que ya han sido cargados y exportados desde R como este formato específico. Estos archivos pueden ser útiles cuando queremos compartir algo con gente que sabemos que va a programar en R, ya que los pueden cargar directamente con load() y pesan realmente poco. Sin embargo, es importante aclarar que estos .RData “rompen” con la reproducibilidad: no podemos replicar exactamente cómo se llegó a esos datos, ni si hubo algún error en el preprocesamiento. En este caso, lo guardé de esta manera porque es una forma simple de compartir estos datos con ustedes, pero en la página de Properati van a encontrar incluso ejemplos sobre consultas en Big Query para consultar información en la que ustedes estén interesados y luego la pueden exportar como CSV o json, archivos que pueden leer sin mayores problemas a R. Carguemos los datos de la Provincia de Córdoba: load(url(&quot;https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/datosCordoba.RData&quot;)) 4.2 Paso 2: análisis exploratorio y corrección de errores Una vez que tenemos nuestro dataset cargado - deberían tener un objeto que se llama datosCordoba en la pestaña de “Environment” arriba a la derecha - lo que siempre debemos preguntarnos es qué es lo que realmente tenemos y detectar algunos patrones, que puede servirnos también para eliminar algunos outliers o errores en los datos. Carguemos nuestro aliado para - entre otras cosas - transformaciones de datos: tidyverse library(tidyverse) glimpse(datosCordoba) ## Rows: 94,257 ## Columns: 29 ## $ type &lt;chr&gt; &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Pr... ## $ type_i18n &lt;chr&gt; &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Propiedad&quot;, &quot;Pr... ## $ country &lt;chr&gt; &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Ar... ## $ id &lt;chr&gt; &quot;45yQ71tIpEQYvQqkqNO1sQ==&quot;, &quot;OnTwKOGAwjN3YAriuZwnvQ==&quot;, &quot;JBEwhyUNiuI9ellRHxvYxg==&quot;, &quot;rCd83Z/+R... ## $ start_date &lt;chr&gt; &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-0... ## $ end_date &lt;chr&gt; &quot;2015-03-26&quot;, &quot;2016-07-19&quot;, &quot;2015-04-16&quot;, &quot;2015-07-13&quot;, &quot;2015-12-16&quot;, &quot;2017-01-27&quot;, &quot;2016-03-0... ## $ created_on &lt;chr&gt; &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-07&quot;, &quot;2015-03-0... ## $ place.l1 &lt;chr&gt; &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Ar... ## $ place.l2 &lt;chr&gt; &quot;Córdoba&quot;, &quot;Córdoba&quot;, &quot;Córdoba&quot;, &quot;Córdoba&quot;, &quot;Córdoba&quot;, &quot;Córdoba&quot;, &quot;Córdoba&quot;, &quot;Córdoba&quot;, &quot;Córdo... ## $ place.l3 &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Punilla&quot;, &quot;Villa Allende&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Córdoba&quot;, &quot;&quot;, &quot;&quot;,... ## $ place.l4 &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;... ## $ place.l5 &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;... ## $ place.l6 &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;... ## $ place.lat &lt;dbl&gt; -31.41447, -31.34248, -31.34272, -31.41799, -31.41240, -31.42659, -31.41581, -31.48314, -31.40... ## $ place.lon &lt;dbl&gt; -64.19103, -64.28139, -64.30744, -64.17910, -64.21879, -64.19021, -64.19762, -64.15483, -64.18... ## $ property.operation &lt;chr&gt; &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Ven... ## $ property.operation_i18n &lt;chr&gt; &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Venta&quot;, &quot;Ven... ## $ property.type &lt;chr&gt; &quot;Departamento&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Departamento&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Casa&quot;... ## $ property.type_i18n &lt;chr&gt; &quot;Departamento&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Departamento&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Casa&quot;, &quot;Casa&quot;... ## $ property.rooms &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;7&quot;, &quot;3&quot;, &quot;1&quot;, &quot;3&quot;, NA, NA, NA, &quot;5&quot;, NA, NA, N... ## $ property.surface_covered &lt;chr&gt; &quot;35&quot;, &quot;170&quot;, &quot;250&quot;, &quot;96&quot;, &quot;68&quot;, &quot;55&quot;, &quot;130&quot;, &quot;375&quot;, &quot;172&quot;, &quot;176&quot;, &quot;98&quot;, &quot;73&quot;, &quot;130&quot;, &quot;39&quot;, &quot;50... ## $ property.price &lt;chr&gt; &quot;580000&quot;, &quot;1600000&quot;, &quot;350000&quot;, &quot;600000&quot;, &quot;1500000&quot;, &quot;1750000&quot;, &quot;1800000&quot;, &quot;380000&quot;, &quot;2000000&quot;,... ## $ property.currency &lt;chr&gt; &quot;ARS&quot;, &quot;ARS&quot;, &quot;USD&quot;, &quot;ARS&quot;, &quot;ARS&quot;, &quot;ARS&quot;, &quot;ARS&quot;, &quot;USD&quot;, &quot;ARS&quot;, &quot;ARS&quot;, &quot;ARS&quot;, &quot;ARS&quot;, &quot;ARS&quot;, &quot;AR... ## $ property.price_period &lt;chr&gt; &quot;Mensual&quot;, &quot;Mensual&quot;, &quot;Mensual&quot;, &quot;Mensual&quot;, &quot;Mensual&quot;, &quot;Mensual&quot;, &quot;Mensual&quot;, &quot;Mensual&quot;, &quot;Mensu... ## $ property.title &lt;chr&gt; &quot;(DG) VENTA 1 Dorm /CENTRO/ Escritura / Posesión Inm (DG)&quot;, &quot;ARGUELLO S/ALMARAZ - 3 DOR . VEND... ## $ property.description &lt;chr&gt; &quot;Duit Propiedades presenta este departamento en venta en el centro de la ciudad de Cordoba en ... ## $ property.bedrooms &lt;chr&gt; &quot;0&quot;, &quot;3&quot;, &quot;3&quot;, &quot;2&quot;, NA, &quot;2&quot;, &quot;3&quot;, &quot;7&quot;, &quot;3&quot;, &quot;3&quot;, &quot;2&quot;, &quot;4&quot;, &quot;3&quot;, &quot;1&quot;, NA, &quot;2&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;... ## $ property.bathrooms &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, NA, NA, &quot;1&quot;, &quot;3&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;1&quot;, NA, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2... ## $ property.surface_total &lt;chr&gt; &quot;45&quot;, &quot;330&quot;, &quot;960&quot;, &quot;155&quot;, &quot;261&quot;, &quot;60&quot;, &quot;160&quot;, &quot;1350&quot;, &quot;301&quot;, &quot;525&quot;, &quot;150&quot;, &quot;103&quot;, &quot;210&quot;, &quot;40&quot;... Nuestro dataset tiene 29 columnas. Quizás piensen que es una buena idea usar View() para tener una primera aproximación a esto, puede que no sea la mejor opción cuando trabajamos con muchos datos. Si quieren ir por ese camino, les recomiendo que usen View(head(100)) para ver las primeras 100 filas, pudiendo cambiar este valor para ajustarlo a sus necesidades. Otra opción consiste en apoyarnos en la descripción que vemos en enviroment. Podemos tener alguna idea de cuáles son las variables para las cuales querríamos ver los valores que toma, particularmente las variables categóricas. Las variables numéricas merecen otro tratamiento, ya que por su naturaleza pueden tomar distintos valores y es muy importante ver su distribución. Una opción con las variables categóricas es usar la función map(). Ya la hemos usado en capítulos anteriores: lo que hace es repetir una tarea que nosotros le decimos para cada uno de objetos que queremos. Si lo usamos con data.frame(), por default lo que hará map() es hacer algo para cada una de las columnas del data.frame. Entonces podemos seleccionar primero las columnas que queremos investigar, y luego pedirle que nos devuelva una tabla, que incluya la cantidad de casos falantes: datosCordoba %&gt;% select(type,type_i18n,country,place.l2,property.operation,property.operation_i18n,property.type,property.operation_i18n, property.currency, property.price_period) %&gt;% map(function(x) table(x,useNA = &quot;always&quot;)) ## $type ## x ## Propiedad &lt;NA&gt; ## 94257 0 ## ## $type_i18n ## x ## Propiedad &lt;NA&gt; ## 94257 0 ## ## $country ## x ## Argentina &lt;NA&gt; ## 94257 0 ## ## $place.l2 ## x ## Córdoba &lt;NA&gt; ## 94257 0 ## ## $property.operation ## x ## Alquiler Alquiler temporal Venta &lt;NA&gt; ## 14500 793 78964 0 ## ## $property.operation_i18n ## x ## Alquiler Alquiler temporal Venta &lt;NA&gt; ## 14500 793 78964 0 ## ## $property.type ## x ## Casa Casa de campo Cochera Departamento Depósito Local comercial Lote Oficina ## 27619 141 936 51638 353 3740 3359 2065 ## Otro PH &lt;NA&gt; ## 1906 2500 0 ## ## $property.currency ## x ## ARS CLP USD &lt;NA&gt; ## 724 49183 1 38844 5505 ## ## $property.price_period ## x ## Diario Mensual Semanal &lt;NA&gt; ## 12 79127 9 15109 Esto nos da una buena idea - y bastante visual - de qué datos son los que tenemos. Para este ejercicio vamos a hacer un modelo predicitivo sobre el valor de venta de los inmuebles para la Ciudad de Córdoba. Además vamos a pedir que los prediga en dólares, por lo que vamos a quedarnos con aquellos que están anunciados en dólares, aunque haya una minoría que se propiedades para la vente que se anuncian en pesos. En rigor, todavía no sabemos exactamente cuál es Córdoba capital. Por el momento, queremos quedarnos con anuncios para venta y que sea un departamento o casa y que estén denominados en USD datosCordoba &lt;- datosCordoba %&gt;% filter(property.currency == &quot;USD&quot; &amp; property.operation==&quot;Venta&quot; &amp; property.type %in% c(&quot;Casa&quot;,&quot;Departamento&quot;)) Ahora ya tenemos 33.148 observaciones pertenecientes a la provincia de Córdoba, que fueron publicados en dólares y que tenían como destino de operación una venta. Otra de la manipulación de datos relevante es el formato de las columnas (vectores). Cada uno de los vectores de nuestro data frame tiene un tipo de datos en particular. Algo que suele ser importante es estar seguros de que los datos que son numéricos están representados de tal manera, caso contrario no podremos hacer ninguna operación matemática, igual que, por ejemplo, lo que sucede en Excel cuando no tienen correctamente definido el tipo de datos. Para esto podemos aplicar el mismo concepto que usamos anteriorente, pero en este caso primero vamos a preguntar que tipo de vector es a aquellos que consideramos que deberían ser numéricos: datosCordoba %&gt;% select(property.rooms, property.surface_covered, property.price, property.surface_total, property.bathrooms, property.bedrooms) %&gt;% map(function(x) class(x)) ## $property.rooms ## [1] &quot;character&quot; ## ## $property.surface_covered ## [1] &quot;character&quot; ## ## $property.price ## [1] &quot;character&quot; ## ## $property.surface_total ## [1] &quot;character&quot; ## ## $property.bathrooms ## [1] &quot;character&quot; ## ## $property.bedrooms ## [1] &quot;character&quot; Las seis columnas que deberían ser numéricas, en realidad son de tipo caracter ¿Como podemos hacer para convertir todas a tipo numérico? Usemos mutate_at() para evitar tener que hacer un mutate para cada una de ellas. Lo que tenemos que pasarse es un vector con el nombre de las columnas a transformar, seguido por una función a realizar en cada una de ellas. En nuestro caso en particular, queremos que todas sean tipo numérico, así que usamos la función as.numeric(). Tengan en cuenta que nos pide el nombre de la función, no ejecutar la función, así que no hace falta pasarle los paréntesis: mutate_at() lo hará automáticamente de manera interna. # Primero convertimos nuestro data.frame en tibble(), solo para aprovechar que se ve mejor en la consola datosCordoba &lt;- datosCordoba %&gt;% as_tibble() datosCordoba &lt;- datosCordoba %&gt;% mutate_at(.vars = c(&quot;property.rooms&quot;, &quot;property.surface_covered&quot;, &quot;property.price&quot;, &quot;property.surface_total&quot;, &quot;property.bedrooms&quot;, &quot;property.bathrooms&quot;),as.numeric) Podemos chequear nuevamente la clase de las columnas de la misma manera que lo hicimos anteriormente, deberían ser numéricas: datosCordoba %&gt;% select(property.rooms, property.surface_covered, property.price, property.surface_total, property.bathrooms, property.bedrooms) %&gt;% map(function(x) class(x)) ## $property.rooms ## [1] &quot;numeric&quot; ## ## $property.surface_covered ## [1] &quot;numeric&quot; ## ## $property.price ## [1] &quot;numeric&quot; ## ## $property.surface_total ## [1] &quot;numeric&quot; ## ## $property.bathrooms ## [1] &quot;numeric&quot; ## ## $property.bedrooms ## [1] &quot;numeric&quot; Perfecto, ahora nos quedan dos tareas adicionales: la primera, es identificar el año de publicación del anuncio y la segunda es seleccionar correctamente cuáles son los inmuebles geolocaliazdos en la Ciudad de Córdoba. La primera parte es muy simple con la ayuda de la función substr(). Lo que hace es recortar el texto según las posiciones que nosotros le digamos. En este caso, queremos quedarnos con los primeros 4 caracteres de la columna created_on, que es la que tiene la fecha. datosCordoba &lt;- datosCordoba %&gt;% mutate(year=substr(created_on,start = 1,stop = 4)) Ahora para elegir aquellos inmuebles que están en Córdoba Capital tenemos dos alternativas. La primera es filtrar los casos que tienen el valor Córdoba enplace.l3, pero no estamos 100% de que ese sea el caso. Aprovechemos de que una gran cantidad de anuncios están geolocalizados y usemos esa información para identificarlos correctamente. El primer paso consiste en eliminar aquellos casos para los cuales no hay información sobre las coordenadas: datosCordoba &lt;- datosCordoba %&gt;% filter(!is.na(place.lat) &amp; !is.na(place.lon)) Luego,podemos convertir este objeto no espacial a uno espacial, usando st_as_sf(). No se olviden que antes debemos cargar el paquete sf library(sf) cordobaSF &lt;- st_as_sf(datosCordoba,coords = c(&quot;place.lon&quot;,&quot;place.lat&quot;),crs=4326) Con leaflet veamos si todo va bien, es decir si estos puntos parecen estar en la provincia de Córdoba library(leaflet) leaflet(cordobaSF) %&gt;% addTiles() %&gt;% addCircles() Parece que hay algunos problemas con algunos puntos localizados fuera de la Provincia de Córdoba. No se preocupen, esto suele ser algo normal. Lo que vamos a hacer es buscar el polígono de la Ciudad de Córdoba y buscar la intersección con los anuncios y quedarnos solo con los que estén dentro de ese polígono. Para esto vamos a usar al paquete osmdata y la función getbb() library(osmdata) polyCordoba &lt;- getbb(place_name = &quot;Córdoba Capital, Argentina&quot;,format_out = &quot;sf_polygon&quot;) Veamos qué es lo que levantó: leaflet(polyCordoba) %&gt;% addTiles() %&gt;% addPolygons() Este límite que nos muestra es el límite administrativo de la Ciudad de Córdoba, vamos a trabajar con él. Para encontrar cuáles puntos tienen una intersección vamos a proyectar a nuestros datos al EPSG 5343, una proyección oficial de Argentina (pueden usar st_crs() para encontrar más información sobre esta proyección) polyCordoba &lt;- st_transform(polyCordoba,5343) cordobaSF &lt;- st_transform(cordobaSF,5343) Y ahora ya podemos crear una columna en nuetro dataset espacial en base a si hay o no intersección. Tengan en cuenta que st_intersects() devuelve por default una matriz sparse, por eso ponemos sparse=FALSE para que nos devuelva una matriz lógica. Sin embargo, esta función esta preparada para que busquemos intersecciones entre más de un polígono, por lo que devuelve es una matriz de una columna, algo muy incómodo para nuestro dataset. Por eso le pedimos que lo convierta a un vector lógico con as.logical(), mucho más fácil para trabajar cordobaSF &lt;- cordobaSF %&gt;% mutate(cordobaCapital=as.logical(st_intersects(cordobaSF,polyCordoba,sparse=FALSE))) Inspeccionemos visualmente que todo ande bien usando leaflet. Recuerden que para usar leaflet debemos tener proyectados nuestros datos en el EPSG 4326, lo vamos a hacer dentro de la función de leaflet, en lugar de modificar nuestros objetos. También vamos a poner dos colores, green y red, en una nueva variable para mostrar correctamente cuál fue el resultado de la intersección. cordobaSF &lt;- cordobaSF %&gt;% mutate(colorCordoba = ifelse(cordobaCapital==TRUE,&quot;green&quot;,&quot;red&quot;)) leaflet(cordobaSF %&gt;% st_transform(4326)) %&gt;% addTiles() %&gt;% addCircles(color = ~colorCordoba) Todo parece estar en orden, ahora vamos a hacer una última limpieza de nuestros datos. Para empezar, vamos a quedarnos con aquellos puntos que están dentro de la Ciudad de Córdoba y las que tengan un valor de propiedad total mayor a cero. También vamos a quedarnos con los casos en los cuales la superficie total y la cubierta es mayor a cero, ya que se trata de algún error en los datos cordobaSF &lt;- cordobaSF %&gt;% filter(cordobaCapital==TRUE &amp; property.price&gt;0 &amp; property.surface_covered&gt;0 &amp; property.surface_total&gt; 0) Ahora vayamos a un tema muy importante: los datos faltantes. Los datos faltantes son siempre un tema que requiere una particular atención. Algunos modelos de aprendizaje automático/estadístico no tienen ningún problema para trabajar con datos faltantes en las variables explicativas, pero otros sí. Más allá de esto, precisamos saber cuáles variables tienen datos faltantes, particularmente aquellas variables que son cuantitativas. Podemos aprovechar esta oportunidad para aprander algo nuevo. Si map() nos devuelve una lista, map_dfr() nos devuelve un data.frame, lo cual es muy bueno para poder aplicar otras funciones como pivot_longer() o arrange(). De hecho, lo que hacemos en este ejemplo es justamente esto: vamos columna por columna calculando la cantidad de casos que tienen casos faltantes, después pasamos todo a formato largo y ordenamos de manera descendiente por “n_faltantes”, una columna que nos dice la cantidad de faltantes para cada una de las variables. cordobaSF %&gt;% map_dfr(function(x) sum(is.na(x))) %&gt;% pivot_longer(cols = 1:ncol(.),values_to=&quot;n_faltantes&quot;) %&gt;% arrange(desc(n_faltantes)) ## # A tibble: 31 x 2 ## name n_faltantes ## &lt;chr&gt; &lt;int&gt; ## 1 property.bedrooms 9032 ## 2 property.price_period 5368 ## 3 property.rooms 3897 ## 4 property.bathrooms 927 ## 5 type 0 ## 6 type_i18n 0 ## 7 country 0 ## 8 id 0 ## 9 start_date 0 ## 10 end_date 0 ## # ... with 21 more rows Tenemos faltantes en cuatro variables, siendo las más importantes la cantidad de dormitorios, la cantidad de ambientes y de baños. También debería aparecerles price_period() entre las variables con datos faltantes, pero no se preocupen, esa variable vamos a descartarla y no vamos a usarla para predecir el precio de los inmuebles Una opción con los datos faltantes es elegir modelos de aprendizaje automático que sepan trabajar con ellos, como por ejemplo el árbol implementado por el paquete rpart en R o quizás xgboost, otro modelo derivado de árboles. También pueden imputarse mediante distintos procedimientos, y de hecho random forest, el modelo que vamos a usar finalmente, brinda una función para imputar estos datos. Dejemos esto un poco es suspenso y continuemos por el paso 3. Ya vamos a levantarlo más adelante. 4.3 Paso 3: crear nuevas variables (o feature engineering) Una parte relevante para lograr entrenar modelos que predigan con alta precisión es generar nuevas variables en base a las que ya existen en nuestro dataset. En particular, el conjunto de datos con el que trabajamos nos deja crear distintos tipos de variables. En primer lugar, veamos el tema de los datos faltantes en las variables de ambientes, dormitorios y baños. Hasta ahora no nos fijamos en dos variables potencialmente muy útiles: propety.title y property.description. Se trata de dos variables que almacenan tanto el título como la descripción de los anuncios de los inmuebles. Veamos el título y la descripción del primer anuncio: cordobaSF %&gt;% slice(1) %&gt;% pull(property.description) ## [1] &quot;CODIGO: ubicado en: LA RUFINA - Publicado por: INMOBILIARIA REYNA NOVILLO. El precio es de USD 350000. EXCELENTE MUY LUMINOSA, AMBIENTES MODERNOS, 3 DORMITORIOS CON PLACARD, UNO EN SUITE CON VESTIDOR, LIVING COMEDOR AMPLIOS VENTANALES, ESCRITORIO, COCINA EQUIPADA, 3 BAÑOS, COCHERA DOBLE, PILETA, AMPLIO PATIO. 960 MTRS TERRENO, 250 MTRS CUBIERTOS. REYNA NOVILLO 4823436 155338655 - CONSULTE . Publicado a través de Mapaprop&quot; cordobaSF %&gt;% slice(1) %&gt;% pull(property.title) ## [1] &quot;LA RUFINA MODERNA MUY LUMINOSA - VENDO&quot; Si sabemos cómo encontrar patrones en estos textos quizás podemos recuperar algo de información. Agreguemos información sobre la existencia - o no - de un gimnasio, cochera o pileta. Para esto, vamos a usar str_detect(), una función que busca un patrón dentro de un texto y nos responde TRUE si aparece y FALSE si no aparece. Presten atención a la función regex() que está dentro del argumento pattern. Se trata de una función que nos ayuda a crear REGular EXpressions de una manera simple. En la primera parte escribimos específicamente lo que queremos buscar, y con el parámetro ignore_case le específicamos que ignore si es mayúscula o minúscula. cordobaSF &lt;- cordobaSF %&gt;% mutate(gimnasio = ifelse(str_detect(pattern = regex(&quot;gym|gimn&quot;, ignore_case = TRUE), string = property.description) | str_detect(pattern = regex(&quot;gym|gimn&quot;, ignore_case = TRUE), string = property.title), 1, 0), cochera = ifelse(str_detect(pattern = regex(&quot;coch|garage&quot;, ignore_case = TRUE), string = property.description) | str_detect(pattern = regex(&quot;coch|garage&quot;, ignore_case = TRUE), string = property.title), 1, 0), pileta =ifelse(str_detect(pattern = regex(&quot;pileta|piscina&quot;, ignore_case = TRUE), string = property.description) | str_detect(pattern = regex(&quot;pileta|piscina&quot;, ignore_case = TRUE), string = property.title), 1, 0)) En segundo lugar, vamos a intentar recuperar la información sobre la cantidad de dormitorios con la función str_extract(). La regex que vamos a usar es un poco más compleja: (\\d)(?= dorm). Lo que está en el primer paréntesis (\\d) quiere decir “dígitos” y lo que está entre los otros paréntesis (?= dorm) quiere decir “seguido por dorm”. Por ejemplo, si dentro de la descripción se encontrara \" 4 dormitorios“, str_extract() nos devolvería”4\". Pero antes de hacer esto, vamos a reemplazar los valores uno, dos y tres por los números correspondientes, para que pueda encontrarlo. Esto lo hacemos con str_replace_all() cordobaSF &lt;- cordobaSF %&gt;% mutate(property.description=str_replace_all(string = property.description, pattern = regex(&quot;un|uno&quot;,ignore_case = TRUE), &quot;1&quot;), property.description=str_replace_all(string = property.description, pattern = regex(&quot;dos&quot;,ignore_case = TRUE), &quot;2&quot;), property.description=str_replace_all(string = property.description, pattern = regex(&quot;tres&quot;,ignore_case = TRUE), &quot;1&quot;)) %&gt;% mutate(ambientes=str_extract(pattern=regex(&quot;(\\\\d)(?= dorm)&quot;,ignore_case = TRUE),string = property.description)) Para conocer más sobre cómo trabajar con texto en R pueden revisar la cheat sheet de stringr. Ya nos quedan los últimos pasos antes de poder entrenar nuestro modelo. Vamos a quedarnos con las variables que vamos a usar: cordobaSF &lt;- cordobaSF %&gt;% select(place.l4,ambientes,property.surface_covered,property.price,property.surface_total,year,gimnasio,cochera,pileta) Por otro lado, vamos a agregar la información sobre la latitud y longitud de la ubicación de los inmuebles. Si es importante, nuestro modelo debería usarla para hacer mejores predicciones. Tendremos más para decir sobre esto más adelante. Para recuperar las coordenadas podemos usar st_coordinates anunciosCoords &lt;- st_coordinates(cordobaSF) %&gt;% as.data.frame() Esta es una forma de incoporar el factor espacial en nuestros modelos predictivos. Otra forma de agregar información espacial es calcular el valor de un conjunto de vecinos e imputar su precio. Esta es también una idea importante para el análisis estadístico de los datos espaciales, algo que se trata más adelante en este libro. Para crear una variable que capture el valor promedio de los vecinos, primero tenemos que definir a los vecinos. En este caso, vamos a identificar como vecinos a todos aquellos puntos que estén a menos de 500 metros de cada uno de los anuncios, usando la función st_is_within_distance(), que nos devuelve una lista con los puntos de que cumplen esa condición, para cada uno de nuestros anuncios. listaVecinos &lt;- st_is_within_distance(cordobaSF,cordobaSF,dist = 500) # Vemos el primer elemento listaVecinos[[1]] ## [1] 1 1008 2178 2179 3234 9983 11773 12059 17573 Esta lista, aunque aparezca como un objeto sgbp podemos trabajarla como si fuera una lista normal con la función map(). Fijénse que el primer elemento tiene un vector númerico, que no es otra cosa que el número de fila, del data.frame cordobaSF, que son vecinos de la fila 1. Con un pequeño problema: st_is_within_distance() incluye al mismo punto como vecino, ya que efectivamente está a menos de 500 metros de sí mismo. Sería incorrecto incluirlo para calcular el precio por metro cuadrado de los vecinos, ya que está justamente relacionado con el precio que queremos calcular. # Creamos datosCordoba, un dataset que ya no es más espacial datosCordoba &lt;- cordobaSF %&gt;% st_set_geometry(NULL) # Agregamos una columna que tenga el precio metro cuadrado datosCordoba &lt;- datosCordoba %&gt;% mutate(precioM2=property.price/property.surface_covered) # Recorremos cada uno de los valores y calculamos el valor mediano por metro cuadrado de los vecinos valorVecinos &lt;- map(1:length(listaVecinos),function(x) { # Esta linea elimina como vecino a la misma fila para la cual estamos calculando el precio promedio vecinos &lt;- listaVecinos[[x]][!listaVecinos[[x]] %in% x] datosCordoba %&gt;% slice(vecinos) %&gt;% summarise(promedio=quantile(precioM2, 0.5)) }) %&gt;% bind_rows() # Ya podemos unir estos datos a los anteriores datosCordoba &lt;- datosCordoba %&gt;% mutate(precioM2Vecino = valorVecinos %&gt;% pull(promedio)) También podemos agregar las coordenadas que guardamos anteriormente en anunciosCoords datosCordoba &lt;- cbind(datosCordoba,anunciosCoords) Finalmente, vamos a necesitar que las variables que tenemos como character sean factores para que nuestro modelo más adelante pueda entrenarse sin problemas. Usamos la misma lógica que hicimos antes con los datos numéricos. Además, vamos a elimnar los casos en los cuales tenemos NA en la variable ambientes y en el valor del precio cuadrado de los vecinos datosCordoba &lt;- datosCordoba %&gt;% filter(!is.na(property.surface_covered)) %&gt;% mutate_at(.vars = c(&quot;place.l4&quot;,&quot;year&quot;),as.factor) datosCordoba &lt;- datosCordoba %&gt;% filter(!is.na(ambientes) &amp; !is.na(precioM2Vecino)) 4.4 Paso 4: criterio de selección y optimización de los parámetros Todos los pasos anteriores sirvieron para tener en nuestros datos un conjunto de variables que pensamos que pueden ser útiles para predecir el precio de venta de los inmuebles. En el capítulo introductorio al aprendizaje automático de Ciencia de Datos Para Curiosos vimos cómo debemos elegir un modelo y, luego, optimizar los parámetros. Vamos a usar un modelo de árboles muy conocido: random forests. Se basan en la idea de bagging, en la cual se entrenan muchos árboles que aprenden “demasiado”, pero que son muy especializados en una sección de nuestros datos. Luego, para predecir un caso nuevo se promedian las predicciones de todos los árboles y ese es el valor final. Podríamos hacer una analogía y decir que en este modelo necesitamos tener un conjunto elevado de personas con un conocimeinto muy específico a las cuales les presentamos un caso que tienen que clasificar. Todas las personas harán en base al conocimiento que tienen, nosotros promediamos la opinión y ese será el resultado final. Recuerden que lo que siempre nos importa en los modelos de aprendizaje automático es predecir a casos nuevos sobre los que no hayamos entrenado nuestro modelo. El objetivo al final del día es aproximar la relación que existe entre el conjunto de variables explicativas y aquella que queremos predecir para todos los casos posibles, y no solo para la muestra que tenemos. En el siguiente capítulo trabajaremos con tidymodels() para este último paso, que tiene distintos componentes y requiere una explicación más pormenroizada. Cerremos este capítulo midiendo de alguna manera el impacto de todo nuestro trabajo en la creación de nuevas variables. Estimemos un modelo de regresión lineal usando solo los datos originales, y luego agreguemos los nuestros y comparemos el cambio en una medida de ajuste, el R2 y también el RMSE de estos modelos usando el paquete yardstick library(yardstick) # Regresión con los datos iniciales regresionOriginal &lt;- lm(formula = property.price ~ place.l4 + property.surface_covered + property.surface_total + year, data = datosCordoba) # rsq_vec calcula el R2 con un vector de predicciones y otro de valores reales rsq_vec(truth = predict(regresionOriginal), estimate = datosCordoba %&gt;% pull(property.price)) ## [1] 0.05277268 # rmse_vec calcula el RMSE con un vector de predicciones y otro de valores reales rmse_vec(truth = predict(regresionOriginal), estimate = datosCordoba %&gt;% pull(property.price)) ## [1] 197389.8 # Regresión con los datos nuevos regresionVariablesNuevas &lt;- lm(formula = property.price ~ place.l4 + ambientes + property.surface_covered + property.surface_total + year + gimnasio + cochera + pileta + precioM2Vecino + X + Y, data = datosCordoba) # R2 rsq_vec(truth = predict(regresionVariablesNuevas), estimate = datosCordoba %&gt;% pull(property.price)) ## [1] 0.1563373 #RMSE rmse_vec(truth = predict(regresionVariablesNuevas), estimate = datosCordoba %&gt;% pull(property.price)) ## [1] 186286.8 Como podemos ver en este simple modelo, solo agregar nuevas variables aumentó el R2 de nuestro modelo del 5,3% al 15.6% y redujo el RMSE un 5.6%. Veremos más sobre qué tan relevantes son estas variables en la segunda parte de esta sección. Quizás quieran ver, también, cual es la distribución de los errores absolutos de predicción de ambos modelos # Modelo original errorAbsolutoOriginal &lt;- round(abs(predict(regresionOriginal)-datosCordoba %&gt;% pull(property.price) ),0) quantile(errorAbsolutoOriginal) ## 0% 25% 50% 75% 100% ## 20.0 27365.0 58665.5 94776.5 9799951.0 # Modelo con variables agregadas errorAbsolutoOriginal &lt;- round(abs(predict(regresionVariablesNuevas)-datosCordoba %&gt;% pull(property.price) ),0) quantile(errorAbsolutoOriginal) ## 0% 25% 50% 75% 100% ## 15.0 19350.5 41431.5 77196.5 9837328.0 "],
["el-ritual-del-aprendizaje-automático-parte-2.html", "5 El ritual del aprendizaje automático (Parte 2) 5.1 Separando dataset de entrenamiento y de testing 5.2 Nuestra receta y su preparación 5.3 Selección de modelo y espacio de búsqueda de los parámetros 5.4 Entrenando los modelos en nuestro espacio de búsqueda 5.5 Seleccionando el mejor modelo y analizando los resultados 5.6 Importancia de las variables", " 5 El ritual del aprendizaje automático (Parte 2) En el capítulo anterior dejamos pendiente el último tramo en un típico proyecto para predecir o clasificar usando modelos de aprendizaje automático: la optimización de parámetros y selección de modelo. Esta parte suele consistir en establecer uno o más criterios de selección y probar distintas combinaciones de parámetros de nuestros modelos para ver cuál es el que mejor predice nuevos datos. Adicionalmente, vamos a investigar un poco cómo podemos obtener información sobre las variables más importante para los modelos. Para este capítulo vamos a usar las funciones de los paquetes que componen tidymodels, que buscan simplificar muchas etapas de este último paso de optimización de parámetros y selección de modelo, aunque también brinda funciones para incoporporar el resto de las etapas que estuvimos haciendo en el capítulo anterior. Ya quedará todo más claro hacia el final de este capítulo. La figura que se muestra debajo de este párrafo muestra, en naranja, las tareas que haremos en este capítulo y que completan una parte muy importante de un proyecto de ciencia de datos en el cual se usan modelos de aprendizaje automático para tareas de predicción o clasificación. Noten que lo que tenemos que hacer es optimizar los parámetros y, en base a alguna medida de performance, seleccionar el mejor modelo e intentar explorar la importancia de las variables y resumir su capacidad predictiva ¿Ven esas flechas de ida y vuelta entre la creación de nuevas variables y la optimización de los parámetros? Muchas veces creamos nuevas variables o modificamos las existentes para mejorar alguno de los resultados que nos arroja la optimización de los parámetros. En este caso no vamos a hacerlo, pero vale la pena aclararlo. Finalmente, para lo que sigue de este capítulo vamos a tener que tener instalado el paquete tidymodels y cargados los datos del capítulo anterior: library(tidyverse) library(tidymodels) # Carga de datos datosCordoba &lt;- read_delim(&quot;https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/datosMachineLearning2.csv&quot;, delim=&quot;;&quot;) %&gt;% select(-precioM2) 5.1 Separando dataset de entrenamiento y de testing En tidymodels podemos separar a los datos en un conjunto de entrenamiento y de testing de una manera muy simple usando la función initial_split(). Recuerden que esto es necesario porque nos interesa conocer la capacidad predictiva de nuestros modelos cuando le presentamos datos con los que NO se entrenaron, no sobre aquellos con los que se entrenó. Esto es así porque si seleccionamos un modelo en base a qué tan bien ajusta a los datos con los que entrena podemos caer en el territorio del overfitting, es decir, cuando nuestro modelo se aprende “de memoria” los datos y extrae una mala aproximación de la relación que existe entre las variables, haciendo muy malas predicciones out-of-sample, es decir en nuevos datos. # Esta primer línea es solo para que separemos los mismos casos en training y testing set.seed(10) # Guardamos un 10% de los datos para después de haber entrenado el modelo cordobaSplit &lt;- initial_split(datosCordoba,prop = 0.7) Con el parámetro prop indicamos cuál es la proporción de los datos que queremos que nos quede en el dataset de entrenamiento. En este caso, elegimos que el 70% de todos nuestros datos sean utilizados para entrenar a nuestro modelo de aprendizaje automático. Pueden verlo usando el método de print() de cordobaSplit: cordobaSplit ## &lt;Analysis/Assess/Total&gt; ## &lt;10653/4565/15218&gt; Aunque los nombra un poco distinto, lo que nos dice es que hay 10.653 observaciones para entrenar a nuestro modelo y 4.565 para evaluar su capacidad predictiva sobre nuevos datos. cordobaSplit es un objeto de rsplit, para efectivamente tener a los data.frames podemos usar las funciones training() y testing() que nos generan estos datasets a partir del split. Presten atención al largo de cada uno de estos objetos. cordobaTrain &lt;- training(cordobaSplit) cordobaTesting &lt;- testing(cordobaSplit) 5.2 Nuestra receta y su preparación En el framework de tidymodels, existen recetas (recipes) que escribimos para nuestros modelos. Pueden incluir transformaciones a los datos y formulas de predicción, entre otras cosas. En este caso, ya hicimos toda la transformación en el capítulo anterior, así que lo que vamos a marcar simplemente la fórmula de lo que queremos predecir: el precio de las propiedades. Como siempre, pueden usar el menú de ayuda de R , ?recipe en este caso, para conocer específicamente lo que podemos incluir en esta función. cordobaRecipe &lt;- recipe(formula = property.price ~., data=cordobaTrain) cordobaRecipe ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 10 Como podemos ver, creamos una “Data Recipe”, y lo que nos indica es que existe 1 variable que queremos predecir y 10 predictores. Ahora, debemos indicar que queremos “preparar” esta receta, es decir evaluar si esta receta puede ser aplicada a nuestros datos y dejarla lista para los siguientes pasos. treePrep &lt;- prep(cordobaRecipe) treePrep ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 10 ## ## Training data contained 10653 data points and no missing data. 5.3 Selección de modelo y espacio de búsqueda de los parámetros Ahora lo que nos queda para definir es el modelo a estimar, y esto requiere el uso de algunas funciones debido a que existen diversas decisiones que tenemos que hacer. La primera de ellas es básicamente elegir el modelo con el queremos trabajar. Ya hemos aclarado anteriormente que vamos a usar random forest, un modelo basado en árboles y bagging, es decir la creación de muchos árboles sumamente especializados que luego hacen una predicción conjunta. Podemos elegir este tipo de modelos con rand_forest(). Dentro de esta función podemos especificar los valores de los parámetros y, más importante, aclarar cuales son los que queremos “tunear”, es decir, optimizar. Esto lo declaramos con la función tune(). En el caso de los random forests, dos de los parámetros que suelen optimizarse son mtry y min_n. El primero determina la cantidad de variables que, al azar, pueden usarse en cada uno de los nodos de los árboles. Recuerden que random forest no elige a todos los predictores en cada nodo, sino que solo candidatea al azar a una cantidad fija de columnas, y esto es lo que marcamos con mtry. Por otro lado, min_n establece una cantidad mínima de valores que tiene que haber en un nodo como para poder seguir profundizando el árbol. Por otro lado, con la función set_mode() debemos identificar si queremos hacer una regressión (para predecir valores númericos) o una clasificación (para predecir valores categóricos). Finalmente, en set_engine indicamos el paquete que tiene la implementación de random forest. Existen muchas librerías en R para hacerlo, vamos a usar la que brinda el paquete ranger tuneSpec &lt;- rand_forest( mtry = tune(), trees = 1000, min_n = tune() ) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;ranger&quot;) tuneSpec ## Random Forest Model Specification (regression) ## ## Main Arguments: ## mtry = tune() ## trees = 1000 ## min_n = tune() ## ## Computational engine: ranger Ya tenemos la especificación de nuestro modelo. Ahora nos queda establecer un workflow, que no es otra cosa que un conjunto de instrucciones secuenciales que hay que hacer para entrenar los modelos. En este caso, le decimos que tome la receta ya preparada que declaramos anteriormente, y luego entrene el modelo que especificamos recién. tuneWf &lt;-workflow() %&gt;% add_recipe(treePrep) %&gt;% add_model(tuneSpec) tuneWf ## == Workflow ==================================================================== ## Preprocessor: Recipe ## Model: rand_forest() ## ## -- Preprocessor ---------------------------------------------------------------- ## 0 Recipe Steps ## ## -- Model ----------------------------------------------------------------------- ## Random Forest Model Specification (regression) ## ## Main Arguments: ## mtry = tune() ## trees = 1000 ## min_n = tune() ## ## Computational engine: ranger Ahora lo que vamos a hacer es crear conjuntos de entrenamiento para realizar cross validation o validación cruzada. Este concepto que puede sonar complejo es realmente simple. Lo que hacemos es cortar nuestros datos al azar grupos de igual tamaño. Para cada combinación de parámetros, entrenamos el modelo en todos los grupos menos uno, y predecimos sobre el que no entrenamos. Promediamos el valor de todos los errores y ese es el valor por el cual vamos a juzgar la performance de nuestro modelo. El gráfico que sigue es muy ilustrativo sobre cómo funciona Podemos crear estos grupos, en base a nuestro conjunto de entrenamiento, con la función vfold_cv() # Hacemos esto para que tengamos los mismos grupos set.seed(234) # Creamos los grupos trees_folds &lt;- vfold_cv(cordobaTrain, v = 5) Ya casi estamos listos para entrenar los modelos. Lo que nos falta es fundamental: específicar cual es el espacio de búsqueda de nuestros parámetros. Hay muchas formas de hacer esto en tidymodels, pero lo que vamos a usar en esta clase es grid_regular(). Le pasamos un valor mínimo y máximo de valores posibles para cada uno de los parámetros que queremos optimizar, y luego una cantidad de valores únicos para cada uno de los parámetros. Obviamente, cómo mtry va entre 2 y 8, vamos a tener solo 7 valores, pero en el caso de min_n tomára 10 valores espaciados entre 5 y 200. Pueden ver por ustedes mismos la grilla, ya que es simplemente un tibble. rf_grid &lt;- grid_regular( mtry(range = c(2,8)), min_n(range = c(5,200)), levels=10 ) 5.4 Entrenando los modelos en nuestro espacio de búsqueda Entrenar 70 modelos con cross validation de 5 grupos implica entrenar 5x70 = 350 modelos… esto puede llevar un buen rato. Usemos al paquete doParallel para aprovechar el procesamiento en paralelo en R. Pueden hacerlo simplemente en una línea: doParallel::registerDoParallel() Ahora ya podemos entrenar todos los parámetros de nuestra grilla con la función tune_grid(). Lo único que hacemos es pasarle objetos que ya creamos anteriormente. Tengan paciencia cuando corran este código, que puede tardar un rato. set.seed(345) resultadoBusqueda &lt;- tune_grid( tuneWf, resamples=trees_folds, grid=rf_grid ) Como siempre, dejo a disposición el resultado de este proceso para que no tengan que esperar a correrlo para entender qué es lo que hicimos. load(url(&quot;https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/resultadoBusqueda.RData&quot;)) resultadoBusqueda ## # 5-fold cross-validation ## # A tibble: 5 x 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [8.5K/2.1K]&gt; Fold1 &lt;tibble [140 x 5]&gt; &lt;tibble [0 x 1]&gt; ## 2 &lt;split [8.5K/2.1K]&gt; Fold2 &lt;tibble [140 x 5]&gt; &lt;tibble [0 x 1]&gt; ## 3 &lt;split [8.5K/2.1K]&gt; Fold3 &lt;tibble [140 x 5]&gt; &lt;tibble [0 x 1]&gt; ## 4 &lt;split [8.5K/2.1K]&gt; Fold4 &lt;tibble [140 x 5]&gt; &lt;tibble [0 x 1]&gt; ## 5 &lt;split [8.5K/2.1K]&gt; Fold5 &lt;tibble [140 x 5]&gt; &lt;tibble [0 x 1]&gt; resultadoBusqueda tiene para cada uno de nuestros grupos de cross validation el resultado de RMSE y R2, dos indicadores que pueden ser utilizados para medir la capacidad predicitiva de nuetro modelo. Nosotros necesitamos el promedio de estos grupos, y para eso usaremos collect_metrics() metricasPerformance &lt;- collect_metrics(resultadoBusqueda) glimpse(metricasPerformance) ## Rows: 140 ## Columns: 7 ## $ mtry &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ... ## $ min_n &lt;int&gt; 5, 5, 26, 26, 48, 48, 70, 70, 91, 91, 113, 113, 135, 135, 156, 156, 178, 178, 200, 200, 5, 5, 26, 26, 48, 48... ## $ .metric &lt;chr&gt; &quot;rmse&quot;, &quot;rsq&quot;, &quot;rmse&quot;, &quot;rsq&quot;, &quot;rmse&quot;, &quot;rsq&quot;, &quot;rmse&quot;, &quot;rsq&quot;, &quot;rmse&quot;, &quot;rsq&quot;, &quot;rmse&quot;, &quot;rsq&quot;, &quot;rmse&quot;, &quot;rsq&quot;, &quot;rm... ## $ .estimator &lt;chr&gt; &quot;standard&quot;, &quot;standard&quot;, &quot;standard&quot;, &quot;standard&quot;, &quot;standard&quot;, &quot;standard&quot;, &quot;standard&quot;, &quot;standard&quot;, &quot;standard&quot;, ... ## $ mean &lt;dbl&gt; 1.704274e+05, 3.887610e-01, 1.707934e+05, 3.871747e-01, 1.714937e+05, 3.835223e-01, 1.722247e+05, 3.793304e-... ## $ n &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ... ## $ std_err &lt;dbl&gt; 4.133305e+04, 1.232524e-01, 4.101059e+04, 1.210491e-01, 4.084192e+04, 1.198834e-01, 4.065547e+04, 1.186181e-... Se trata de un dataset con 140 filas, dado que entrenamos 70 modelos y tenemos 2 indicadores de performance para cada uno de ellos. Refrescemos un poco la memoría sobre qué es lo que hace cada uno de ellos. Por un lado, la raíz del error cuadrático medio (RMSE, por su sigla en inglés) tiene la siguiente fórmula \\[ \\color{#E7298A}{RMSE} = \\sqrt{(1/\\color{olive}{n})\\sum_{i=1}^{\\color{olive}{n}}(\\color{purple}{y_i} - \\color{orange}{\\hat{f}(x_i)})^2}=\\] Donde \\(\\color{olive}{n}\\) es la cantidad de observaciones, \\(\\color{purple}{y_i}\\) es el valor observado para la observación i; y \\(\\color{orange}{\\hat{f}(x_i)}\\) es el valor predicho dada la aproximación y los valores de las variables para la observación i. En otras palabras, el RMSE es cuanto nos confundimos en promedio al predecir los datos. Por otro lado, el R2 o “Rsquared” puede explicitarse de la siguiente manera: \\[ \\color{#e41a1c}{R^2} = 1 - \\frac{\\color{#377eb8}{SS_{res}}} {\\color{#4daf4a}{SS_{tot}}} \\], donde \\(\\color{#377eb8}{SS_{res}}\\) es la suma al cuadrado de los residuos, es decir \\(\\color{#377eb8}{SS_{res}} = \\sum_{i}^{N}(y_i-\\hat{y})^2\\): cuanto de la variabilidad queda plara explicar luego de que hacemos la predicción, mientras que \\(\\color{#4daf4a}{SS_{tot}} = \\sum_{i}^{N}(y_i-\\bar{y})^2\\), es decir la diferencia entre el valor observado y el promedio. Estas son dos de las métricas más utilizadas para seleccionar los modelos. Veamos cómo dan los resultados para la combinación de los parámetros que hemos usado: ggplot(metricasPerformance) + geom_point(aes(x=mtry,color=factor(min_n),y=mean)) + facet_wrap(~ .metric,scales = &quot;free&quot;) En ambos casos podemos ver que el RMSE mínimo y el R2 máximo se alcanzan con los parámetros min_n = 5 y mtry = 2. Además podemos ver que cuanto más aumenta mtry, el modelo empeora, señalando una de las ventajas de random forest. Parece contrar un mínimo cercano al valor 2 o 3. 5.5 Seleccionando el mejor modelo y analizando los resultados Ya estamos en condiciones de elegir el mejor modelo y medir su capacidad predictiva en el conjunto de testing - o validación, en rigor - que separamos al principio de la clase. Para elegir el mejor modelo tenemos que usar select_best(), función a la cual solo tenemos que pasarle el resultado de la búsqueda de parámetros y una métrica para elegir al modelo: select_best(resultadoBusqueda, metric=&quot;rmse&quot;) ## # A tibble: 1 x 2 ## mtry min_n ## &lt;int&gt; &lt;int&gt; ## 1 2 5 Nos dice algo que ya podíamos ver anteriormente, que el mejor modelo dentro de los que buscamos es el que tiene mtry 2 y min_n 5. Veamos si r2 elige al mismo modelo: select_best(resultadoBusqueda, metric=&quot;rsq&quot;) ## # A tibble: 1 x 2 ## mtry min_n ## &lt;int&gt; &lt;int&gt; ## 1 2 5 Ambos criterios eligen exactamente al mismo modelo. Ahora estimamos ese modelo para esos parámetros, será nuestro modelo final con el que probaremos la capacidad predictiva sobre los datos nuevos. Para seleccionar al modelo ganador solo tenemos que usar finalize_model() rfWinner &lt;- finalize_model( tuneSpec, select_best(resultadoBusqueda, metric=&quot;rmse&quot;) ) Y ya podemos entrenar al modelo con esos parámetros sobre todos los datos de training set.seed(456) modeloGanador &lt;-rfWinner %&gt;% set_engine(&quot;ranger&quot;) %&gt;% fit(property.price ~., data=cordobaTrain) Y predecir sobre nuevos datos # R2 rsq_vec(truth = predict(modeloGanador,new_data = cordobaTesting) %&gt;% pull(.pred), estimate = cordobaTesting %&gt;% pull(property.price)) ## [1] 0.6737393 #RMSE rmse_vec(truth = predict(modeloGanador,new_data = cordobaTesting) %&gt;% pull(.pred), estimate = cordobaTesting %&gt;% pull(property.price)) ## [1] 90609.13 5.6 Importancia de las variables Cerremos este ejercicio teniendo alguna idea de la importancia que tienen cada una de las variables en nuestro modelo ganador. Usemos el paquete vip para esta tarea. Fijense que tenemos que elegir el método para elegir la importancia de las variables, algo que está fuera del objetivo de este libro, aunque merece una breve introducción. “Permutation” lo que hace es calcular la contribución promedio de una variable a la capacidad predictiva del modelo comparando el MSE (Mean squared error) obtenido cuando se “mezcla” la variable con respecto al valor cuando se usa como se observa en los datos. La idea es que si una variable no es muy importante, entonces la diferencia en la capacidad predictiva de la variable observada y de la “mezcla” debería ser baja y viceversa. library(vip) set.seed(456) modeloGanador &lt;-rfWinner %&gt;% set_engine(&quot;ranger&quot;, importance=&quot;permutation&quot;) %&gt;% fit(property.price ~., data=cordobaTrain) vip(modeloGanador,geom=&quot;point&quot;) ¿Qué observamos? Que la información espacial importa para este modelo. Más allá de que las primeras dos variables más importantes para explicar este modelo son property.surface_covered y property.surface_total, las variables Y, precioM2Vecino y X son la tercera, cuarta y quinta variable más importantes según esta medida de importancia. "],
["mapas.html", "6 Mapas 6.1 Trabajando con el censo 2010 6.2 Haciendo visualizaciones 6.3 Leaflet 6.4 Ejercicios", " 6 Mapas require(tidyverse) # Para manipulación de nuestros datos require(sf) # Para manipulación de nuestros datos (espaciales) require(ggplot2) require(ggthemes) # Sirve para dejar más lindos nuestros gráficos de ggplot require(RColorBrewer) # Ídem require(leaflet) # Sirve para hacer mapas intereactivos require(tmap) 6.1 Trabajando con el censo 2010 6.1.1 Datos del censo 2010 Hasta ahora hemos hecho énfasis en el dataset de propiedades de Properati en CABA. Vamos a retomar ese tema un poco más adelante, pero para volver a entrenar las fases anteriores del proceso de data science (importar, transformar y graficar datos) vamos a utilizar los datos del Censo 2010, provistos por http://datar.noip.me/. Debido a que el archivo .rar pesa 400mb - y que varias de las actividades que vamos a hacer probablemente no puedan ejecutarse en todas las computadores del curso, por un tema de memoria RAM - lo que hice fue, al igual que la clase 2, tomar una muestra estratificada del censo de tal manera que puedan recorrer todo el camino. El archivo con la muestra estratificada pueden descargarlo desde nuestro repositorio. Para replicar la experiencia que tendrían si descargaran los datos del censo desde http://datar.noip.me/, se trata de un .rar cuyo contenido deberían extraer en la carpeta del proyecto (o alguna subcarpeta). Hay ocho archivos .csv, que eventualmente usaremos, y un archivo descripción.txt, donde tenemos la información sobre qué es cada variable y cuales son los valores que pueden tomar. Además, existe una carpeta que se llama “labels”, donde se encuentra la información sobre qué significa cada uno de los valores de las variables para un subconjunto de ellas. De cualquier manera, no lo vamos a usar ya que esa información se encuentra en el archivo .txt. Pueden acceder al archivo .rar que contiene todos estos archivos desde https://github.com/datalab-UTDT/datasets/raw/master/CNPHyV-2010Sample.rar. Recuerden que deben descargarlo en alguna de las carpetas dentro de sus proyectos. El censo del 2010 en Argentina distingue entre las siguientes tres entidades principales: Viviendas. Según el INDEC, una vivienda es “un espacio donde viven personas”. En realidad, un lugar donde podrían vivir personas, pero al momento del censo pueden estar ocupados o desocupadas. Hogares. Un hogar para el INDEC es una persona o grupo de personas que viven 1) bajo el mismo techo y 2) comparten los gastos de alimentación. Individuos Los datos vinculados a cada una de estas unidades están almacenadas en diversos archivos .csv: “VIVIENDA.csv”, “HOGAR.csv” y “PERSONA.csv”. Comencemos primero con las viviendas: viviendas &lt;- read_delim(file = &#39;data/CNPHyV-2010Sample/VIVIENDA.csv&#39;,delim=&quot;;&quot;) Estén atentos que estamos usando la función read_delim en lugar de read_csv. La principal diferencia es que con read_delim() podemos indicar con el argumento delim el caracter que marca la separación entre datos glimpse(viviendas) ## Rows: 52,508 ## Columns: 13 ## $ VIVIENDA_REF_ID &lt;dbl&gt; 17831, 32505, 99783, 8879, 78302, 12649, 87383, 50584, 4089, 4494, 91055, 58043, 131148, 108284, 23444,... ## $ RADIO_REF_ID &lt;dbl&gt; 61, 88, 236, 45, 181, 51, 204, 121, 27, 29, 214, 133, 329, 259, 72, 221, 66, 264, 121, 117, 285, 122, 3... ## $ TIPVV &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ V01 &lt;dbl&gt; 4, 4, 1, 4, 4, 4, 4, 4, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1... ## $ V02 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ V00 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ URP &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ INCALSERV &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3... ## $ INMAT &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2... ## $ MUNI &lt;dbl&gt; 20010001, 20010001, 20010001, 20010001, 20010001, 20010001, 20010001, 20010001, 20010001, 20010001, 200... ## $ LOCAL &lt;dbl&gt; 2001010, 2001010, 2001010, 2001010, 2001010, 2001010, 2001010, 2001010, 2001010, 2001010, 2001010, 2001... ## $ INCALCONS &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 2, 3, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2... ## $ TOTHOG &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... Son 52.508 viviendas, con 13 variables. Varias de ellas nos dan información sobre la ubicación geógrafica. Por ejemplo, RADIO_REF_ID es una variable que vincula a cada vivienda con un determinado radio censal, la unidad geográfica de menor tamaño del censo. La muestra con la que estamos trabajando está estratificada a nivel de departamentos, por lo que pueden existir radios censales para los cuales no exista ninguna información. VIVIENDA_REF_ID identifica de manera única a las viviendas, y será de mucha utilidad más tarde para usar información sobre los hogares y los invididuos. Con respecto a los datos específicos sobre las viviendas, dos de los más interesantes son INCALSERV, e INCALCONS que nos indican el índice de calidad de conexiones a servicios públicos y el índice de calidad de la construcción de la vivienda, respectivamente. La página Redatam (https://redatam.indec.gob.ar/argbin/RpWebEngine.exe/PortalAction?&amp;MODE=MAIN&amp;BASE=CPV2010B&amp;MAIN=WebServerMain.inl) del Censo 2010 de Argentina nos permite ver cuáles son todas las variables del “cuestionario básico” y las variables que pueden tomar. En este caso, ambas variables pueden tomar los valores 1,2 o 3 (“Satisfactoria”, “Básica” e “Insuficiente”). Dejemos por un lado las variables referidas a los índices de calidad. Recuerden que estamos trabajando con una muestra estratificada a nivel departamental, por lo que nuestros gráficos tendrán más sentido a esa agregación o mayor (nivel provincial). Lamentablemente, nuestro dataset de viviendas no nos dice a cuál departamento pertenece cada vivienda, pero, como anticipamos anteriormente, sí nos da información sobre su radio censal en RADIO_REF_ID. Los radios censales se agrupan en fracciones censales, que a su vez se asocian a departamentos y estos últimos a las provincias. Como les dije, hoy vamos a leer muchos archivos para prácticar ¡Allá vamos! radios &lt;- read_delim(file = &#39;data/CNPHyV-2010Sample/RADIO.csv&#39;, delim=&quot;;&quot;) fracciones &lt;- read_delim(file = &quot;data/CNPHyV-2010Sample/FRAC.csv&quot;, delim=&quot;;&quot;) # El parámetro LOCALE nos permite especificar algunas caracteristicas sobre cómo leer los datos, en este caso pedimos que el encoding sea UTF-8. Además, usamos trimws() para sacar alguno de los espacios que tiene la columna NOMDPTO departamentos &lt;- read_delim(file = &quot;data/CNPHyV-2010Sample/DPTO.csv&quot;, delim=&quot;;&quot;,locale = locale(encoding = &quot;UTF-8&quot;)) %&gt;% mutate(NOMDPTO=trimws(NOMDPTO)) Bien, si no tuvieron ningun problemas con los separadores - ni con la dirección del archivo -, ya están en condiciones de observar qué contenido tiene cada uno de nuestros objetos. Veamos: glimpse(radios) ## Rows: 52,382 ## Columns: 3 ## $ RADIO_REF_ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,... ## $ FRAC_REF_ID &lt;dbl&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3... ## $ IDRADIO &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;17&quot;... radios es un Data Frame con 3 variables y 52.382 observaciones. Cada una de las observaciones corresponde a un radio único, por lo que en el Censo 2010 los radios censales fueron levemente superior a 52.000. Como vemos, cada radio censal se asocia a una única fracción censal. Veamos ahora nuestro data frame que contiene información sobre las fracciones. glimpse(fracciones) ## Rows: 5,428 ## Columns: 3 ## $ FRAC_REF_ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, ... ## $ DPTO_REF_ID &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,... ## $ IDFRAC &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;,... Bien, las fracciones censales son 5.428 y cada una de ellas corresponde a un determinado departamento, que podemos a su vez consultar en el data frame homónimo: glimpse(departamentos) ## Rows: 527 ## Columns: 5 ## $ DPTO_REF_ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, ... ## $ PROV_REF_ID &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,... ## $ IDDPTO &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;, &quot;008&quot;, &quot;009&quot;, &quot;010&quot;, &quot;011&quot;, &quot;012&quot;, &quot;013&quot;, &quot;014&quot;, &quot;015&quot;, &quot;0... ## $ DPTO &lt;dbl&gt; 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 6028, 6035, 6091,... ## $ NOMDPTO &lt;chr&gt; &quot;Comuna 01&quot;, &quot;Comuna 02&quot;, &quot;Comuna 03&quot;, &quot;Comuna 04&quot;, &quot;Comuna 05&quot;, &quot;Comuna 06&quot;, &quot;Comuna 07&quot;, &quot;Comuna 08&quot;, &quot;Co... El data frame departamentos cuenta con más información, ya que también incluye el nombre de los departamentos. Ya tenemos nuestro camino para unir a las viviendas con sus respectivos departamentos: tenemos que combinar los radios con las fracciones y luego las fracciones con sus departamentos. Por suerte, tidyverse nos brinda una función importante para lograr esto: left_join. Veamos cómo funciona: radios &lt;- left_join(radios, fracciones, by = c(&quot;FRAC_REF_ID&quot;)) glimpse(radios) ## Rows: 52,382 ## Columns: 5 ## $ RADIO_REF_ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,... ## $ FRAC_REF_ID &lt;dbl&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3... ## $ IDRADIO &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;17&quot;... ## $ DPTO_REF_ID &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ IDFRAC &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;... Si se fijan bien, el data frame radios ahora tiene 5 variables: las 3 iniciales y dos nuevas: DPTO_REF_ID e IDFRAC. La cantidad de filas es la mismas que antes ¿Qué es lo que hace la función left_join? Para cada observación del primer data frame que le pasamos (radios) incorpora las variables que están en el segundo dataset que le pasamos (fracciones) usando una o más columnas que sirven para unir ambos data frames, en este caso FRAC_REF_ID. Uno podria pensar que como ahora tenemos el ID del departamento ya podemos unir a las viviendas con los departamentos. Sin embargo, como vamos a ver más adelante los ids de los departamentos en estas tablas no coinciden con los del shapefile disponible en el mismo repositorio. Por esta razón, vamos a tener que trabajar con los códigos de los departamentos. Para esto, debemos hacer un nuevo left_join entre las fracciones censales y el data frame con el listado de departamentos para obtener el código: radios &lt;- left_join(radios, departamentos %&gt;% select(DPTO_REF_ID, DPTO), by = c(&quot;DPTO_REF_ID&quot;)) glimpse(radios) ## Rows: 52,382 ## Columns: 6 ## $ RADIO_REF_ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,... ## $ FRAC_REF_ID &lt;dbl&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3... ## $ IDRADIO &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;, &quot;16&quot;, &quot;17&quot;... ## $ DPTO_REF_ID &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ IDFRAC &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;... ## $ DPTO &lt;dbl&gt; 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001... Ya podemos vincular las viviendas a los departamentos, ya que nuestro data frame de viviendas tiene una variable que identifica a cada radio, y nuestro data frame que acabamos de crear contiene el mismo ID y lo asigna al departamento correspondiente. Antes de aplicar el left_join, es importante marcar que siempre nos devuelve todas las columnas de ambos dataframes. Esto es ineficiente en muchos casos, por ejemplo a nosotros solo nos interesa agregar la información sobre DPTO (el código del departamento). La forma más fácil de evitar hacer innecesariamente grande nuestro dataset es eliminar las variables antes de hacer el join: viviendas &lt;- left_join(x = viviendas, y = radios %&gt;% select(RADIO_REF_ID, DPTO), by = &quot;RADIO_REF_ID&quot;) str(viviendas) # En la 14va columna tenemos el departamento al que pertenece ## tibble [52,508 x 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ VIVIENDA_REF_ID: num [1:52508] 17831 32505 99783 8879 78302 ... ## $ RADIO_REF_ID : num [1:52508] 61 88 236 45 181 51 204 121 27 29 ... ## $ TIPVV : num [1:52508] 1 1 1 1 1 1 1 1 1 1 ... ## $ V01 : num [1:52508] 4 4 1 4 4 4 4 4 1 1 ... ## $ V02 : num [1:52508] 1 1 1 1 1 1 1 1 1 1 ... ## $ V00 : num [1:52508] 0 0 0 0 0 0 0 0 0 0 ... ## $ URP : num [1:52508] 1 1 1 1 1 1 1 1 1 1 ... ## $ INCALSERV : num [1:52508] 1 1 1 1 1 1 1 1 1 1 ... ## $ INMAT : num [1:52508] 1 1 1 1 1 2 1 1 2 2 ... ## $ MUNI : num [1:52508] 2e+07 2e+07 2e+07 2e+07 2e+07 ... ## $ LOCAL : num [1:52508] 2e+06 2e+06 2e+06 2e+06 2e+06 ... ## $ INCALCONS : num [1:52508] 1 1 1 1 1 2 1 1 2 3 ... ## $ TOTHOG : num [1:52508] 1 1 1 1 1 2 1 1 1 1 ... ## $ DPTO : num [1:52508] 2001 2001 2001 2001 2001 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. VIVIENDA_REF_ID = col_double(), ## .. RADIO_REF_ID = col_double(), ## .. TIPVV = col_double(), ## .. V01 = col_double(), ## .. V02 = col_double(), ## .. V00 = col_double(), ## .. URP = col_double(), ## .. INCALSERV = col_double(), ## .. INMAT = col_double(), ## .. MUNI = col_double(), ## .. LOCAL = col_double(), ## .. INCALCONS = col_double(), ## .. TOTHOG = col_double() ## .. ) 6.1.2 Polígonos del Censo 2010 El caso del censo es uno de los más clásicos cuando queremos cruzar datos que pertenecen a una determinada organización territorial pero que no tienen incluido un shapefile o geojson (otra forma de guardar datos espaciales). En ese caso debemos encontrar los polígonos de cada región administrativa (departamentos) y unir los polígonos con los datos (joinearlos), del mismo modo que hicimos anteriormente. Pueden descargar estos datos desde aquí. Ya saben como leer shapefiles a R con read_sf() sfDpto &lt;- read_sf(&quot;data/deptosShapefile/pxdptodatosok.shp&quot;) glimpse(sfDpto) ## Rows: 527 ## Columns: 11 ## $ link &lt;chr&gt; &quot;02001&quot;, &quot;02002&quot;, &quot;02003&quot;, &quot;02004&quot;, &quot;06091&quot;, &quot;74056&quot;, &quot;02005&quot;, &quot;02006&quot;, &quot;06147&quot;, &quot;02007&quot;, &quot;02008&quot;, &quot;02009&quot;, ... ## $ codpcia &lt;chr&gt; &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;06&quot;, &quot;74&quot;, &quot;02&quot;, &quot;02&quot;, &quot;06&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;06&quot;, &quot;02&quot;, &quot;06&quot;, &quot;06&quot;, &quot;02&quot;, &quot;02&quot;, ... ## $ departamen &lt;chr&gt; &quot;Comuna 1&quot;, &quot;Comuna 2&quot;, &quot;Comuna 3&quot;, &quot;Comuna 4&quot;, &quot;Berazategui&quot;, &quot;La Capital&quot;, &quot;Comuna 5&quot;, &quot;Comuna 6&quot;, &quot;Carlos... ## $ provincia &lt;chr&gt; &quot;Ciudad Autónoma de Buenos Aires&quot;, &quot;Ciudad Autónoma de Buenos Aires&quot;, &quot;Ciudad Autónoma de Buenos Aires&quot;, &quot;Ci... ## $ mujeres &lt;chr&gt; &quot;107789&quot;, &quot;89890&quot;, &quot;101936&quot;, &quot;115079&quot;, &quot;165636&quot;, &quot;104417&quot;, &quot;98199&quot;, &quot;97206&quot;, &quot;11381&quot;, &quot;118110&quot;, &quot;97692&quot;, &quot;85... ## $ varones &lt;chr&gt; &quot;98097&quot;, &quot;68042&quot;, &quot;85601&quot;, &quot;103166&quot;, &quot;158608&quot;, &quot;99602&quot;, &quot;80806&quot;, &quot;78870&quot;, &quot;10856&quot;, &quot;102481&quot;, &quot;89545&quot;, &quot;76207... ## $ personas &lt;chr&gt; &quot;205886&quot;, &quot;157932&quot;, &quot;187537&quot;, &quot;218245&quot;, &quot;324244&quot;, &quot;204019&quot;, &quot;179005&quot;, &quot;176076&quot;, &quot;22237&quot;, &quot;220591&quot;, &quot;187237&quot;,... ## $ hogares &lt;chr&gt; &quot;84468&quot;, &quot;73156&quot;, &quot;80489&quot;, &quot;76455&quot;, &quot;93164&quot;, &quot;58559&quot;, &quot;76846&quot;, &quot;75189&quot;, &quot;7775&quot;, &quot;81483&quot;, &quot;58204&quot;, &quot;56495&quot;, &quot;... ## $ viv_part &lt;chr&gt; &quot;130771&quot;, &quot;107967&quot;, &quot;101161&quot;, &quot;82926&quot;, &quot;96025&quot;, &quot;62166&quot;, &quot;92750&quot;, &quot;93368&quot;, &quot;9446&quot;, &quot;89520&quot;, &quot;55377&quot;, &quot;63322&quot;... ## $ viv_part_h &lt;chr&gt; &quot;78360&quot;, &quot;70869&quot;, &quot;75605&quot;, &quot;69680&quot;, &quot;86248&quot;, &quot;53317&quot;, &quot;73226&quot;, &quot;72942&quot;, &quot;7504&quot;, &quot;73034&quot;, &quot;48631&quot;, &quot;52355&quot;, &quot;... ## $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-58.37501 -..., MULTIPOLYGON (((-58.40084 -..., MULTIPOLYGON (((-58.39365 -...,... Para hacer el join precisamos identificar la variable que tiene a los ids de los departamentos. En este caso podemos ver que la variable link (es una variable factor) tiene 527 valores (la misma cantidad de departamentos que tiene Argentina) y toma los mismos valores que nuestra variable en el dataset de viviendas (DPTO). Sin embargo, están en distinto formato: una es una variable numérica, mientras que la otra es una variable factor. Para hacer el join precisamos que ambas variables tengan el mismo formato: sfDpto$link &lt;- as.numeric(as.character(sfDpto$link)) Como no nos importa ninguna de las otras variables con las que viene el objeto sf, con excepción de su geometría (obviamente) y la varible “link” que contiene la información necesaria para hacer el join, vamos a quedarnos solo con ellas. Usemos lo que ya conocemos: sfDpto &lt;- sfDpto %&gt;% select(link) 6.1.3 Uniendo los datos con sus polígonos Una de las opciones que podrían hacer es unir las observaciones de las viviendas a cada uno de los polígonos que pertenece. Sin embargo, esto no es eficiente. Lo mejor es hacer nuestras transformaciones sobre el dataset de viviendas para luego hacer la unión con los polígonos cuando tengamos un dataset con los indicadores por departamento. Las operaciones de join suelen ocupar mucho espacio de memoria (y tiempo), especialmente cuando se trata de muchas filas y columnas. Además, siempre que se pueda es mejor hacer nuestras transformaciones de datos cuando no son objetos Simple Feature. Vamos a generar un indicador de “mala infraestructura”. En rigor, vamos a medir la proporción de viviendas que presentan un índice de calidad de servicios igual a 3 (insuficiente) o de calidad de construcción de viviendas igual a 3 (insuficiente). Luego, vamos a agrupar el dataset por los departamentos. Por suerte, ya sabemos como hacerlo con tidyverse: datosCalidad &lt;- viviendas %&gt;% mutate(indiceCalidad = ifelse(INCALSERV == 3 | INCALCONS == 3, 1, 0)) %&gt;% group_by(DPTO) %&gt;% summarise(indiceCalidad = round(sum(indiceCalidad)/n()*100)) ## `summarise()` ungrouping output (override with `.groups` argument) str(datosCalidad) ## tibble [526 x 2] (S3: tbl_df/tbl/data.frame) ## $ DPTO : num [1:526] 2001 2002 2003 2004 2005 ... ## $ indiceCalidad: num [1:526] 9 2 6 7 5 1 4 10 1 1 ... Tenemos el porcentaje de viviendas que tienen alguno de estos dos índices como insuficientes. Para esto usamos la función n(), que es sumamente útil: nos dice la cantidad de casos que existe para cada grupo. Es especialmente interesante cuando queremos calcular proporciones, ya que al ponerlo en el denominador, solo nos queda poner en el numerador la cantidad de casos que cumplen con nuestro criterio (que lo hicimos con la función sum()). Luego multiplicamos por 100, porque así se representan los datos en porcentajes. Ahora sí, ya podemos hacer la unión con nuestro dataset espacial: datosCalidad &lt;- left_join(sfDpto, datosCalidad, by = c(&quot;link&quot;=&quot;DPTO&quot;)) ¡Importante! Vean que pudimos asignar a las observaciones a sus polígonos sin necesidad de hacer un join espacial, como hicimos con los inmuebles y los polígonos de CABA en la anterior clase. Esto se puede hacer siempre que haya un ID que nos permita asignarlo. En general, esto no existe y hay que trabajar con los nombres de las entidades (en este caso, los departamentos). Antes de seguir, vamos a agregar los nombres de los departamentos, que nos será útil para las visuaizaciones: datosCalidad&lt;- left_join(datosCalidad, departamentos %&gt;% select(DPTO, NOMDPTO), by = c(&quot;link&quot;=&quot;DPTO&quot;)) str(datosCalidad) ## tibble [527 x 4] (S3: sf/tbl_df/tbl/data.frame) ## $ geometry :sfc_MULTIPOLYGON of length 527; first list element: List of 1 ## ..$ :List of 1 ## .. ..$ : num [1:539, 1:2] -58.4 -58.4 -58.4 -58.4 -58.4 ... ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;XY&quot; &quot;MULTIPOLYGON&quot; &quot;sfg&quot; ## $ link : num [1:527] 2001 2002 2003 2004 6091 ... ## $ indiceCalidad: num [1:527] 9 2 6 7 25 21 5 1 30 4 ... ## $ NOMDPTO : chr [1:527] &quot;Comuna 01&quot; &quot;Comuna 02&quot; &quot;Comuna 03&quot; &quot;Comuna 04&quot; ... ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA ## ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;link&quot; &quot;indiceCalidad&quot; &quot;NOMDPTO&quot; 6.2 Haciendo visualizaciones Hace unos años uno podría haber dicho que una de las desventajas de R es su falta de realizar visualizaciones de relevancia, ni siquiera hablar de trabajar con visualizaciones interactivas. Ejecuten el siguiente código, tal como hace Lovelace en su libro de Geocomputation, y verán que ya no es el caso: popup = c(&quot;Nuestra clase&quot;) leaflet() %&gt;% addProviderTiles(&quot;NASAGIBS.ViirsEarthAtNight2012&quot;) %&gt;% addMarkers(lng = c(-58.381592), lat = c(-34.603722), popup = popup) %&gt;% setView(lng = -58.381592, lat =-34.603722, zoom = 4) Este mapa interactivo es posible gracias a la implementación de Leaflet para R (deben tener instalado y cargado en la sesión el paquete homónimo para poder correrlo). Hoy en día R nos da la posiblidad de realizar excelentes gráficos y visualizaciones. Vamos a presentar los principales paquetes para hacerlo durante las próximas dos clases, haciendo énfasis en los mapas. 6.2.1 Ggplot2 Hace tiempo que ggplot2 se ha convertido en el paquete de referencia para hacer visualizaciones. En distintas partes de nuestro curso lo he mostrado, en la mayoría de veces sin su aplicación a datos espaciales. En esta clase voy a mostrar que es posible hacer gráficos con ggplot2 y sf, pero más tarde también voy a mostrar que sus funcionalidades ampliamente exceden esta extensión. Ggplot2 (al igual que tmap y leaflet) organiza su trabajo en capas. En general, primero pasamos los datos y después vamos agregando “mejoras incrementales”. Siempre debemos comenzar con ggplot() y luego indicar qué tipo de gráfico queremos: ggplot(data = datosCalidad) + geom_sf() Como vemos, los departamentos de la Antártida y las Islas del Atlántico Sur no tienen mucho sentido para nuestras visualizaciones, y además nuestra visualización. Vamos a filtrarlas para lo que resta de la clase: datosCalidad &lt;- datosCalidad %&gt;% filter(!NOMDPTO %in% c(&#39;Islas del Atlántico sur&#39;, &quot;Antártida Argentina&quot;)) ggplot(data = datosCalidad) + geom_sf() Mucho mejor. Ahora debemos rellenar los polígonos con nuestras variables, en este caso el índicador de deficiencias en la calidad de la construcción o servicios de las viviendas. Esto lo hacemos SIEMPRE dentro de la función aes(), en este caso con fill: ggplot(data = datosCalidad, aes(fill = indiceCalidad)) + geom_sf() Lo que hace aes() es pasarle a ciertos objetos del gráfico (en este caso, fill) los valores que debe tomar. En español, lo que le decimos es “de ahora en más, rellená todos los polígonos en base a los valores de la variable indiceCalidad”. Luego, le decimos que grafique las geometrias de sf, simplemente pasándole el argumento “geom_sf”. Los colores que tiene por default son bastante malos y no queda muy en claro el valor que le corresponde a cada departamento. Vamos a empezar a hacer uso de algunas mejoras en los gráficos de ggplot2 ggplot(data = datosCalidad, aes(fill = indiceCalidad)) + geom_sf(colour = &quot;white&quot;) + scale_fill_gradientn(colours = brewer.pal(n = 5, name = &quot;Blues&quot;), name = &quot;Viviendas de calidad \\ninsuficiente (%)&quot;) Bastante mejor. Lo que hicimos fue 1) cambiar el color de los límites entre los polígonos (colour = “white”) y también cambiamos los colores correspondientes a cada uno de los valores a través de scale_fill_gradientn. A esta función solo debemos asignarle una cantidad de colores, y lo que hace es generar las transiciones entre ellos para generar el gradiente de colores. Aquí es donde usamos las paletas que RColorBrewer nos ofrece (http://colorbrewer2.org/). la función brewer.pal nos pide solo dos argumentos: la cantidad de colores y la paleta. Luego, en name indicamos el título de la leyenda. Los caracteres \\n son interpretados como un enter en la paleta. Prueben cambiar la paleta “Blues” por la paleta “Oranges” o \"Reds. Todavía podemos mejorar sustancialmente el gráfico. Por ejemplo, con el siguiente código: mapa &lt;- ggplot(data = datosCalidad, aes(fill = indiceCalidad)) + geom_sf(colour = NA) + scale_fill_gradientn(colours = brewer.pal(n = 5, name = &quot;Reds&quot;), name = &quot;Viviendas de calidad insuficiente (%)&quot;, guide = guide_colourbar( direction = &quot;horizontal&quot;, barheight = 0.8, barwidth = 16, title.position = &#39;top&#39;, title.hjust = 0.5)) mapa ¡Mucho mejor! Pero hay varias cosas que incluímos, vamos a explicarlo paso por paso. En primer lugar, ya no hay línea divisora entre los polígonos, recomiendo enfáticamente removerlos cuando las unidades son muchas, pequeñas y el mapa no es interactivo, esto lo hacemos mediante colour = NA dentro de geom_sf. Luego, dentro de el espacio donde definimos la paleta scale_fill_gradientn agregamos una colourbar, que lo que haces es modificar la leyenda dándole un mejor aspecto. Podemos elegir la dirección (en este caso, horizontal), debemos elegir tanto su largo como su ancho, elegir la posición del titulo y elegir su “justificación”, que es básicamente lo que hacemos en word eligiendo si queremos que el texto se “pegue” a la parte izquierda, este centrado o se “pegue” a la parte derecha. hjust = 0.5 lo deja centrado. Los tamaños en ggplot2 están representados en milimetros, suponiendo que en cada pulgada hay 72 puntos (72 PPI), vamos a volver a esto un poco más adelante. Noten algo más: guardamos nuestro gráfico en un objeto que se llama “mapa”. De ahora en más podemos ir modificando este objeto agregándole capas con + como estuvimos haciendo hasta ahora. Vamos a cambiar ese fondo gris que no queda bien: mapa &lt;- mapa + theme_void() mapa Como ggplot2 trabaja por capas, le asignamos un conjunto de formatos mediante la función theme_void. Los temas (themes) predefinidos son muy útiles ya que nos ahorran mucho código de formateo de la visualización. el paquete ggthemes contiene varios themes muy interesantes, como los del Wall Street Journal, Five Thirty Eight o Stata, más adelante vamos a probarlos. Probemos qué hubiera pasado con aplicando otro theme: mapaStata &lt;- mapa + theme_stata() mapaStata Bien, ahora querríamos colocar la colourbar abajo, tal como hace el theme_stata(), y también remover las líneas de fondo. Estas modificaciones se pueden hacer dentro de la función theme(). Dentro de ella elegimos los objetos y los modificamos. Por ejemplo, en legend.position decimos dónde queremos la leyenda ,en este caso la queremos abajo del gráfico y por eso usamos “bottom”. Luego, tomamos la grilla de fondo “panel.grid.major” y a la línea le asignamos un color transparente. También seteamos algunos parámetros para el título (que esté centrado, que tenga tamaño 16 y que esté en negrita) y el título de nuestra leyenda. mapa &lt;- mapa + theme(legend.position = &quot;bottom&quot;, panel.grid.major=element_line(colour=&quot;transparent&quot;), plot.title = element_text(hjust = 0.5, size = 16, face = &#39;bold&#39;), legend.title = element_text(hjust = 0.5, size = 12)) mapa Solo nos queda agregar un título y una fuente: mapa &lt;- mapa + labs(title = &quot;Mapa de calidad de viviendas&quot;, caption = &quot;Fuente: INDEC&quot;) mapa Ahora nuestro gráfico quedó mucho mejor. Pueden guardardo desde “Export”, aunque rápidamente van a darse cuenta, cuando vayan cambiando las dimensiones de los plots, que a medida que lo hacen más grande, el texto queda más chico. Además, solo pueden exportar el formato del tamaño del planel multiuso. Pero no se preocupen, hice una función que hace todo esto más fácil: van a poder elegir el ancho y alto en píxeles y después solo tienen que elegir cuánto quieren que se agrande el texto. Ejecuten el siguiente código: # Source ejecuta código R de otros archivos source(&quot;https://raw.githubusercontent.com/datalab-UTDT/datasets/master/auxiliares.R&quot;) plot.save(plot = mapa, width = 500, height = 1000, filename =&#39;mapa.png&#39;, bg = &quot;#f5f5f2&quot;) plot.save(plot = mapa, width = 1000, height = 2000, text.expansion = 2, filename =&#39;mapaText2.png&#39;, bg = &quot;#f5f5f2&quot;) ¿Qué patrón observan en el mapa? Por lo que vemos, el norte del país muestra los peores valores en nuestro indicador, especialmente en las provincias del noreste. 6.2.2 Tmap Aun siendo muy útil para nuestros mapas, Gplo2 es mucho más útil para otro tipo de visualizaciones. Para hacer clorophets, una de las visualizaciones más típicas, el paquete más simple y que más utilizarán es tmap. Ya lo hemos usado en otras ocasiones, y veamos cómo podemos hacer lo que hicimos con ggplot2 de una manera más simple: tmapClorophet &lt;- tm_shape(datosCalidad) + tm_polygons(border.col = &quot;transparent&quot;, col = &quot;indiceCalidad&quot;, palette=brewer.pal(n=5,&quot;Reds&quot;), title = &quot;Viviendas de \\ncalidad \\ninsuficiente&quot;) + tm_layout(legend.format = list(text.separator = &quot;-&quot;), main.title = &quot;Mapa de calidad de viviendas&quot;, main.title.size = 0.8, frame = FALSE) tmapClorophet Como vemos, la sintaxis es relativamente similar a ggplot2. Primero debemos informar cuáles son los datos dentro de tm_shape (en este caso, datosCalidad). Luego indicamos qué queremos graficar: polígónos (tm_polygons). Al igual que en el anterior gráfico, no queremos que haya divisiones entre los polígonos así que lo pasamos a “transparent”. Elegimos el color por el cual queremos que rellene los polígonos con col, la paleta con palette y el título de la leyenda con title. Luego, con tm_layout() modificamos el aspecto de la leyenda, específicamente el separador entre las categorías (en lugar de “to”, que use “-”). Cambiamos el título y sacamos el frame que viene por default ¡Muy fácil! Todavía más, con estos mapas podemos discretizar rápidamente nuestros plots. Por default nos genera las 5 categorías que dividen a la variable (de 0 a 100). En lugar de eso, podemos mejorar esta aproximación al graficar en 4 categorías a los departamentos de tal manera que en cada una de estas categorías se acumule aproximadamente la misma cantidad de departamentos. Estos son los quintiles de una distribución y tmap nos hace el trabajo muy fácil. tmapClorophet &lt;- tm_shape(datosCalidad) + tm_polygons(border.col = &quot;transparent&quot;, col = &quot;indiceCalidad&quot;, palette=rev(brewer.pal(n=4,&quot;RdYlGn&quot;)), breaks = quantile(datosCalidad$indiceCalidad), title = &quot;Viviendas de \\ncalidad \\ninsuficiente&quot;) + tm_layout(legend.format = list(text.separator = &quot;-&quot;), main.title = &quot;Mapa de calidad de viviendas&quot;, main.title.size = 0.8, frame = FALSE) tmapClorophet La clave fue el argumento breaks, donde indicamos dónde queremos que corte a la variable. quantile() corta en cuatro porciones a la variable índice de calidad, mientras que usamos otro tipo de paleta y, ademas, invertimos el orden usando rev(). Vamos a mejorar nuestro gráfico utilizando simultáneamente los polígonos de las provincias de Argentina. Podrían repetir el procedimiento leyendo los shapefiles con el comando readOGR tal como hicimos con los departamentos. Sin embargo, para aprender a leer otro tipo de archivos donde se guardan datos espaciales, vamos a usar la función st_read y leer un archivo geojson: mapaProvincias &lt;- read_sf(&quot;https://github.com/datalab-UTDT/datasets/raw/master/provincias.geojson&quot;) mapaProvincias &lt;- mapaProvincias[-24,] tmapClorophet &lt;- tm_shape(datosCalidad) + tm_polygons(border.col = &quot;transparent&quot;, col = &quot;indiceCalidad&quot;, palette=rev(brewer.pal(n=4,&quot;RdYlGn&quot;)), breaks = quantile(datosCalidad$indiceCalidad), title = &quot;Viviendas de \\ncalidad \\ninsuficiente&quot;) + tm_layout(legend.format = list(text.separator = &quot;-&quot;), main.title = &quot;Mapa de calidad de viviendas&quot;, main.title.size = 0.8, main.title.position = &#39;center&#39;, frame = FALSE) + tm_shape(mapaProvincias) + tm_borders(col = &quot;white&quot;) tmapClorophet Ahora podemos ver como guardar un objeto tmap usando save_tmap. Tener en cuenta que el ancho y el alto en este caso tienen que multiplicarlo por 300 para conocer el tamaño en píxeles total (esto es porque tiene predefinido ppi=300). Por ejemplo, el archivo “tmapClorophet.png” debería tener 900px de ancho. save_tmap(tm = tmapClorophet, filename = &quot;tmapClorophet.png&quot;,width = 3) 6.3 Leaflet Hasta ahora hemos estado trabajando con gráficos estáticos. En muchas oportunidades son útiles, y cuando debemos hacer reportes o determinadas presentaciones son nuestra única opción. Sin embargo, los mapas interactivos nos permiten consultar los datos de una manera distinta y son una buena alternativa para compartir las visualizaciones. Vamos a ver un simple ejemplo de utilización de Leaflet, mostrando las mismas visualizaciones que estuvimos haciendo hasta el momento para los datos del censo. Todas las visualizaciones de leaflet deben comenzar con la función leaflet() y, a diferencia de ggplot2 y tmap, en leaflet usamos %&gt;% para ir agregando las capas. addTiles() plotea la “imagen” del mundo, si no usamos esa función solo veremos un output gris leaflet() %&gt;% addTiles() Ahora deberíamos indicar qué tipo de figura deseamos agregar. En nuestro caso, queremos agregar polígonos, por lo que usaremos la función addPolygons leaflet(datosCalidad) %&gt;% addTiles() %&gt;% addPolygons(data = datosCalidad) Debemos rellenarlos con los valores correspondientes al índice de deficiencia de viviendas que armamos anteriormente. Para eso tenemos que crear una paleta, como ya hicimos en ggplot2 y tmap. En Leaflet lo que tenemos que usar es la función colorBin. A ella le pasamos una paleta de RColorBrewer, en orden reverso para que vaya desde verde (valores más bajo) a rojo (valores más altos). Al mismo tiempo, seteamos a FALSE los bordes de los polígonos (stroke=FALSE) y la opacidad a 1 del relleno (fillOpacity=1) para que veamos correctamente los colores. pal &lt;- colorBin(&quot;RdYlGn&quot;, reverse = TRUE, domain = datosCalidad$indiceCalidad, bins = quantile(datosCalidad$indiceCalidad)) leaflet() %&gt;% addTiles() %&gt;% addPolygons(data=datosCalidad, stroke = FALSE, fillColor = ~pal(indiceCalidad), fillOpacity = 1) Para terminar, vamos a agregar los labels, es decir lo que aparece cuando nos posicionamos sobre alguno de los polígonos. Esto lo hacemos con el arugmento label dentro de los polígonos. Al igual que fillColor, debemos pasarlo como fórmula, es decir usando el prefijo ~, y luego usamos la función paste0, que como ya hemos visto en clases anteriores “pega” o colapsa los vectores. En este ejemplo queremos que diga el nombre del departamento, dos puntos, el valor del índice de calidad y luego el signo %. Además, agregamos el contorno de las provincias agregando otra capa de polígonos, pero sin relleno y solo con el contorno en blanco leaflet() %&gt;% addTiles() %&gt;% addPolygons(data = datosCalidad, stroke = FALSE, fillColor = ~pal(indiceCalidad), fillOpacity = 1, label = ~paste0(NOMDPTO,&quot;: &quot;, indiceCalidad, &#39;%&#39;)) %&gt;% addPolygons(data=mapaProvincias, color = &#39;white&#39;, weight = 2, fill = NA) 6.4 Ejercicios En la carpeta de datos del censo todavía hay dos archivos csv que no usamos: los datos de hogares y de individuos. Usando nuestro dataset de viviendas, unan los hogares a las viviendas y, luego, los hogares a los individuos. Una vez que tengan el dataset con los individuos, y el departamento al que pertenecen, creen un mapa con tmap, tal como hicimos con el índice de vivienda, pero ahora con la edad promedio por departamento (tener en cuenta que la variable “años cumplidos” es la P03) "],
["making-polygons-comparable-again.html", "7 Making polygons comparable again 7.1 Carga de los datos 7.2 Diferentes radios censales 7.3 Make polygons comparable again 7.4 Haciendo mapas de nuestros nuevos datos 7.5 Ejercicio", " 7 Making polygons comparable again El INDEC realiza censos con un intervalo de aproximadamente 10 años en Argentina. Las últimas dos ediciones al escribir estas lineas fueron en los años 2001 y 2010. Los censos poblacionales son una de las formas más clásicas de conseguir información homogenéa, de alcance nacional sobre las condiciones de vida de toda la población Argentina, tanto urbana y rural, con un nivel de granuralidad bastante aceptable. Todavía más, al hacerse cada cirto período de tiempo, es posible analizar variaciones entre décadas. La unidad espacial mínima de los censos de Argentina son los radios censales. Comparar la evolución de diversas variables entre censos a ese nivel de desagregación es muy útil para responder muchas interesantes. En este ejercicio vamos a poder analizar cómo cambió la edad promedio de los habitantes de estos radios censales entre 2001 y 2010 en la Ciudad de Buenos Aires. Para hacerlo, vamos a encontrarnos con un problema entre los polígonos de ambos censos. Miren la figura que aparece abajo ¿Son iguales? Figure 7.1: La proyección MERCATOR distorsiona nuestra percepción de los tamaños Claramente no: los radios censales cambiaron de forma entre 2001 y 2010. Esto es atendible, ya que estos radios suelen crearse respetando ciertos criterios metodológicos que no priorizan la comparación entre censos. Con todo, sería una pena no poder trabajar con estos datos: vamos a aprender una alternativa para resolver este problema, pero a un costo. Para hacer comparables los datos entre ambos polígonos, debemos hacer determinados supuestos de cómo se distribuyen los datos hacia adentro de ellos Uno de los supuestos más usuales es asumir que los datos se distribuyen de manera homogénea hacia dento de los polígonos. Esto hace muy fácil poder trabajar con los polígonos mediante lo que se conoce como Interpolación ponderada por peso, o areal weighted interpolation. 7.1 Carga de los datos Lo primero que habría que hacer es cargar los datos. Sin embargo, sabemos que debemos cargar las librerias en nuestra sesión de R para que muchas de las funciones que usamos se ejecuten. library(tidyverse) # Para manipular datos y graficarlos library(sf) # Para manipular datos espaciales library(tmap) # Para graficar datos espaciales Una vez que tengamos cargadas las librerías vamos a descargar los datos desde el repositorio. Podría haber enviado todo en un RData, pero quiero introducirlos a una nueva forma de guardar datos espaciales: geojson. La ventaja más obvia de este tipo de archivos espaciales es que consiste de un solo archivo que contiene todos los datos en listas. No vamos a preocuparnos estrictamente por su forma, pero lo que sí vamos a hacer es leer un geojson porque probablemente sea uno de los archivos con los que eventualmente se encuentren. datosCenso2001 &lt;- read_sf(&quot;https://github.com/datalab-UTDT/GIS2/raw/master/AWI/radiosCensalesCABA2001.geojson&quot;) datosCenso2010 &lt;- read_sf(&quot;https://github.com/datalab-UTDT/GIS2/raw/master/AWI/radiosCensalesCABA2010.geojson&quot;) Los geojson se leen con la misma función read_sf. Un punto importante a tener en cuenta cuando trabajamos con este formato de datos espaciales es que obliga a que el CRS sea el 4326, es decir el WSG84 sin proyectar. Aunque podríamos seguir trabajando con esta proyección, vamos a transformarlos a la proyección inicial en la cual vinieron. No conocemos el código EPSG, pero sí el proj4string correspondiente: +proj=tmerc +lat_0=-90 +lon_0=-66 +k=1 +x_0=3500000 +y_0=0 +ellps=WGS84 +units=m +no_defs. Con el siguiente código van a tener a los dos objetos en la proyección con la que arrancaron. datosCenso2001 &lt;- datosCenso2001 %&gt;% st_transform(&#39;+proj=tmerc +lat_0=-90 +lon_0=-66 +k=1 +x_0=3500000 +y_0=0 +ellps=WGS84 +units=m +no_defs&quot;&#39;) datosCenso2010 &lt;- datosCenso2010 %&gt;% st_transform(&#39;+proj=tmerc +lat_0=-90 +lon_0=-66 +k=1 +x_0=3500000 +y_0=0 +ellps=WGS84 +units=m +no_defs&quot;&#39;) Si quieren investiguen que variables hay en cada uno de los datos censales con las funciones que ya vimos en clase, tales como str o colnames. Pueden probar una función adicional de tidyverse: glimpse() glimpse(datosCenso2001) ## Rows: 3,407 ## Columns: 23 ## $ COD_2001 &lt;chr&gt; &quot;020100402&quot;, &quot;020100305&quot;, &quot;020100301&quot;, &quot;020100303&quot;, &quot;020100307&quot;, &quot;020100306&quot;, &quot;020100205&quot;, &quot;0... ## $ PROV_ &lt;chr&gt; &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02&quot;, &quot;02... ## $ DEPTO_ &lt;chr&gt; &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;010&quot;, &quot;0... ## $ pop &lt;int&gt; 110, 1405, 1162, 762, 591, 738, 752, 481, 1129, 555, 1047, 803, 906, 659, 726, 802, 786, 759,... ## $ pop014 &lt;int&gt; 14, 180, 177, 74, 109, 124, 133, 81, 147, 99, 155, 144, 150, 108, 124, 146, 117, 124, 174, 12... ## $ pop1564 &lt;int&gt; 96, 1049, 757, 528, 366, 532, 473, 297, 768, 336, 681, 503, 650, 436, 506, 522, 520, 536, 556... ## $ pop64plus &lt;int&gt; 0, 176, 228, 160, 116, 82, 146, 103, 214, 120, 211, 156, 106, 115, 96, 134, 149, 99, 170, 105... ## $ highSchoolAboveALLPOP &lt;int&gt; 25, 851, 619, 396, 286, 329, 352, 256, 554, 307, 528, 300, 435, 234, 351, 291, 338, 350, 314,... ## $ terciarioAboveALLPOP &lt;int&gt; 6, 600, 427, 217, 193, 221, 191, 148, 365, 211, 299, 165, 285, 108, 249, 137, 165, 222, 136, ... ## $ universitaryAboveALLPOP &lt;int&gt; 2, 354, 248, 118, 123, 135, 81, 77, 190, 108, 150, 82, 159, 46, 136, 66, 91, 138, 55, 157, 86... ## $ highSchoolAbove2564 &lt;int&gt; 10, 704, 477, 310, 204, 284, 274, 194, 451, 224, 419, 250, 383, 192, 279, 242, 268, 304, 266,... ## $ terciarioAbove2564 &lt;int&gt; 3, 530, 357, 187, 142, 200, 165, 121, 325, 165, 259, 150, 262, 97, 212, 129, 151, 204, 126, 2... ## $ universitaryAbove2564 &lt;int&gt; 2, 324, 209, 110, 97, 126, 75, 64, 177, 95, 142, 77, 153, 46, 122, 64, 83, 132, 53, 147, 78, ... ## $ pop2564 &lt;int&gt; 60, 891, 593, 407, 277, 422, 384, 248, 642, 280, 586, 387, 536, 327, 377, 401, 390, 403, 436,... ## $ avgAge &lt;dbl&gt; 28.72727, 36.58434, 40.85714, 43.49213, 41.24196, 36.57182, 40.04255, 41.98753, 41.31709, 41.... ## $ descuentoJubil &lt;dbl&gt; 0.6333333, 0.8321256, 0.8084291, 0.7699387, 0.7460938, 0.8010899, 0.7523511, 0.8271028, 0.785... ## $ descuentoJubilAsalariado &lt;dbl&gt; 0.8181818, 0.8921933, 0.8626198, 0.8403756, 0.7751938, 0.8604651, 0.8512821, 0.8661972, 0.837... ## $ asalariado &lt;dbl&gt; 0.7333333, 0.6497585, 0.5996169, 0.6533742, 0.5039062, 0.7029973, 0.6112853, 0.6635514, 0.616... ## $ cuentapropia &lt;dbl&gt; 0.2000000, 0.3393720, 0.3888889, 0.3404908, 0.4765625, 0.2833787, 0.3761755, 0.3364486, 0.367... ## $ servDomestico &lt;dbl&gt; 0.1000000, 0.0205314, 0.0287356, 0.0276074, 0.0546875, 0.0299728, 0.0250784, 0.0467290, 0.040... ## $ asalariadoPrivado &lt;dbl&gt; 0.1333333, 0.1086957, 0.1800766, 0.1932515, 0.1757812, 0.1771117, 0.1285266, 0.1495327, 0.093... ## $ ocupadosUniversitariaComp &lt;dbl&gt; 0.0000000, 0.3683575, 0.3639847, 0.3036810, 0.3945312, 0.3024523, 0.2100313, 0.2897196, 0.281... ## $ geometry &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((4192913 615..., MULTIPOLYGON (((4192240 615..., MULTIPOLYGON (((... 7.2 Diferentes radios censales Como les comenté anteriormente, los radios censales para la Ciudad de Buenos Aires sufrieron importantes cambios en su forma. Pero no hace falta que me crean a mí o a la imagen que puse anteriormente: con R pueden verlo por ustedes mismos. De paso, vamos a introducir una función nueva de sf y algo nuevo sobre cómo hacer múltiples mapas. Vamos a observar cómo son los radios censales en Palermo para 2001 y para 2010. Para esto lo primero que debemos hacer es hacer “zoom” en Palermo para ambos censos y luego hacer un mapa que los muestre lado a lado. La función st_crop será muy útil para esto. Lo que hace recortar los polígonos en base a las coordenadas más “externas” de un determinado polígono. De esta manera, siempre queda un cuadrado o un rectángulo en general “centrado” con respecto a los datos espaciales que nosotros queremos. # Palermo tiene el código de departamento número 014 radiosCensalesPalermo2010 &lt;- datosCenso2010[datosCenso2010$DEPTO_ %in% &#39;014&#39;,] %&gt;% st_union() palermo2010 &lt;- st_crop(x = datosCenso2010, y = radiosCensalesPalermo2010) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries palermo2001 &lt;- st_crop(x = datosCenso2001, y = radiosCensalesPalermo2010) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries ¿Qué significa ese warning attribute variables are assumed to be spatially constant throughout all geometries? Lo que nos avisa es que st_crop cortó algunos polígonos y les asignó el valor de los atributos que tenían cuando los polígonos estaban completos ¿Esto es correcto? Depende de la variable con la que estamos trabajando. Imaginen que la variable nos describe el uso de la tierra. Si el polígono más grande era una zona industrial, un polígono recortado de él también tiene que ser una zona industrial, por lo cual no hay problema. Ahora imaginen que es el la cantidad de personas que viven allí. Es altamente problable que al recortar el polígono no sea correcto asignar la misma cantidad de personas. En este ejemplo particular esto nos tiene sin cuidado, ya que solo queremos quedarnos con la zona de Palermo y no usar sus variables. Ahora hagamos unos gráficos simples de Palermo en ambos censos con tmaps. Presten atención a la nueva función tm_sf(). Lo que hace es graficar la columna geometry que se encuentra en los datos que le pasamos con tm_shape. De esta manera, imita el tipo de datos que hay y no hace falta aclararle si son puntos o polígonos, por ejemplo. Luego usamos la función tm_layout que nos ayuda para modificar algunas cuestiones relacionadas con el diseño, como el título. Finalmente usamos tmap_arrange al que le podemos pasar gráficos y los acomoda en un mismo panel para mostrarlos de un solo golpe. mapa2001 &lt;- tm_shape(palermo2001) + tm_sf(border.col = &#39;black&#39;,col = &#39;grey90&#39;) + tm_layout(title=&#39;Radios censales 2001 - Palermo&#39;) mapa2010 &lt;- tm_shape(palermo2010) + tm_sf(border.col = &#39;black&#39;,col = &#39;grey90&#39;) + tm_layout(title=&#39;Radios censales 2010 - Palermo&#39;) tmap_arrange(mapa2001, mapa2010) Antes de analizar los gráficos, un par de puntos a tener en cuenta. Noten que guardé los mapas individualmente en objetos. Esto es realmente útil en algunas cirscunstancias, y no solo para luego ponerlos en un panel. Nos sirve, por ejemplo, para ir agregando capas de a poco a nuestro gráfico e ir guardando las versiones intermedias. En segundo lugar, es importante saber cómo exportar estos gráficos. Es realmente simple con tmap y también con ggplot. En este caso, al estar trabajando con tmap ejecuten lo siguiente: graficoSalida &lt;- tmap_arrange(mapa2001, mapa2010) tmap_save(graficoSalida,dpi = 300, filename = &#39;comparacionradioscensales.png&#39;) tmap_save solo requiere del objeto a exportar (en nuestro caso, graficoSalida) y la dirección y nombre donde se guardará (en este caso, lo guardamos en la misma carpeta del proyecto con el nombre ‘comparacionradioscensales.png’). DPI es un parámetro adicional que indica la cantidad de Dots Per Inch (DPI) que queremos que tenga nuestra imagen. 300 DPI es una cantidad elevada para gráficos en la computadora. Volvamos a los gráficos de los radios censales. Comparen distintos puntos de Palermo y verán que las diferencias son elevadas entre ambos censos. Aun si este cambio en los polígonos no es del todo reparable, podemos comparar la evolución de diferentes variables si estamos dispuestos a tolerar un error basado en la asunción de que las variables se distribuyen uniformemente en el espacio. 7.2.1 Breve desvío: haciendo lo mismo con ggplot() La librería ggplot2 es la más utilizada para hacer gráficos en R. Veamos cómo hacer lo mismo que hicimos en la subsección anterior, pero esta vez usando esta librería. La principal diferencia es que ggplot, para graficar más de un gráfico en un panel, nos pide que los datos estén todos juntos en un mismo data.frame. Esto es lo que se muestra en el capítulo de visualizaciones de este mismo libro. Sin embargo, no siempre esta es la mejor elección. Muchas veces queremos hacer gráficos con datos que están representados en distintos objetos y esto no debería ser un impedimento para poder armar nuestro panele de graficos. Por suerte, el paquete gridExtra tiene exactamente lo que necesitamos para solucionar este problema. No se olviden de instalar el paquete gridExtra para poder ejecutar lo siguiente: library(gridExtra) mapa2001 &lt;- ggplot(palermo2001) + geom_sf(col = &#39;black&#39;,fill = &#39;grey90&#39;) + labs(title=&#39;Radios censales 2001 - Palermo&#39;) + coord_sf(datum = NA) + theme_minimal() mapa2010 &lt;- ggplot(palermo2010) + geom_sf(col = &#39;black&#39;,fill = &#39;grey90&#39;) + coord_sf(datum = NA) + labs(title=&#39;Radios censales 2010 - Palermo&#39;) + theme_minimal() grid.arrange(mapa2001, mapa2010,ncol=2) Muy similar a lo que hicimos con tmap, no? El código incluso se parece bastente, con algunos cambios. coord_sf(datum = NA) lo usamos para que ggplot no nos muestre información sobre el sistemas de coordenadas de referencia (puede ser muy molesto para nuestro gráfico). themee_minimal() saca mucho de los elementos que los gráficos en ggplot tienen por default. Ahora sí, sigamos con lo importante. 7.3 Make polygons comparable again Estamos en condiciones de hacer lo que buscábamos: hacer comparables a los polígonos ¿Qué hace nuestro algoritmo de Areal Weighted Interpolation? Resumiendo y simplificando un poco, lo que hace es realmente simple: estima el porcentaje de un polígono que se solapa con otro y le asigna de manera proporcional el valor de las variables. Pero hay que tener en cuenta que las variables pueden ser conteos (como población) o porcentajes (como por ejemplo el porcentaje de población con estudios secundarios completos). En el primer caso, el algoritmo debe sumar las distintas partes que forman al polígono, mientras que en el segundo debe ponderar el porcentaje de acuerdo al solapamiento. En nuestros objetos tenemos ejemplos de ambas variables. Por ejemplo, en ambos casos tenemos el promedio de edad (avgAge), pero también la población de personas menores a 14 años (pop014). Veamos cómo podemos transformar los polígonos de 2001 a los del 2010 para poder observar la distribución de estas dos variables para los dos censos. El paquete que nos ayudará para lograr este objetivo es areal. Instalenlo si es la primera vez que están trabajando con este paquete. Ya deberían saber como hacerlo: install.packages(‘areal’). Una vez que lo tengan instalado, usen require o library para poder usar la función aw_interpolate. La función pide algunos parámetros. En primer lugar, source pide el objeto cuya información espacial queremos cambiar, en este caso los datos del censo 2001. Luego, nos pide el sid, que no es otra cosa que un conjunto de identificadores únicos para cada uno de los datos de nuestra source. Luego, en .data nos pide el target, es decir la forma que van a tomar nuestros datos espaciales, en nuestro ejemplo los radios censales de 2010, y también nos pide los ids (identificadores) correspondientes en tid. Luego, nos pide como quiere que ponderemos los nuevos polígonos con el parámetro weight, en nuestro caso queremos que sume (sum) proporcionalmente cada uno de los polígonos que contribuirán con el polígono de 2010. El parámetro output nos indica si queremos que nos devuelva un objeto sf con la columna de geometry incluida, o simplemente un data frame, elegimos ‘sf’. Finalmente, debemos indicar las variables intensivas o extensivas de los datos de origen. Las primeras hacen referencia a las variables que son porcentajes, mientras que las segundas hacen referencia a variables que son simplemente conteos. Nosotros tenemos una variable de cada una: pop014 es conteo mientras que avgAge es una variable extensiva. library(areal) nuevosDatos &lt;- aw_interpolate(source=datosCenso2001, sid = COD_2001, .data=datosCenso2010, tid = COD_2010, weight=&quot;sum&quot;, output = &quot;sf&quot;, extensive = &#39;pop014&#39;, intensive = &#39;avgAge&#39;) ## Error in aw_interpolate(source = datosCenso2001, sid = COD_2001, .data = datosCenso2010, : Data validation failed. Use ar_validate with verbose = TRUE to identify concerns. ¡No funcionó! Veamos por qué no funcionó usando la función ar_validate() que hace un chequeo de si la interpolación puede funcionar o no entre dos objetos ar_validate(source = datosCenso2001, target = datosCenso2010, varList = c(&#39;pop014&#39;,&#39;avgAge&#39;), verbose = TRUE) ## # A tibble: 6 x 2 ## test result ## &lt;chr&gt; &lt;lgl&gt; ## 1 sf Objects TRUE ## 2 CRS Match TRUE ## 3 CRS is Planar TRUE ## 4 Variables Exist in Source TRUE ## 5 No Variable Conflicts in Target FALSE ## 6 Overall Evaluation FALSE La función chequea 5 condiciones necesarias para que la interpolación funcione y nos indica si se cumple o no. En la primera se pregunta si ambos objetos son sf, la segunda si los sistema de coordenadas de referencia son iguales, la tercera si se encuentran proyectados, la cuarta nos indica si las variables que queremos convertir existen en el objeto de origen (source) y la quinta si no existen conflictos con el nombre de las variables en el objeto de destino. La sexta, overall evaluation, solo nos dice si todas se cumplen o no. En nuestro caso nos indica que hay algún conflicto en el objeto de destino. Lo que sucede es que las variables se llaman igual en el objeto de origen y destino, por lo que debemos cambiar los nombres de las columnas para que funcione aw_interpolate. Para esto, debemos usar la función colnames y paste0: colnames(datosCenso2001)[colnames(datosCenso2001) %in% c(&#39;pop014&#39;,&#39;avgAge&#39;)] &lt;- paste0(c(&#39;pop014&#39;,&#39;avgAge&#39;),&#39;_2001&#39;) colnames(datosCenso2001) ## [1] &quot;COD_2001&quot; &quot;PROV_&quot; &quot;DEPTO_&quot; &quot;pop&quot; ## [5] &quot;pop014_2001&quot; &quot;pop1564&quot; &quot;pop64plus&quot; &quot;highSchoolAboveALLPOP&quot; ## [9] &quot;terciarioAboveALLPOP&quot; &quot;universitaryAboveALLPOP&quot; &quot;highSchoolAbove2564&quot; &quot;terciarioAbove2564&quot; ## [13] &quot;universitaryAbove2564&quot; &quot;pop2564&quot; &quot;avgAge_2001&quot; &quot;descuentoJubil&quot; ## [17] &quot;descuentoJubilAsalariado&quot; &quot;asalariado&quot; &quot;cuentapropia&quot; &quot;servDomestico&quot; ## [21] &quot;asalariadoPrivado&quot; &quot;ocupadosUniversitariaComp&quot; &quot;geometry&quot; Ya sabemos que la función colnames() nos devuelve los nombres de las columnas de un data frame. Luego, elegimos aquellas columnas que tienen nombre de ‘pop014’ y ‘avgAge’. Finalmente, le asignamos lo que devuelve paste0(). Lo que hace es concatenar cadenas de texto, en este caso pop014 con _2001 y avgAge con _2001. Chequeen nuevamente los nombres de las columnas y verán que están cambiados. Ahora sí podemos correr nuestro código nuevosDatos &lt;- aw_interpolate(source=datosCenso2001, sid = COD_2001, .data=datosCenso2010, tid = COD_2010, weight=&quot;sum&quot;, output = &quot;sf&quot;, extensive = &#39;pop014_2001&#39;, intensive = &#39;avgAge_2001&#39;) colnames(nuevosDatos) ## [1] &quot;COD_2010&quot; &quot;PROV_&quot; &quot;DEPTO_&quot; &quot;pop&quot; ## [5] &quot;pop014&quot; &quot;pop1564&quot; &quot;pop64plus&quot; &quot;highSchoolAboveALLPOP&quot; ## [9] &quot;terciarioAboveALLPOP&quot; &quot;universitaryAboveALLPOP&quot; &quot;highSchoolAbove2564&quot; &quot;terciarioAbove2564&quot; ## [13] &quot;universitaryAbove2564&quot; &quot;pop2564&quot; &quot;avgAge&quot; &quot;ocupadosUniversitariaComp&quot; ## [17] &quot;geometry&quot; &quot;pop014_2001&quot; &quot;avgAge_2001&quot; 7.4 Haciendo mapas de nuestros nuevos datos Aprovechemos la nueva información que tenemos para poder hacer mapas. Usando ggplot2, grafiquemos la edad promedio en 2001, 2010 y la variación entre ambos censos. Lo primero que tenemos que hacer en ggplot, para que la leyenda represente correctamente los colores de nuestro mapa, es juntar todo en un mismo dataset que sea largo. Recuerden que esto es lo necesario para trabajar con paneles y facet_wrap(). Seleccionamos las dos y usamos pivot_longer() nuevosDatosLonger &lt;- nuevosDatos %&gt;% select(avgAge, avgAge_2001) %&gt;% pivot_longer(names_to = &quot;Year&quot;, values_to = &quot;AvgAge&quot;, cols = c(&quot;avgAge&quot;, &quot;avgAge_2001&quot;)) %&gt;% st_as_sf() select() y pivot_longer() no deberían traer demasiados problemas. Ahora bien, fijense que luego de hacer esto, usamos st_as_sf(), que como vimos en el capítulo de datos espaciales define a un data.frame como un objeto espacial sf. Esto no debería pasar, pero cuando usan pivot_longer, el data.frame deja de ser sf, pero como no pierde la columna de geometry, rápidamente podemos volver a convertirlo en un objeto sf. El mensaje que reciben es un error de cuando se usa pivot_longer. Queda resuelto cuando usamos st_as_sf() luego. Si quisieran, ya podrían hacer el gráfico, pero no sería el que ustedes quisieran guardar para compartir. Vamos a hacer algunos cambios, y de paso aprender un poco más sobre los factores en R. Para que facet_wrap() ponga en el lado izquierdo a 2001 y en el derecho a 2010, debemos convertir la variable Year a factor en el orden que queremos. nuevosDatosLonger &lt;- nuevosDatosLonger %&gt;% mutate(Year=factor(Year,levels = c(&quot;avgAge_2001&quot;,&quot;avgAge&quot;),labels = c(2001,2010))) Lo que hicimos es decirle a R que 1) queremos que la columna R se convierta a factor, 2) que los valores que tiene esa variable son “avgAge_2001” y “avgAge” y 3) que queremos los represente como “2001” y 2010, respectivamente. Internamente, R va a hacer que Year sea un vector numérico, con 1 represenetando a “2001” y 2 a “2010”, aunque todo el tiempo podemos trabajarlo como si fuera un vector de character. Esto es lo que logran los factores. nuevosDatosLonger$Year[1:6] # Lo que nos muestra ## [1] 2010 2001 2010 2001 2010 2001 ## Levels: 2001 2010 as.numeric(nuevosDatosLonger$Year)[1:6] # Como lo tiene en memoria ## [1] 2 1 2 1 2 1 Muy bien, ahora vamos a mejorar nuestro gráfico y, en lugar de hacerlo continuo, vamos a hacerlo discreto, haciendo que cada color represente el 25% de nuestros datos, lo que se conoce como cuartiles. Para discretizar un vector continuo, lo que tenemos que usar es cut(). Solo nos pide el vector y los puntos de quiebre. Estos puntos de quiebre no son otra cosa que un vector con estos valores. Veamos qué hace la funcion quantile() quantile(nuevosDatosLonger$AvgAge,na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 20.17079 38.02628 39.80790 41.38003 52.92446 De manera predeterminada, nos devuelvee los puntos de corte de los cuartiles, nada mal. El na.rm=TRUE es tan solo para que no tome en cuenta un valor NA que hay ene AvgAge, con el que lidiamos más adelante. Con estos datos podemos discretizar nuestra variable. nuevosDatosLonger &lt;- nuevosDatosLonger %&gt;% mutate(AvgAge=cut(AvgAge,breaks = quantile(AvgAge,na.rm = TRUE))) Perfecto, veamos qué es lo que hizo usando table() table(nuevosDatosLonger$AvgAge) ## ## (20.2,38] (38,39.8] (39.8,41.4] (41.4,52.9] ## 1776 1776 1776 1776 Lo hizo muy bien, pero esos intervalos los entendería solo una matemática. Podemos cambiarlos por algo mucho más interpretable: levels(nuevosDatosLonger$AvgAge) &lt;- c(&quot;De 20 a 38 años&quot;,&quot;entre 38 y 39.8 años&quot;,&quot;entre 39.8 y 41.4 años&quot;,&quot;más de 41.4 años y menos de 52.9&quot;) Por último, eliminamos los NAs que había en nuestra variable de edad nuevosDatosLonger &lt;- nuevosDatosLonger %&gt;% filter(!is.na(AvgAge)) Finalmente, podemos hacer nuestro gráfico. Si no comprenden totalmente el código, revisen el capítulo de visualizaciones. ggplot(nuevosDatosLonger) + geom_sf(col = NA, aes(fill=AvgAge)) + scale_fill_viridis_d() + labs(title=&#39;Edad promedio por radio censal en años&#39;, fill=&quot;&quot;) + coord_sf(datum = NA) + theme_minimal() + facet_wrap(~Year) + theme(legend.position = &quot;bottom&quot;) 7.5 Ejercicio La variable ocupadosUniversitariaComp indica la proporción de ocupados con estudios universitarios completos como el total de ocupados para cada uno de los radios censales en 2001 y 2010. Hagan un mapa usando ggplot que muestre a los datos de 2001 y 2010 en formato de polígonos 2010 en la Ciudad de Buenos Aire ¿Qué patrones encuentran? "]
]
