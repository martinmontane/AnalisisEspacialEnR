---
title: "Herramientas de análisis espacial en R"
author: "Martin Montane"
description: "Usando R para analizar datos espaciales"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook
documentclass: book
---
# Herramientas de análisis espacial en R {-}

Los datos espaciales requieren un tratamiento particular tanto en su representación, su almacenamiento, sus transformaciones, sus visualizaciones y su análisis. En este compendio de notas de clase se introducen las herramientas necesarias para comenzar a aprovechar todas las oportunidades que los datos espaciales nos brindan.

## ¿Qué necesitamos para arrancar? {-}

En estas notas de clase se usa el lenguaje de programación R. Recomiendo utilizar RStudio para que nos ayude con los proyectos y la edición de los códigos en R Descargar instalar estos dos softwares es muy simple ya que son gratuitos. R de hecho es un lenguaje de programación *open source*, o de código abierto, lo que significa que cualquiera puede colaborar. [Haciendo click aquí](https://cran.r-project.org/) van a poder descargar la última versión de R para Windows, Mac o Linux. Una vez que lo hayan descargado solo tienen que instalarlo.

Ahora [descarguen RStudio](https://www.rstudio.com/products/rstudio/download/), también van a poder elegir la versión que corresponde según su sistema operativo. RStudio va a identificar automáticamente la versión de R que ya tienen instalada, **por lo que es importante que instalen RStudio luego de haber instalado R**. Una vez que tienen todo esto instalado pueden pasar al primer capítulo de este libro

<!--chapter:end:index.Rmd-->

# Datos espaciales en R

```
Al terminar este capítulo ustedes van a poder:
- Comprender por qué los datos espaciales son distintos al resto de los datos
- Las dificultades de la representación de esos datos y los estándares utilizados
- Trabajar con datos espaciales en R: su importación, manipulación e introducción a los gráficos
- Identificar los principales tipos de archivos donde suelen compartirse estos datos
```
## ¿Qué es un dato espacial?

Un dato **espacial** o **georreferenciado** tiene una característica que lo hace único: posee información sobre su ubicación en la Tierra. No es el único tipo de dato que tiene particularidades, por ejemplo las **series de tiempo** tienen información sobre un específico período de tiempo donde se registró la información. Esto trae importantes consideraciones al momento de realizar el análisis estadístico, lo que generó el desarrollo de toda una rama de la estadística. No obstante, los **datos espaciales** no presentan un desafío solo al momento de su análisis, sino que presentan específicidades en la forma de representar su información geográfica y realizar transformaciones en los datos.

## ¿Dónde estamos en la Tierra?

La respuesta a esta pregunta puede ser un poco más compleja de lo que uno piensa, al menos si desea realizar un análisis con esta información. La respuesta más fácil en este momento sería decir: Av. Figueroa Alcorta 7350, Ciudad de Buenos Aires, Argentina. Bien, es un primer paso. Ahora: ¿Cómo calculamos la distancia con respecto a Abbey Road 2, Londres, Inglaterra, donde se encuentra el famoso cruce peatonal de la tapa del disco de los Beatles, _Abbey Road_? Imposible saberlo solo con esa información.

Si nosotros introdujéramos esos datos en un GPS (o _Google Maps_), lo que haría es _traducir_ las direcciones que les pasamos a un sistema de grillas que divide al globo en celdas en base a líneas imaginarias en sentido paralelo a los polos (paralelos) y perpendicular a ellos (meridianos). Nuestra dirección quedaría transformada directamente en un vector con dos posiciones: **latitud** y **longitud**. Ahora "Av. Figueroa Alcorta 7350, Ciudad de Buenos Aires, Argentina" se convirtió en (-34.714656,-58.785999) y "Abbey Road 2, Londres, Inglaterra" en (51.532068, -0.177305). Las latitudes y longitudes se expresan en **grados**, así que ya podemos establecer una diferencia cuantitativa entre nuestras dos posiciones ¡Incluso podemos expresarlo en una medida de distancia como metros o kilómetros!

Para esta clase vamos a necesitar varios paquetes, así que los cargamos. Recordemos que si no están instalados hay que usar la función `install.packages()`

```{r message=FALSE , tidy=TRUE, tidy.opts=list(width.cutoff=60) }
library(tidyverse) # Paquete multiuso
library(sf) # Paquete clave para manipular datos espaciales
library(leaflet) # Uno de los paquetes para 
```

Una vez que los cargamos, vamos a crear nuestro dataframe con datos espaciales en base a las coordenadas latitud y longitud que definimos anteriormente:

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Creamos un Data Frame con los datos necesarios
datos <- data.frame(lat = c(-34.714656, 51.532068),
                    long = c(-58.785999, -0.177305),
                    ubicacion = c("UTDT", "Abbey Road"))
# Lo convertimos a un objeto sf
puntosEspaciales <- st_as_sf(datos, 
                             coords = c("long", "lat"),
                             crs = 4326)
# st_distance() nos permite encontrar la diferencia en la unidad que diga el CRS (sistema de coordenadas de referencia)
st_distance(puntosEspaciales) # En metros
st_distance(puntosEspaciales)/1000 # En kilómetros
```

Según estos cálculos, nos separan aproximadamente 11.131 kms de Abbey Road. Perfecto, pudimos definir nuestra ubicación en la tierra e incluso medir la distancia con otro punto. Hay un parámetro que todavía no introdujimos y que resulta clave cuando lidiamos con datos espaciales: **CRS**, las siglas de Coordinate Reference System.

Incluso podemos hacer nuestro primer gráfico interactivo de una manera muy rápida

```{r}
leaflet(puntosEspaciales) %>% 
  addTiles() %>% 
  addMarkers()
```

Vayamos paso por paso. En la siguiente sección veremos los distintos modelos de la tierra que usamos para poder representar estas ubicaciones espaciales.

## Coordinate Reference Systems

### Elipsoides, sistemas de coordenadas y datums

Representar una ubicación en la superficie de la tierra implica superar diversos obstáculos. Para empezar, la tierra no es una esfera: tiene una forma que suele modelarse como **geoide**, pero incluso eso es una aproximación. La tierra tiene una forma particular, con diversos accidentes geográficos que la hacen única (y difícil de manipular matemáticamente). Sin embargo, nosotros - y a fines prácticos, todas las personas que trabajan con datos georreferenciados - trabajamos en su versión como **geoide**, y es en relación a esta modelización de la tierra que se montan los CRS.

Definido el **geoide**, ese modelo de la forma de la tierra, introducimos el primer componente de los CRS: el **elipsoide**. El elipsoide es una aproximación al **geoide** con mejores propiedades matemáticas. Para definir un **elipsoide** necesitamos un par de parámetros que definen su forma. Una vez que contamos con un **elipsoide** podemos establecer un sistema de grillas tridimensional, como el de latitud y longitud, que lo segmenta según los ángulos que se forman entre la línea imaginaria paralela a los polos (paralelos) y la línea imaginaria perpendicular a los polos (meridiano) en un determinado punto, en relación al paralelo y meridiano de origen.

Pero ¿cómo relacionamos al **elipsoide** con el **geoide**? Si bien el primero es una aproximación del segundo, para establecer un CRS necesitamos saber como se relacionan entre ellos: tenemos que "fijar" el **elipsoide** al **geoide**. Esto es lo que hace el **datum**: define el origen y la orientación de los ejes de coordenadas. Piensen en el **datum** como la información necesaria para "dibujar" el **sistema de coordenadas** en el **elipsoide**

Entonces ya tenemos tres elementos que poseen los CRS:

1. Un **elipsoide** (un modelo de la tierra, en rigor de un  **geoide**)
2. Un **sistema de coordenadas**, que nos permite determinar la posición de un punto en relación a otro en base a líneas imaginarias
3. Un **datum**, que nos permite dibujar ese **sistema de coordenadas** en el **elipsoide** de tal manera que represente al ubicaciones específicas en el **geoide**

Si no quedó del todo claro no se preocupen: es un tema complejo que, en la mayoría de los casos, solo basta con saber que estos conceptos existen y qué significan. El objetivo de esta subsección es dar la definición básica de cada elemento porque probablemente se encuentren con esta información en diversos lugares, pero a fines prácticos suele utilizarse siempre el mismo elipsoide, datum y sistema de coordenadas, o variaciones que no tienen grandes efectos a los fines prácticos de nuestros trabajos. El World Geodetic System (**WGS84**) es un standard en la industria a nivel mundial, y existen algunas variaciones locales (la más famosa, el North American Datum (**NAD83**)) que no nos traerán mayores problemas al momento de las transformaciones. Piensen en el CRS como las unidades de peso o de distancia: cada observación que veamos de datos espaciales corresponde a un determinado CRS y no corresponde hacer operaciones entre dos observaciones pertenecientes a distintos CRS.

### Proyecciones

Hasta ahora hemos trabajado en la representación de la Tierra en tres dimensiones. Sin embargo, todos los mapas con los que hemos trabajado desde chicos tienen dos dimensiones ¿Cómo transformamos un objeto de tres dimensiones a uno de dos dimensiones? Debemos realizar **proyecciones** de ese objeto tridimensional que, como veremos en breve, involucra diversos **tradeoffs**[^1]. Piensen en la proyección como una tarea de traducción: algo se pierde en el proceso.

La proyección hoy en día más famosa es **MERCATOR**, la proyección que usa, entre otros servicios, **Google Maps**. Diseñada hace ya varios siglos para la navegación marítima, esta transformación es relativamente buena en lo relativo preservar formas y útil para navegar.

En lo que realmente falla este tipo de proyección es en definir el tamaño de las unidades geógraficas: los países que están cerca de los polos aparentan tener un tamaño mucho más grande del que realmente tienen, mientras que lo inverso sucede con los que están cerca de la línea del ecuador. Tal es así que existe una página web (https://thetruesize.com/) que permite experimentar de manera interactiva con los tamaños de los países en diversas partes de la proyección. En la Figura 1 muestro un ejemplo: Groenlandia, Islandia, Noruega, Suecia, Finlandia y Reino Unido combinadas ocupan aproximadamente el 50% de Brasil (Figura 1).

```{r out.width="800px",echo=FALSE, fig.cap="La proyección MERCATOR distorsiona nuestra percepción de los tamaños", fig.align="center", fig.pos="htb!"}
knitr::include_graphics(path = "data/Tutorial 3/projections.png")
```

La oferta de proyecciones es prácticamente ilimitada. El paquete `mapproj` en R nos permite transformar el mundo en base a diversas proyecciones, incluyendo algunas que preservan el tamaño de los países. La Figura 2 muestra el mundo desde otra perspectiva: los países del norte son más chicos de lo que parecen en la proyección mercator.

```{r out.width="800px",echo=FALSE, fig.cap="La proyección MOLLWEIDE mantiene la representación de los tamaños", fig.align="center", fig.pos="htb!"}
knitr::include_graphics(path = "data/Tutorial 3/mollyout.png")
```

Las proyecciones también forman parte de los CRS, que pueden o no tener una proyección. Sea como sea, lo importante de esta sección es haberlos convencido de que importa **conocer en que CRS están expresados los datos espaciales**. Las transformaciones entre CRS no hace falta conocerlas, sino que el paquete **sf** lo hará por nosotros. Insisto: lo importante es saber que los datos espaciales SIEMPRE tienen un CRS, aun si no está definido explícitamente en nuestro archivo. Volvamos al ejemplo de los inmuebles de las propiedades de la introducción de este libro para ver un qué formato de archivos tienen los datos espaciales y un ejemplo sobre transformación de CRS.

## Manos a la obra ¿Dónde construir el próximo centro de salud?

Mostremos algunas de las funciones de datos espaciales de R con un problema muy concreto. Pónganse en la piel de una funcionara pública que debe decidir en qué manzana específica de la Ciudad de Buenos Aires debe abrir un nuevo centro de salud. Existen múltiples maneras de lidiar con este problema, pero supongamos por un segundo que esta funcionaria sabe como trabajar con GIS y, específicamente, con R. Antes de escuchar las demandas de los habitantes, prefiere conocer, en base a los datos, en qué manzanas hay una mayor necesidad de construir un nuevo centro de salud. Para eso, va a utilizar distintos conjuntos de datos y herramientas de R.

El objetivo va a ser generar un Índice de Demanda de Salud (IDS) que, para cada manzana de la Ciudad de Buenos Aires, nos va a indicar qué tan necesaria es la construcción de un centro de salud. Este indicador se va a basar en distintas variables, a saber:

- La densidad poblacional en la zona
- La cantidad de hogares con Necesidades Básicas Insatisfechas (NBI)
- La atención preexistente por el sistema de salud
- La distancia con la avenida más cercana

Veamos de dónde podemos conseguir estos datos y cómo podemos leerlos en R

### Cargando los datos espaciales

Los datos espaciales, a diferencia de otros tipos de datos, tienen formatos específicos para su almacenamiento. No es el objetivo de este libro introducir a todos los formatos, que son efectivamente muchos. Vamos a leer datos de tres formatos distintos: 

- **Geojson**: Se trata de una forma de representación de datos como listas, siguiendo el formato de *json*, pero adaptado para almacenar datos espaciales. Por default, el sistema de coordenada de referencias es el EPSG número 4325, que utiliza el WGS 84, el standard más utilizado en el mundo. El hecho de que lo guarde por default con este CRS es muy importante: no hace falta información de contexto sobre en qué CRS está, porque solo puede estar en ese-

- **Shapefiles**: Los shapefiles son un formato antiguo para almacenar datos espaciales, y es propiedad de la empresa ESRI (los creadores de ArcGIS). Los shapefiles están siempre compuestos múltiples archivos, cada uno cumpliendo una función. Los datos específicos de las coordenadas se encuentran en un archivo .shp, pero la proyección se encuentra en .proj, y no siempre viene incluida en nuestros datos.

- **csv**: los famosos archivos separados por comas pueden tener información espacial dentro dellos, particularmente cunado se trata de puntos. Suelen especificarse coordenadas como columnas "X" e "Y" o "lat" y "lon"


El GCBA ofrece un dataset espacial con todas las vías de circulación de autos en la Ciudad de Buenos Aires (en este link)[https://data.buenosaires.gob.ar/dataset/calles]. Les va a dar la opción de descargar los datos en shapefiles o geojson. Descarguen el shapefile (estará comprimido en un .zip) y extraiganlo en la carpeta del proyecto.

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(sf)
calles <- read_sf("data/callejero-rar/callejero.shp")
```

Para leer a este y al resto de los archivos espaciales - con excepción de csv - podemos usar **read_sf()**. Acá estoy suponiendo que tienen a todos los archivos de este shapefile dentro de la carpeta callejero-rar/

```{r message=FALSE, warning=FALSE, eval=FALSE}
# Cargamos la librería de SF
library(sf)
calles <- read_sf("callejero-rar/callejero.shp")
```

Si lograron hacerlo, entonces deberían tener un objeto que se llama **calles**. Lo primero que tenemos que entender cuando leemos datos espaciales es el sistema de coordenadas de referencia en el que están representados. Esto podemos hacerlo con **st_crs()**

```{r}
st_crs(calles)
```

En este caso nuestro dataset de calles está expresado en el EPSG 4326 ¿Qué significa esto? El EPSG es un sistema con índices de sistema de coordenadas de referencia. Cada uno de estos números representa a cada combinación de parámetros que determinan un sistema de coordenadas de referencia. Pueden buscar estos números en https://spatialreference.org/. [Aquí pueden ver específicamente información sobre el 4326](https://spatialreference.org/ref/epsg/wgs-84/).


Ahora carguemos información sobre la población y la cantidad de hogares con necesidades básicas insatisfechas - una forma de medir la pobreza - por radio censal según el censo 2010, siendo los radios censales la mínima unidad de medida del censo. Nuevamente vamos a poder descargar estos datos desde [aquí](https://data.buenosaires.gob.ar/dataset/informacion-censal-por-radio). Esta vez descarguen la opción de **geojson** y, nuevamente, guárdenlo en la carpeta de su proyecto

```{r message=FALSE, warning=FALSE, echo=FALSE}
radiosCensales <- read_sf("data/caba_radios_censales.geojson")
```

```{r message=FALSE, warning=FALSE, eval=FALSE}
radiosCensales <- read_sf("caba_radios_censales.geojson")
```

En este caso no necesitamos saber cuál el es el sistema de coordenada de referencias... siempre será el WSG84. Veamos qué variables trae

```{r}
glimpse(radiosCensales)
```

¿Ven esa columna **geometry**? Es donde están almacenados nuestros datos espaciales, en este caso son polígonos. Es una forma muy prolija de guardar la información espacial, al mismo tiempo que el resto de las variables pueden trabajarse como si fuera un data.frame normal. Si están atentos y atentas se van a dar cuenta de que no tenemos una medida de densidad... todavía, ya lo vamos a solucionar.

Finalmente, nos falta información sobre la oferta actual de cobertura de salud, para esto vamos a usar solamente a los hospitales de la Ciudad de Buenos Aires, siendo conscientes de que existen otras formas de brindar atención de salud, como los Centros de Salud y Acción Comunitaria (CeSACs).

El dataset de hospitales se encuentra en un csv al que le hice pequeñas modificaciones y pueden descargar haciendo [click acá](https://raw.githubusercontent.com/martinmontane/AnalisisEspacialEnR/master/data/Hospitales.csv)

```{r message=FALSE, warning=FALSE, echo=FALSE}
hospitales <- read_delim("data/Hospitales.csv",delim = ";")
```

```{r message=FALSE, warning=FALSE, eval=FALSE}
hospitales <- read_delim("Hospitales.csv",delim = ";")
```

Todavía esto no es un dato espacial... al menos no para R: para eso tenemos que convertirlo en un objeto SF, que tenga a las coordenadas en la columna **geometry**. Veamos qué columnas tiene

```{r}
glimpse(hospitales)
```

Entre todas las variables, **long** y **lat** tienen las variables espaciales. Podemos convertir este data.frame en un objeto sf, es decir espacial, de la siguiente manera:

```{r}
hospitales <- st_as_sf(hospitales,coords=c("long","lat"), crs=4326)
```

Nos falta el último de nuestros datos: las manzanas. Pueden [descargar el geojson de acá](https://data.buenosaires.gob.ar/dataset/manzanas)

```{r message=FALSE, warning=FALSE, echo=FALSE}
manzanas <- read_sf("data/manzanas.geojson")
```

```{r message=FALSE, warning=FALSE, eval=FALSE}
manzanas <- read_sf("manzanas.geojson")
```

Ahora sí ya podemos aplicar las distintas herramientas de análisis espacial para elegir las mejores ubicaciones para el nuevo centro de salud.

### Detectando la cobertura actual.

Una de las variables que dijimos que ibamos a tomar para detectar un espacio para un nuevo centro de salud iba a ser que la zona no estuviera atendida por un hospital. Existen distintas maneras de cubrir esta atención existente, pero vamos a usar una herramienta en particular: **st_buffer()**. Lo que hace esta función de sf es generar un polígono a una distancia fija desde cualquiera de los puntos de nuestros objetos espaciales. En el caso de puntos - como nuestros hospitales - esto significa un círculo del radio que nosotros querramos.

Una recomendación cuando trabajemos con funciones que requieren medir distancias y relaciones entre distintos objetos espaciales es que trabajemos con CRS que se encuentren proyectados en dos dimensiones, en lugar del ESPG 4326 que se encuentra no proyectado. Usemos la proyección oficial del GCBA, cuya definición podemos encontrar acá https://spatialreference.org/ref/sr-org/8333/.

Vamos a usar la función **st_transform()**, que es la que nos permite transformar entre sistemas de coordenadas de referncia. No vamos a usar los códigos de EPSG, sino la representación proj4string, que la pueden ver en el link que puse anteriormente.

```{r}
hospitales <- st_transform(hospitales, crs="+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
```

Si usan **st_crs(hospitales)** van a ver que es distinto ¿Ven la parte de +units=m? Bueno, eso implica que ahora todo lo que hagamos en términos espaciales será tomado en metros, por lo cual podemos encontrar círculos a una distancia fija, por ejemplo, 1000 metros, de cada hospital

```{r}
coberturaHospitales <- st_buffer(hospitales,dist = 1000)
```

Como alguno de los circulos se van a solapar, mejor juntemos todo en un gran polígono que diga que en esa zona hay cobertura, con herramientas de tidyverse

```{r}
coberturaHospitales <- coberturaHospitales %>% summarise(cobertura=TRUE) 
```

Dejemos por este dataset cargado en nuestra sesión de R por un rato, ya vamos a volver a él

### Distancia contra la avenida más cercana

Definitivamente es importante tener una buena conexión con la red de transporte para poder mover mejor a los eventuales pacientes. Teniendo nuestras calles, podemos seleccionar solo las avenidas y despues usar **st_distance()** para estimar la distancia entre cada una de nuestras manzanas y las avenidas.

```{r}
avenidas <- calles %>% filter(tipo_c == "AVENIDA")
```

Antes de usar las avenidas, lo que vamos a hacer es quedarnos con los centroides de las manzanas. Los centroides son el punto central dentro de un polígono. Hacemos esto especialmente por las manzanas que son muy grandes, para tener un "promedio" en lugar de uno solo de sus puntos externos. Es muy fácil tener los centroides, utlizando st_centroid(). Antes de hacerlo, vamos a convertir a manzanas al crs de hospitales, es decir el de la ciudad de buenos aires:

```{r}
manzanas <- st_transform(manzanas,crs = st_crs(hospitales))
```

Ahora vamosa extraer los centroides de cada una de las manzanas y hacemos un gráfico para verlo en detalle:

```{r message=FALSE, warning=FALSE}
manzanasCentroides <- st_centroid(manzanas)
ggplot() +
  geom_sf(data = manzanas) +
  geom_sf(data = manzanasCentroides,color="red", size= 0.001)
```

Si no se distinguen los puntos en este gráfico, hagánlo en su computadoras y van a poder cambiar el zoom para ver cómo cada punto está en el centro de cada manzana. Ahora midamos la distancia entre cada uno de estos puntos y todas las avenidas, pero tienen que estar en el mismo Sistema de Coordenadas de Referencia:

```{r}
avenidas <- st_transform(avenidas,crs = st_crs(hospitales))
```

Ahora sí, solo hay que esperar un poco, puede tardar unos minutos:
```{r}
distanciaAvenidas <- st_distance(manzanasCentroides,avenidas)
```

Lo que acaba de hacer es computar la distancia, en metros, entre cada uno de los centroides y las avenidas, con dim podemos ver que efectivamente esto es lo que sucedió

```{r}
# 12.520 filas (centroides de manzanas) y 6,758 columnas (tramos de avenidas)
dim(distanciaAvenidas)
```

Sin embargo, nuestro objetivo es tener la distancia mínima por lo cual podemos hacer uso de **apply**. Es una función muy poderosa que recorre nuestra matriz en filas o culmnas haciendo algo que queremos. Por ejemplo, podemos recorrer todas las filas y tomar el valor mínimo, con lo cual nos quedaríamos con el valor mínimo para cada uno de los centroides de manzanas.


```{r}
# 1 significa filas, 2 columnas. functon(x) min(x) significa que para cada fila devuelva el valor mínimo
avenidaMasCercana <- apply(distanciaAvenidas,1,function(x) min(x))
# Rendondeamos
avenidaMasCercana <- round(avenidaMasCercana,0)
```

Agregamos esta distancia a cada una de las manzanas

```{r}
manzanas <- manzanas %>% mutate(distanciaAvenida=avenidaMasCercana)
```
 
### Incorporando el resto de las variables

Ahora ya podemos agregar a los centroides, y luego a las manzanas, la información que nos falta para la construcción del índice. Para empezar, podemos hacer lo más fácil, que es marcar si el centroide está dentro de los radios de cobertura que dijimos anteriormente. Para esto, usamos **st_join()**, es decir la **herramienta de unión espacial**. Lo que hace es unir la información de dos **datasets espaciales distintos** en base a algún criterio espacial. De manera predeterminada, utiliza el criterio **st_intersects()**, es decir si existe algún tipo de intersección entre cada una de los puntos, lineas o polígonos que están en cada uno de nuestros objetos **sf**.

```{r}
manzanasCentroides <- st_join(manzanasCentroides,coberturaHospitales)
```

Igual que el **left_join()**, cuando no encuentra ningun matcheo entre las unidades devuelve **NA**, por lo que todos los valores que están en cobertura como NA en rigor no están cubiertos por la oferta de hospitales actual. Dejamos eso en claro:

```{r}
manzanasCentroides <- manzanasCentroides %>% mutate(cobertura=ifelse(is.na(cobertura),FALSE,cobertura))
```

Ahora tenemos que hacer algo muy similar con los datos del censo 2010, que lo tenemos en el objeto **radiosCensales**.

```{r}
radiosCensales <- radiosCensales %>% mutate(densidadPob=POBLACION/AREA_KM2)
radiosCensales <- radiosCensales %>% st_transform(radiosCensales,crs=st_crs(hospitales))
manzanasCentroides <- st_join(manzanasCentroides,radiosCensales)
```

Nos queda agregar toda esta información a los polígonos de las manzanas y ya podemos ponernos a construir, finalmente,nuestro índice

```{r}
# Elegimos solo las variables que queremos unir, antes lo convertimos en data frame para poder perder la columna geometry
manzanasCentroides <- manzanasCentroides %>% 
                      as.data.frame() %>%
                      select(SM,densidadPob,HOGARES_NBI,cobertura)
manzanas <-  left_join(manzanas,manzanasCentroides,by="SM")
```

### Creando nuestro índice de demanda de salud


Nuestro índice de demanda de salud va a estar compuesto por un promedio ponderado de variables que construimos, pero anteriormente deberíamos normalizarlas. Lo que vamos a hacer es ponerle un valor del 1 al 5 si está entre el 0%-20%, 20%-40%, 40%-60%, 60%-80% u 80%-100% de esa variable. Esto lo podemos hacer fácilmente con la función **quantile()**, que calcula justamente estos quiebres, veamos:

```{r}
# Queremos que nos muestre en que porcentaje de estos está cada observación...
quiebres <- c(0,0.2,0.4,0.6,0.8,1)

manzanas <- manzanas %>%
            mutate(cat_densidad=cut(densidadPob,breaks = quantile(densidadPob,quiebres,na.rm = TRUE ),include.lowest = TRUE),
                   cat_NBI=cut(HOGARES_NBI,breaks = quantile(HOGARES_NBI,quiebres,na.rm = TRUE ),include.lowest = TRUE),
                   cat_distanciaAv=cut(-distanciaAvenida,breaks = quantile(-distanciaAvenida,quiebres,na.rm = TRUE ),include.lowest = TRUE))
```

Vamos parte por parte. La función **cut()** corta a una variable númerica por los cortes que nosotros le indiquemos en el parámetro *breaks*. Por otro lado, la función **quantile()** nos devuelve los valores de una variable numérica que alcanza a un determinado porcentaje cuando se ordena de mayor a menor (es decir, calcula el percentil). Como buscamos los puntos que acumulan 0%, 20%, 40%, 60%, 80% y 100%, por los valores que le ponemos al vector quiebres, entonces nos va a devolver esos valores. **na.rm=TRUE** dentro de quantile() es para que ignore los casos donde hay NA en la variable, mientras que **include.lowest = TRUE** es para que cut tome al primer valor, 0, y no lo excluya de la nueva variable categórica.

Además, ponemos **-distanciaAvenida** porque es un truco para que nos quede como valor más alto (5) cuando la distancia es menor a la avenida y más bajo (1) cuando la distancia es mayor a la avenida.

Si todavía no quedó del todo claro, un gráfico habla más que mil palabras

```{r}
ggplot() +
  geom_sf(data=manzanas %>% filter(!is.na(cat_NBI)) ,aes(fill=cat_NBI), color=NA) +
  scale_fill_viridis_d() +
  theme_minimal() +
  coord_sf(datum=NA)
```

Bien, ahora una última vuelta de tuerca: podemos convertir estas categorías a números con la función **as.numeric()**, y estarán ordenados de 1, menor valor, a 5, mayor valor ¡Exactamente lo que queríamos! Esto es por cómo funcionan los **factores** en R

```{r}
# En el caso de cobertura convertira 0 cuando era FALSE y 1 cuando era TRUE
manzanas <- manzanas %>%
            mutate(cat_densidad=as.numeric(cat_densidad),
                   cat_NBI=as.numeric(cat_NBI),
                   cat_distanciaAv=as.numeric(cat_distanciaAv),
                   cat_cobertura=as.numeric(!cobertura))
```

Ya podemos crear nuestro índice con las ponderaciones que queramos. Usemos un 10% para la densidad, un 30% para los NBI, un 10% para la distancia con la avenida más cercana y un 50% sobre si en el lugar falta o no cobertura.

```{r}
manzanas <- manzanas %>%
            mutate(IDS=cat_densidad*0.1+cat_NBI*0.3+cat_distanciaAv*0.1+cat_cobertura*0.5,
                   IDS=ifelse(IDS>quantile(IDS,probs = 0.9,na.rm = TRUE),TRUE,FALSE))
```

Hagamos nuestro gráfico:

```{r}
# Podemos agrupar a los radioscensales por barrio para que nos queden los polígonos de los barrios.
# Tambien es posible bajarlos directamente desde la página del GCBA
barrios <- radiosCensales %>% group_by(BARRIO) %>% summarise(n())
ggplot() +
  geom_sf(data=barrios) +
  geom_sf(data=manzanas %>% filter(!is.na(IDS)) ,aes(fill=IDS), color=NA) +
  scale_fill_manual(values = c(NA,"#f03b20")) +
  guides(fill=FALSE) +
  theme_minimal() +
  coord_sf(datum=NA) +
  labs(title="Índice de Demanda de Salud",
       subtitle="Ciudad de Buenos Aires")
```


## Ejercicio

La Ciudad de Buenos Aires cuenta con otras alternativas a los hospitales, por ejemplo los CESAC. Descargá los datos de los CESAC y agregalos al gráfico donde se encuentran las áreas más acuciantes del IDS ¿Coinciden las zonas? ¿No? ¿Dónde construirían el próximo CESAC, ahora que saben dónde están emplazados los actuales? 


<!--chapter:end:DatosEspaciales.Rmd-->

# Geocoding: de la representación humana al sistema de coordenadas

En capítulos anteriores ya descubrimos la particularidad de la representación de los datos espaciales: los ubicamos en base a un modelo de la tierra en lo que se conoce como Coordinate Reference System (CRS). Esta forma de representación es muy distinta a la que tenemos en nuestras cabezas cuando nos dicen que tenemos que ir a cursar a Figueroa Alcorta 7350. En diversas circunstancias vamos a necesitar convertir esta direcciones a puntos espaciales para poder agregar información relevante para nuestros análisis y encontrar nuevos patrones en nuestros datos.
Veamos cómo podemos hacerlo usando servicios del Estado de Argentina y también de Google.

## ¿Qué es la geocodificación o geocoding?

Geocoding no es otra cosa que la transformación de una ubicación en el formato que manejamos a diario hacia una coordenada en un sistema de coordenadas de referencia (CRS, en inglés). Esta simple operación es sumamente útil para muchos de nuestros objetivos. Imaginemos que queremos tener alguna medida de la cobertura de atención para la salud en la Ciudad de Buenos Aires. Tenemos la dirección de distintos hospitales y centros de salud, pero no conocemos cómo se distribuyen en el espacio. Carguemos primero estos datos [desde aquí](https://raw.githubusercontent.com/martinmontane/AnalisisEspacialEnR/master/data/HospitalesYCentrosSalud.csv)

```{r eval=TRUE, message=FALSE}
library(tidyverse)
salud <- read_csv("https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/HospitalesYCentrosSalud.csv")
```

Y veamos un poco las variables que tenemos

```{r}
glimpse(salud)
```

Geocodificemos con la ayuda del paquete **wrapar**

```{r message=FALSE,results=FALSE}
# Si no lo tenés insalado
# require(devtools)
# install_github("martinmontane/wrapar")
library(wrapar)
# Agregamos una variable de ID y una columna que indique la provincia
salud <- salud %>%
         mutate(id=row_number(),
                provincia="Ciudad de Buenos Aires")
saludGeoreferenciado <- geocodeDirecciones(datos = salud,
                                           col_id = "id",
                                           col_direccion = "Dirección",
                                           col_provincia = "provincia")
# Seleccionamos las que tuvimos algún match
saludGeoreferenciado <- saludGeoreferenciado %>% 
                        filter(nMatchAPI %in% 1)
# Le agregamos información que estaba en el anterior data.frame
saludGeoreferenciado <- left_join(saludGeoreferenciado,
                                  salud,
                                  by=c("id"))
library(sf)
# Convertimos a objeto sf
saludSf <- st_as_sf(saludGeoreferenciado,
                    coords=c("ubicacion.lon","ubicacion.lat"),
                    crs=4326)

```

Con la ayuda de leaflet hagamos un simple mapa interactivo

```{r}
library(leaflet)
leaflet(saludSf) %>% 
  addTiles() %>%
  addMarkers(label = ~ Establecimiento,
             popup = ~ Tipo)
```

Nada mal, no? No se preocupen si no entienden lo que hicimos, la única idea de esta introducción es mostrarle lo que van a ser capaces de hacer, nada más ni nada menos. Vamos a ir explicando cómo funciona todo esto.

## API: Interfaz de programación de aplicaciones

Sin saberlo, en el ejemplo anterior usamos la [API de georreferenciación de Argentina](https://datosgobar.github.io/georef-ar-api/) a través de la función **geocodeDirecciones()** del paquete wrapar ¿Qué es una API? Una API es un conjunto de reglas preestablecidas que nos permiten comunicarnos con servicios que están escritos en diferente lenguaje y con un conjunto de procedimientos específicos. Imaginenlo como que nuestro código de R es español, y la API de geolocalización del Gobierno está en francés, la API podría ser un idioma intermedio, como el inglés, para comunicarnos. Si quieren aprender como comunicarse sin hacerlo por intermedio de **wrapar** [pueden aprenderlo leyendo los documentos del desarrollo del gobierno]. La idea de wrapar es no tener que aprender otro idioma y hacer todo desde R.

Las APIs exceden a este desarrollo particular del gobierno y casi cualquier servicio de cualquier empresa tiene una API para que distintos usuarios puedan hacer consultas sin tener que conocer específicamente cómo es que el servicio trabaja por detrás, es muy útil y eficiente. En este capítulo vamos a usar dos APIs: la del Gobierno, que ya fue presentada, y la de Google. Veamos las funcionalidades y ventajas y desventajas que cada una tiene.

## Servicio de Normalización de Datos Geográficos de Argentina

Podemos comunicarnos con el servicio de normalización de datos geográficos de Argentina mediante la función **geocodeDirecciones()** del paquete **wrapar**. La función necesita que le pasemos algunos parámetros para poder hacer bien su trabajo.

- **datos**: En este parámtro simplemente hay que poner el data.frame que tiene la información que querés georeferencair
- **col_id**: es el nombre de la columna que tiene los códigos identificadores únicos de cada uno de los puntos. La función lo hace obligatorio porque va a ser la columna que después va a ser útil para incorporar el resultado de la geocodificación.
- **col_direccion**: es el nombre de la columna donde se encuentra la dirección, sin incluid información sobre la localidad, provincia, pais, etc
- **col_provincia**: es el nombre de la columna que tiene el nombre de la provincia

Con estos cuatro parámetros, cómo hicimos anteriormente, la función pasa las dirección a la API y la API nos devuelve un data.frame con las siguientes variables:

- **id**: la columna que identifica a cada uno de los puntos
- **nMatchAPI**: nos dice cuántos resultados encontró la API para esa dirección (puede ser más de uno)
- **codigoAPI**: Un código que dice "Exito" cuando se pudo comunicar con la API o "Error" cuando hubo algún problema en la comunicación
- **calle.nombre, departamento.nombre, localidad_censal.nombre, y nomenclatura**: variables donde tenemos más información sobre la dirección que encontró la API
- **ubicacion.lat y ubicacion.lon** las coordenadas de latitud y longitud en el sistema de coordenads EPSG 4326

Ahora que ya sabemos cómo funciona, relean el código que estaba en la parte de arriba ¿Cuántos puntos pudo geolocalizar? 97. En principio, no sabemos si están bien o mal (la API puede haber devuelto simplemente un punto que no correspondía), pero podemos estar seguros que para 18 de nuestros 115 casos no encontró ninguna respuesta. Esto suele pasar cuando la API no es capaz de mapear la dirección que le pasamos con otra que si reconozca. Usando otras herramientas vistas anteriormente, hagamos un mapa de distancia mínima entre cada una de las manzanas de la Ciudad de Buenos Aires y los puntos de los establecimientos de salud.

Lo primero que tenemos que hacer para medir las distancias es tener proyectados a los CRS para que la distancia midan metros y no diferencias entre coordenadas latitud y longitud.

```{r message=FALSE}
manzanas <- read_sf("http://cdn.buenosaires.gob.ar/datosabiertos/datasets/manzanas/manzanas.geojson")
# El objeto manzanas está representado en el CRS que corresponde al ESPG 4326
st_crs(manzanas)
# El objeto de saludSf no tiene información sobre el CRS de las coordenadas, pero sabemos que es 4326
st_crs(saludSf) <- 4326
# Convertimos a los dos a la proyección que usa el GCBA
manzanas <- st_transform(manzanas,
                         crs="+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs ")
saludSf <- st_transform(saludSf,
                         crs="+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs ")
```

Fìjense que usamos un texto largo en lugar de un código para especificar el CRS de la Ciudad de Buenos Aires. Esto suele pasar cuando una proyección no se encuentra correctamente indexada en el catálogo de EPSG. Lo único que hace EPSG es ponerle un número a cada conjunto de parámetros que determinan un CRS (como los que vemos en el texto que usamos dentro de st_transform). Para buscar los CRS cuando no saben cual usar, les recomiendo https://spatialreference.org/

Ya podemos calcular las distancias.

```{r message=FALSE}
# Tomamos las distancias
distanciaManzanas <-st_distance(x = manzanas, y=saludSf)
# Nos quedamos con el valor mínimo de distancia entre cada punto y cada manzana
distanciaManzanas <- apply(distanciaManzanas,1,min)
# Agregamos estos datos al dataset de manzanas
manzanas <- manzanas %>%
            mutate(distMinima=distanciaManzanas)
```

Quizás la parte más rara o con la que están menos familiares es la que dice **apply(distanciaManzanas,1,min)**. La función **apply()** sirve para iterar, es decir, para realizar el mismo procedimiento para todas las filas o columnas de una matriz. Cuando ponemos 1 luego de pasar a la matriz, va a hacer todo fila por fila, si dice 2 lo hará columna por columna. *distanciaManzanas* tenía la distancia en metros de cada uno de los centroides de las manzanas contra cada uno de los puntos, por lo cual lo que hace ahí es ir fila por fila y tomar el valor mìnimo (fijense que le pedimos que aplique la función min en el ùltimo argumento). Con esto, ya podemos hacer nuestro gráfico de la accesibilidad de las manzanas a establecimientos de salud:

```{r}
manzanas <- manzanas %>%
            mutate(distCat=cut(distMinima,
                               breaks = c(0,400,600,900,max(manzanas$distMinima)),
                               labels = c("Hasta 400 metros", "Entre 400 y 600 metros", "Entre 600 y 900 metros","Más de 900 metros"),
                               include.lowest = TRUE))
ggplot(manzanas) +
  geom_sf(aes(fill=distCat),color=NA) +
  scale_fill_viridis_d(direction = -1,name="Distancia") +
  theme_minimal() +
  coord_sf(datum = NA)
```

Se observa una importante parte de la Ciudad de Buenos Aires cubierta por al menos un hospital/centro de salud a menos de 400m, con algunas manchas azules en algunas zonas, que puede o no ser explicada por los 18 puntos que nos faltaron georeferenciar. Repliquemos lo mismo, pero ahora usando el servicio de google.

## Google geocode API

Google ofrece una amplia gama de servicios para lo que ellos llaman “desarrolladors”, que excede largamente el uso de la API de Google Maps. Sin embargo, la forma para acceder a todos estos servicios tiene como origen una misma cuenta. Desde esta cuenta se pueden ir agregando diversos modulos con funcionalidades distintas. Veamos como registrar la cuenta y habilitar los servicios que necesitamos.

Vayan a https://cloud.google.com/ e ingresen con una cuenta de Google (no es necesario que usen una cuenta personal, pueden crear una nueva en caso de ser necesario). Una vez que esten logueados, van a ver un botón en la esquina derecha que dicen “Consola”. Ingresen ahí y desde el menú de navegación en la parte izquierda de la pantalla vayan a APIs y servicios. Ahí van a ver que les va a pedir que creen un proyecto: para usar una API es necesario asociar a un “proyecto” en la plataforma de Google, al que le pueden poner el nombre
que ustedes quieran.

Lo que nosotros necesitamos ahora es activar las APIs que queremos usar. Google ofrece una multitud de APIs, por lo que más simple es buscarlas en la barra que nos ofrece. Vamos a activar dos APIs: Geocoding API y Distance Matrix API. Primero habilitemos Geocoding API y veamos qué pasa. Si todo funcionó bien, deberían estar en la página de su proyecto nuevo, pero ahora en el submenú de API deberían ver que dentro de API habilitadas tienen una que dice “Geocoding API” ¡Muy bien! Abajo de ese título van a ver que les recomienda otras API en API adicionales. Elijan Distance Matrix API. Habilítenla y listo, ya tenemos las dos APIs con las que vamos a trabajar en el curso.

Ahora solo nos queda crear una especie de “contraseña” con la que vamos a vincular nuestra cuenta de Google desde R. Esto es necesario porque las API de Google son servicios pagos. Dan USD 300 de crédito inicial, pero luego hay que ingresar una tarjeta de crédito para continuar usando el servicio, aunque dan USD 200 de crédito todos los meses, el equivalente aproximadamente a 40.000 búsquedas de geocoding. Esta “contraseña” se llama credencial o "key". Pueden activarla desde el menú de la izquierda en “API y servicios”
y luego “credenciales”. Allí pueden crear una credencial, que es básicamente una clave de muchos caracteres que será necesaria para lo que sigue.

**Importante: Para que efectivamente funcione lo que sigue en esta clase tienen que asociar una tarjeta de crédito a la cuenta de Google. Si bien “regala”" USD 300 en el primer uso, y USD 200 todos los meses, hay que tener cuidado una vez que la tarjeta ha sido ingresada. Si quieren probar el uso de esta herramienta pongan quotas desde su cuenta de Google.**

Hechas las presentaciones formales, veamos cómo podemos hacer lo mismo con la funcion **geocodeGoogle()**. Esta función requiere parámetros similares a la de **geocodeDirecciones()**, veamos:

- **datos**: En este parámtro simplemente hay que poner el data.frame que tiene la información que querés georeferencair
- **col_id**: es el nombre de la columna que tiene los códigos identificadores únicos de cada uno de los puntos. La función lo hace obligatorio porque va a ser la columna que después va a ser útil para incorporar el resultado de la geocodificación.
- **cols_query**: En esta columna hay que pasar un vector character con las variables que queremos que se incluyan en la consulta
- **col_key**: nombre de la columna que tiene los datos de la "key" o "credencial" de google
- **col_region**: Este parámetro es opcional y se puede delimitar la búsqueda de google a un área. Por default, region no tiene ningún valor por lo cual busca en todo el mundo.

Veamosla en función. **Este código, así como está, no debería funcionar porque la key es incorrecta**. Deben colocar una propia para que funcione

```{r message=FALSE,error=TRUE}
# Mismas operaciones que anteriormente, pero esta vez agregamos una columna con la key y de region AR
salud <- read_csv("https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/HospitalesYCentrosSalud.csv")
salud <- salud %>%
         mutate(id=row_number(),
                provincia="CABA",
                key="aknfadgnadoigdagoida",
                region="AR")
```

Ahora ya podemos hacer el geocoding con google !

```{r eval=FALSE}
saludGoogle <- geocodeGoogle(datos = salud,
                             col_id = "id",
                             cols_query =c("Dirección","provincia","Barrio"),
                             col_key = "key",
                             col_region = "region")
```

Carguemos los datos de la geolocalización, si hubieran ejecutado el código de Google

```{r }
load("https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/geocodingSaludExample.RData")
```

Un mapa rápido en leaflet para ver qué encontró:

```{r}
saludGoogle <- st_as_sf(x = saludGoogle,
                        coords=c("results.geometry.location.lng","results.geometry.location.lat"),
                        crs=4326)
leaflet(saludGoogle) %>% 
  addTiles() %>%
  addMarkers()
```

Parece bastante bien, veamos cuáles son las varaibles que nos devolvió

```{r}
glimpse(saludGoogle)
```

Wow, muchas variables. La primera columna tiene, adentro, un data.frame con información que está en el resto de las columnas, así que vamos a ignorarla. Luego tenemos la **formatted_address**, que nos permite ver cuál es la calle que efectivamente nos devolvió Google. Luego tenemos cuatro coordenadas bounds, que sirven solo cuando nos devolvió un polígono en lugar de un punto (no le vamos a prestar atención, vamos a trabajar con los puntos que haya devuelto). Luego en location.lat y location.lng tenemos específicamente la latitud y longitud de nuestra dirección. Existen otras columnas, pero con estas y **id** y **partial_match** ya podemos seguir.

Si están atentos/as a lo que estamos haciendo, van a ver que saludGoogle devolvió 118 resultados cuando mandamos 115 ! Qué fue lo que pasó? Siempre que pasa esto es porque google no encontró un resultado "perfecto" para nuestra consulta y nos devolvió más de uno. Cómo nos damos cuenta? De distintas maneras, pero **partial_match**, **status** y mirar a los duplicados nos va a servir mucho. Para empezar, hagamos esto último: veamos cuáles son los que están duplicados

```{r, eval=FALSE}
# Buscamos los duplicados
idsDuplicados <- saludGoogle %>% 
                 group_by(id) %>%
                 summarise(conteo=n()) %>%
                 filter(conteo>1) %>%
                 pull(id)
# Los vemos con View()
View(saludGoogle[saludGoogle$id %in% idsDuplicados,])
```

En "results.types" van a ver que los ids 78 y 79 son rutas en lugar de puntos, por lo cual no nos sirve: no pudo encontrar un punto que se parezca a la dirección que le pasamos. Vamos a tener que filtrar esos casos. Finalmente, el id 62 nos devuelve dos casos para la misma dirección, uno pareciera ser el hospital y otro la policía (en la misma dirección): eso es algo que no nos interesa. Entonces lo que vamos a hacer es sacar a 78 y 79 y quedarnos solo con uno de los dos del 62. Lo hacemos así:

```{r}
# Filtramos los ids y cualquier caso duplicado en id
saludGoogle <- saludGoogle %>%
               filter(!id %in% c(78,79)) %>%
               filter(!duplicated(id))

```

La función duplicated nos elimina a todas las filas que aparecen por segunda vez en el data.frame con el mismo id. En este caso, elimina la segunda vez que aparece el número 62, por lo que funcionó bien. Ahora tenemos 113 puntos georeferenciados, solo perdimos el 78 y el 79. Hagamos el mismo gráfico que hicimos anteriormente:

```{r}

# El objeto de saludGoogle no tiene información sobre el CRS de las coordenadas, pero sabemos que es 4326
st_crs(saludGoogle) <- 4326
# Convertimos a los dos a la proyección que usa el GCBA
manzanas <- st_transform(manzanas,
                         crs="+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
saludGoogle <- st_transform(saludGoogle,
                         crs="+proj=tmerc +lat_0=-34.629269 +lon_0=-58.4633 +k=0.9999980000000001 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
# Tomamos las distancias
distanciaManzanas <- st_distance(x = manzanas, y=saludGoogle)
# Nos quedamos con el valor mínimo de distancia entre cada punto y cada manzana
distanciaManzanas <- apply(distanciaManzanas,1,min)
# Agregamos estos datos al dataset de manzanas
manzanas <- manzanas %>%
            mutate(distMinima=distanciaManzanas)

manzanas <- manzanas %>%
            mutate(distCat=cut(distMinima,
                               breaks = c(0,400,600,900,max(manzanas$distMinima)),
                               labels = c("Hasta 400 metros", "Entre 400 y 600 metros", "Entre 600 y 900 metros","Más de 900 metros"),
                               include.lowest = TRUE))
ggplot(manzanas) +
  geom_sf(aes(fill=distCat),color=NA) +
  scale_fill_viridis_d(direction = -1,name="Distancia") +
  theme_minimal() +
  coord_sf(datum = NA)
```

Bastante parecida a la geolocalización de la API del gobierno, a simple vista, pero con una mayor cobertura. Veamos, ahora, en cuanto difieren los cálculos de uno y otro dataset

## Diferencias entre las dos APIs

Podemos ver las diferencias de muchas maneras, pero poniendo los puntos con un color de cada uno no sería lo mejor, ya que no podemos ver entre cual par de puntos fue la diferencia. Hagamos algo más simple: la distancia entre cada uno de los puntos.

```{r}
distancias <- st_distance(x = saludGoogle[saludGoogle$id %in% saludSf$id,],
            saludSf[saludSf$id %in% saludGoogle$id,],
            by_element = TRUE)
distancias <- data.frame(distancia=as.numeric(distancias))
ggplot(distancias) +
  geom_histogram(aes(x=distancia)) +
  theme_minimal() +
  labs(x="Discrepancia (metros)",y="Cantidad de casos")
```

La diferencia es baja en casi todos los casos, menos uno que está en aproximadamente 900 metros. Si aceptamos una tolerancia de 200 metros de diferencia, la coincidencia es casi total, nada mal. Google tiene una ventaja, igual: devolvió todos menos dos puntos.

## Una tercera alternativa: hereR

En general, la tarea de geocodificación requiere cierta prueba y error. No todos los servicios de geocodificación saben interpretar de la misma manera a las direcciones, y muchas veces las asignan a distintas coordenadas (este problema puede ser especialmente importante al trabajar con Google Maps, que siempre intenta devolver algún resultado). El paquete **hereR** nos brinda una alternativa muy interesante para geocodificar direcciones. 

El servicio de Here Maps permite hasta 250.000 consultas mensuales de manera gratuita, lo cual puede ser suficiente para muchos de nuestros proyectos. Lo único que precisan para poder hacer uso del sistema de mapas de HERE Maps es simplemente instalar el paquete **hereR**, y vincular a nuestra sesión de R con la **API Key** correspondiente a nuestra cuenta de HERE Maps.


```{r message=FALSE, warning=FALSE, eval=FALSE}
library(hereR)
set_key("<YOUR API KEY>")
salud <- salud %>%
         mutate(consultaHERE=paste(Dirección,", Ciudad de Buenos Aires, Argentina",sep=""))
saludHERE <- geocode(salud %>% pull(consultaHERE))
```
```{r eval=TRUE, echo=FALSE}
load("geocodingSaludHereCapitulo2.RData")
```

A diferencia de lo que hicimos con **wrapar**, en este caso ya nos devolvió un objeto sf, podemos usar leaflet para hacer un control de que, al menos, nuestros puntos están en la Ciudad de Buenos Aires

```{r}
leaflet(saludHERE) %>% 
  addTiles() %>%
  addMarkers()
```

## Comparando todos los métodos 

Llegó el momento de comparar todos los métodos que usamos en la clase. Una primera aproximación para evaluar qué tan efectivos han sido para geolocalizar las direcciones sin ningún tipo de normalización es analizar la cantidad de respuestas a las consultas que le envíamos. Si revisamos la cantidad de respuestas, HERE devolvió una posición para cada una de las direcciones, google también - aunque luego de hacer una limpieza nos quedamos con 113 -, mientras que la API de georreferenciación de Argentina devolvió 97.

Ahora bien, estas diferencias en la cantidad de respuestas solo miden la cobertura, no la precisión. Para testear esto vamos a aprovechar que el GCBA ya geolocalizó estas direcciones. Vamos a cargar un dataset que tiene la longitud y latitud según el Gobierno de la Ciudad de Buenos Aires de 88 de estos puntos de salud

```{r}
load("https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/saludGCBA.RData")
```

El objeto *saludGCBA* cuenta con esta información, pero si se fijan la clase verán que no es un objeto espacial. Vamos a hacer eso y, también a proyectarlo en el CRS que estuvimos trabajando para poder medir las distancias.

```{r}
# Acá va el código para poder cargar los datos del GCBA
saludGCBA <- st_as_sf(saludGCBA,coords=c("longGCBA","latGCBA"),crs=4326)
saludGCBA <- st_transform(saludGCBA,crs =  st_crs(saludSf))
```

Nos queda convertir el dataset de georeferenciación de HERE Maps también al sistema de coordenadas proyectado que usamos para el resto de los datos. Una vez que tenemos todo esto, ya podemos agregar como una columna la distancia - en metros - entre cada una de las metodologías que estuvimos aplicando y la ubicación asignada por el GCBA

```{r}
# Proyectamos los datos de HERE Maps
saludHERE <- saludHERE %>% st_transform(st_crs(saludSf))
# Agregamos los datos de distancia para cada uno de los 88 puntos de salud geocodificados por el GCBA
# Creamos data frames que tengan el ID y la distancia usando st_distance()
distanciaHERE <- data.frame(id=saludGCBA %>% filter(saludGCBA$id %in% saludHERE$id) %>% pull(id),
                            distanciaHERE = st_distance(saludGCBA %>% filter(saludGCBA$id %in% saludHERE$id),
                                              saludHERE %>% filter(saludHERE$id %in% saludGCBA$id),
                                              by_element = TRUE) %>% as.numeric())

distanciaARG <- data.frame(id=saludGCBA %>% filter(saludGCBA$id %in% saludSf$id) %>% pull(id),
                            distanciaARG = st_distance(saludGCBA %>% filter(saludGCBA$id %in% saludSf$id),
                                              saludSf %>% filter(saludSf$id %in% saludGCBA$id),
                                              by_element = TRUE) %>% as.numeric())

distanciaGoogle <- data.frame(id=saludGCBA %>% filter(saludGCBA$id %in% saludGoogle$id) %>% pull(id),
                              distanciaGoogle = st_distance(saludGCBA %>% filter(saludGCBA$id %in% saludGoogle$id),
                                              saludGoogle %>% filter(saludGoogle$id %in% saludGCBA$id),
                                              by_element = TRUE) %>% as.numeric())
# Unimos todas las distancias en un solo data frame
distancias <- left_join(distanciaHERE,distanciaARG)
distancias <- left_join(distancias,distanciaGoogle)
# Incorporamos esta información al dataset de saludGCBA
saludGCBA <- left_join(saludGCBA,distancias)
# Eliminamos los objetos que ya no necesitamos más
rm(distanciaARG,distanciaGoogle,distanciaHERE)
# Nos quedamos solo con los casos donde hay información de distancia con todos los métodos
saludGCBA <- saludGCBA %>%
             filter(!is.na(distanciaARG) & !is.na(distanciaGoogle) & !is.na(distanciaHERE))
```

Finalmente, saludGCBA ahora cuenta con información sobre la ubicación de 72 puntos que ya habían sido geolocalizadas por el GCBA y que pudimos georeferrenciar mediante los tres métodos de este capítulo. Veamos cuales son las diferencias más comunes al observar los cuartiles de la distribución de cada uno de los métodos. Presten atención a **st_set_geometry(NULL)**, una función que nos permite eliminar la columna de geometría para hacer cálculos que no necesita de ellos.

```{r}
saludGCBA %>% 
  st_set_geometry(NULL) %>% 
  summarise_at(c("distanciaARG","distanciaGoogle","distanciaHERE"),function(x) quantile(x,probs = c(0,0.25,0.5,0.75,1)))
```

## Ejercicios

Busquen un conjunto de direcciones que les interese y calculen las distancias entre ellas usando la aplicación de geolocalización de Argentina y la función de distancia del paquete sf.



<!--chapter:end:Geocoding.Rmd-->

# Tiempos de viaje y análisis de accesibilidad

Nuestros datos espaciales pueden ser enriquecidos de distintas maneras, no solo a través de la relación con otras entidades espaciales, tal como se mostró en el ejercicio de ubicación óptima para uno nuevo centro de salud anteriormente. Es posible agregar información sobre tiempos de desplazamiento en distintos tipos de desplazamiento o aproximar un polígono que, para un determinado punto, muestra cuáles son los lugares a los que se pueden acceder en u determinado tiempo. Yendo un paso más atrás, incluso podemos lograr encontrar la coordenadas a partir del texto de una dirección. Vamos a aplicar todos estos conceptos y verlos en acción en un tema  muy interesante: el acceso a los espacios verdes en la Ciudad de Buenos Aires.

## La distancia espacial y la distancia de viaje

Las personas que vivimos en ciudades entendemos a la perfección que la distancia entre punto A y B puede medirse de distintas maneras. A veces, caminar 1000 metros suele tardar mucho menos que hacerlo en auto, y a veces los tiempos de transporte público pueden variar mucho según la proximidad con distintos medios de transporte disponibles.

En distintas circunstancias puede ser muy importante entender cuáles son las condiciones de acceso de cada una de las zonas de una ciudad para distintas razones: trabajo, recreación, salud, entre otras variables. En esta clase vamos a medir el acceso a la los espacios verdes de la Ciudad de Buenos Aires, pero tan solo por una cuestión de acceso a datos. Si tuvieramos, por ejemplo, información sobre la ubicación de los puntos donde las empresas están ubicadas, podríamos estimar la accesibilidad al mercado de trabajo de cada uno de los lugares. Sea como fuere, basta de preámbulos y veamos cómo podemos procesar los datos espaciales para tener una medida espacial del acceso a espacios verdes en la Ciudad de Buenos Aires.

Para esto vamos a hacer lo siguiente: 

- Medir la cobertura de los espacios verdes de la CABA, midiendo desde qué lugares se puede llegar a 15 minutos caminando
- Cruzar estos datos con las manzanas y establecer que aquellas manzanas que no se encuentran en este espacio de cobertura están "mal atendidas"


## Los paquetes que vamos a utilizar

Los capítulos de este libro en general no suelen introducir los paquetes que tienen las herramientas que vamos a utilizar antes de que sean necesarias para resolver un problema en particular. Sin embargo, para este capítulo hacemos una pequeña excepción, ya que vamos a cargar un conjunto de paquetes en los que vamos a tener que hacer zoom tanto en lo que ofrecen como en qué pasos adicionales tenemos que hacer para utilizarlas. Vamos paso por paso:

- **sf**: Este paquete ya lo conocemos, es el que nos permite trabajar con datos espaciales en R.
- **tidyverse**: Colección de paquetes que nos permite cumplir muchas de las tareas necesarias en un proyecto de ciencia de datos. En esta clase vamos a investigar una función del paquete **purrr**, parte de tidyverse, y que nos permite
- **leaflet**: Herramienta muy poderosa para generar mapas interactivos. En este capítulo lo vamos a utilizar para que resolver uno de los potenciales problemas que puede tener R para actuar como GIS: la falta de modificaciones o exploraciones "manuales" de los datos espaciales
- **hereR**: Si bien existen distintas alternativas para medir los tiempos de desplazamiento, en este caso vamos a usar el servicio de HERE maps. La API de here puede usarse utilizando el lenguaje de R gracias a las personas que desarrollaron **hereR**.

### Cómo usar los servicios de HERE

HERE Maps es una empresa que nos brinda distintas herramientas de geolocalización, medidas de tiempo de viaje entre distintos puntos. Piensen en ella como una caja de herramientas donde podemos elegir entre ellas para poder resolver problemas particulares. A diferencia de otras alternativas, como Google Maps, HERE nos permite realizar hasta 250.000 consultas gratuitas por mes sin tener que poner nuestra tarjeta de crédito como garantía. Solía ser de esta manera con Google Maps en el pasado, pero cambiaron las condiciones de un tiempo para acá, por lo cual hay que buscar alternativas y HERE nos puede ser útil.

Para usar HERE Maps en R, además de instalar el paquete **hereR**, debemos tener una *key*, que no es otra cosa que una contraseña única que nos pide HERE Maps para poder vincular el uso que le damos a la cuenta desde R con sus registros internos. Para esto, primero hay que crear una cuenta en https://developer.here.com/. Una vez que hayan creado la cuenta - gratuita - tienen que ir a los "projects" que tiene y hacer click en **create API key** donde dice REST. Una vez que la creen, van a ver que les aparece una tabla con "API KEY" y un botón que dice "COPY". Hagan click ahí y peguénlo por algún notepad o similar, ya van a ver cómo vamos a poder utilizarlo.


## Los datos: repositorio de datos del GCBA

Nuestra materia prima para la introducción a estas herramientas estatales será el dataset espacial de espacios verdes de la Ciudad de Buenos Aires y la información sobre el trazado urbano de la Ciudad, también provisto por el GCBA. Estos datos pueden reemplazarse por otros datasets en caso que quieran replicar este análisis para otras ciudades. En particular para Argentina, es posible usar los radios censales, la mínima unidad espacial para un censo en Argentina, para reemplazar las manzanas. Por otro lado, los datasets de espacios verdes puede ser un poco más difíciles de encontrar, pero siempre existen esfuerzos para construir estos datasets que pueden encontrar haciendo búsquedas por Google

Los datos de las manzanas se pueden descargar desde https://data.buenosaires.gob.ar/dataset/manzanas, mientras que los de espacios verdes desde https://data.buenosaires.gob.ar/dataset/espacios-verdes. En mi caso, yo descargué los geojson y los guardé en una carpeta que se llama **data**, pero pueden hacer lo que ustedes crean necesario!


```{r, message=FALSE, warning=FALSE,results='hide'}
library(tidyverse)
library(sf)
library(hereR)
library(leaflet)
manzanas <- st_read("data/manzanas.geojson")
espaciosVerdes <- st_read("data/espaciosVerdes.geojson")
```

Veamos rápidamente qué es lo que cargamos con la ayuda de leaflet:

```{r}
leaflet(espaciosVerdes) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons()
```

Parece que tenemos un conjunto de espacios urbanos de la ciudad de buenos aires, que es exactamente lo que queríamos. Igualmente vamos a hacer algunos cambios a este dataset, ya que vamos a quedarnos con espacios que sean lo suficientemente grande, de una manera muy arbitraria, como para poder ser aprovechados recreativamente: vamos a imponer que el espacio verde tenga al menos una superficie de 10.000 metros cuadrados, es decir una manzana.

## Transformando nuestros datos

Ahora que tenemos ya cargados nuestros datos espaciales, podemos transformarlos como para poder medir la cobertura de la oferta de espacio verde en la CABA. Para empezar, vamos a restringir espacios verdes a aquellos espacios verdes que tengan más de 1km2 de superficie total. Esto lo hacemos porque queremos medir de alguna manera el "uso" que le pueden dar a esos espacios, no solo si existe o no un lugar con zonas verdes. Es un criterio discutible, pero acá viene la mejor parte: cuando terminen este capítulo van a poder ir cambiando estos criterios. 

¿Cómo podemos saber el tamaño de los polígonos de los espacios verdes? Esto es posible hacer siempre que tengamos nuestros datos espaciales cargados como un objeto sf. Para esto, primero vamos a proyectar en 2 dimensiones a los datos que cargamos, usando la proyección adaptada a la Ciudad de Buenos Aires:

```{r}
espaciosVerdes <- st_transform(espaciosVerdes,
                               crs = "+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
```

Una vez que hicimos esto, entonces ya estamos en condiciones de poder conocer el área de cada uno de los polígonos. La función que usamos es **st_area()**, pero debemos usar un punto, que hace referencia a "todo el dataset" dentro de tidyverse. Es decir, cuando usemos mutate, por ejemplo, el "." hará referencia a todas las filas, es decir, a todos los polígonos de nuestro dataset. La función st_area() nos devuelve un tipo particular de datos: **units**, es decir que son datos con unidad (en este caso, metros cuadrados). Como esto ya lo sabemos, lo convertimos en un vector clásico con **as.numeric()** como para poder luego usar **filter()** y quedarnos solos con los que queríamos

```{r}
espaciosVerdes <- espaciosVerdes %>% 
                  mutate(area=st_area(.)) %>% 
                  mutate(area=as.numeric(area)) %>%
                  filter(area>10000) 

```

Probemos nuevamente con qué logramos quedaros:

```{r}
leaflet(espaciosVerdes %>% st_transform(4326)) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons()
```

Muy bien, ahora pensemos en un problema un poco más específico. Para medir la distancia entre dos puntos necesitamos... ¡puntos! Y nuestros dos datasets, manzanas y espacios verdes, son polígonos, tenemos que resolver esto de alguna manera. Podríamos tomar centroides de ambos casos pero tendríamos un problema muy importante: cuando las figuras son largas y/o grandes, el centroide no puede ser un buen indicador de punto de partida o llegada.

La solución propuesta es solo medir el tiempo de viaje caminando desde los espacios verdes, pero desde un conjunto de puntos **al azar** que estén dentro de los polígonos, como para poder hacer más representativo el hecho de que al espacio verde se puede acceder desde distintos puntos. Si esto no queda del todo claro, vamos con un ejemplo bien sencillo. Enfoquémonos un segundo en el Parque Chacabuco. Para eso vamos a filtrarlo y convertirlo a ESPG 4326, que es lo que necesita leaflet para graficar

```{r}
parqueChacabuco <- espaciosVerdes %>% 
                   filter(nombre == "Parque Chacabuco") %>% 
                   st_transform(4326)
leaflet(parqueChacabuco) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons()
```

El parque es grande, si tomáramos una esquina, probablemente caminando 10 o 15 minutos estaríamos todavía dentro del mismo parque, y quizás arrancando desde el centro también... Retomemos este problema mientras aprendemos a pedir tiempo de viajes.

## ¿Cuánto tardo en llegar al monumental?

El equipo más grande de la Argentina tiene su estadio en Avenida Presidente Figueroa Alcorta 7597, en la Ciudad de Buenos Aires. Yo vivo por alguna zona de Palermo - vamos a mantener un poco la anonimidad - digamos Charcas 3591. Solo tengo esta información, pero me gustaría saber cuánto tiempo puedo tardar caminando, en auto o en bicicleta ¿Puedo hacerlo? Claró, puedo entrar a Google Maps y buscar las opciones. Pero acá vamos a hacerlo desde R y con el servicio de HERE Maps, lo cual después nos va a permitir llevar esto un paso más adelante y responder nuestro problema inicial.

Lo primero que tenemos que hacer es generar un data frame con la información que sí tenemos:

```{r}
direcciones <- data.frame(lugar=c("Casa","Monumental"),
                          direccion=c("Charcas 3591, Ciudad de Buenos Aires, Argentina","Avenida Presidente Figueroa Alcorta 7597, Ciudad de Buenos Aires, Argentina"))

```

Luego, usamos la función de **geocode()** de hereR, que solo nos pide un vector con las direcciones a geolocalizar. Para esto, antes deben usar **set_key()** para que here maps sea capaz de asociar las consultas que hacemos desde R con nuestra cuenta en HERE Maps.

```{r echo=FALSE}
load(file = 'data/ubicacionRiver.RData')
```

```{r eval=FALSE}
# Esta key es falsa, por obvias razones. Reemplacen este valor por uno que funcione
set_key("<YOUR API KEY>")
ubicaciones <- geocode(direcciones %>% pull(direccion))
```

Si todo salió bien, ahora deberían tener un objeto *ubicaciones* con información espacial sobre nuestros datos... veamos si están bien georreferenciados

```{r}
leaflet(ubicaciones) %>% 
  addTiles() %>% 
  addMarkers()
```

Si exploran un poco, van a ver que todo salió relativamente bien. Ahora lo que vamos a hacer es usar la función **route()**, la que se encarga de consultarle a HERE Maps cuánto tardamos entre el punto A y el B. En principio, solo necesita un origen, un destino y un modo de transporte, que en este caso va a ser "pedestrian", es decir, cuánto tardamos caminando entre los dos puntos

```{r echo = FALSE}
load("data/viajeRiver.RData")
```
```{r eval=FALSE}
viaje <- route(origin = ubicaciones[1,],
               destination = ubicaciones[2,],
               mode = "pedestrian")
```
```{r}
glimpse(viaje)
```
El tiempo lo devuelve en la variable **travelTime** en segundos, por lo que dice que tardariamos 6480/60 = 108 minutos (una hora y 48 minutos) en hacer 6.389 metros (vean "distance"). Muy bien, ahora probablemente les haya aparecido un horario distinto al que aparece en la salida de este libro... y esto es porque por default hereR consulta por cuanto tardaría saliendo en el mismo instante de la consulta. No se preocupen, acá les explico como replicar lo que hice recién, y también puede serles útiles para medir otro tiempo de distancia. Solo tienen que cambiar la fecha y el horario y les va a funcionar correctamente

```{r eval=FALSE}
viaje <- route(origin = ubicaciones[1,],
               destination = ubicaciones[2,],
               mode = "pedestrian",
               datetime = as.POSIXct(x = "17/01/2020 18:00:00",format="%d/%m/%Y %H:%M:%S"))
```

Medir tiempo de viaje en auto es muy similar, solo tienen que cambiar "pedestrian" por "car", de la siguiente manera:

```{r eval=FALSE}
viaje <- route(origin = ubicaciones[1,],
               destination = ubicaciones[2,],
               mode = "car",
               datetime = as.POSIXct(x = "17/01/2020 18:00:00",format="%d/%m/%Y %H:%M:%S"))
```

Bien, ahora estamos más cerca de la herramienta que finalmente vamos a usar: las isocronas. Esto que suena horrible es simplemente **un polígono que delimita el espacio al cual se puede llegar en un tiempo fijo**. Se trata de una aproximación, ya que nunca puede saberse exactamente cuál es este polígono, pero hagamos la prueba desde mi supuesto hogar: cuál es el polígono que podemos alcanzar caminando solo 15 minutos

```{r echo=FALSE}
load("data/isolinea.RData")
```

```{r eval=FALSE}
viajeCaminando <- isoline(ubicaciones[1,],mode = "pedestrian",range = 60*15)
```

Ahora veamos qué es lo que nos devolvió:

```{r}
leaflet(viajeCaminando) %>% 
  addTiles() %>% 
  addPolygons() %>% 
  addMarkers(data=ubicaciones[1,])
```

Ese polígono nos muestra todos los lugares a los que podemos acceder en una caminata de 15 minutos desde Charcas 3591. Ahora bien, recuerden nuestro punto inicial: queremos medir la cobertura de los espacios verdes en la Ciudad de Buenos Aires. Algunos son muy grandes, entonces nos conviene conseguir esta isocrona para más de un punto dentro del mismo parque, de tal manera de poder capturar este efecto. Veamoslo, de nuevo, con el el ejemplo del Parque Chacabuco.

## Midiendo la cobertura de los parques

Si siguieron este capítulo correctamente, deberían tener un objeto **parqueChacabuco** con sus polígonos. Vamos a medir la cobertura mediante dos métodos alternativos: usando el **centroide** de los polígonos y tomando al azar 4 puntos del parque. Comparemos los resultados.

Lo primero que tenemos que hacer, es reproyectar esos polígonos para poder tomar puntos al azar y también para tomar los centroides. En general, cuando realizamos esta clase de operaciones conviene tener a los datos espaciales proyectados en dos dimensiones

```{r}
parqueChacabuco <- parqueChacabuco %>% 
                   st_transform("+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
```

Ahora ya podemos tomar el centroide, con la función **st_centroid**

```{r}
centroideChacabuco <- st_centroid(parqueChacabuco)
```

Y también cuatro puntos al azar, con la función **st_sample**

```{r echo = FALSE}
load("data/puntosChacabuco.RData")
```

```{r eval = FALSE}
set.seed(1)
puntosChacabuco <- st_sample(parqueChacabuco,size = 4)
```

Como siempre, podemos ver muy fácilmente que es lo que acabamos de hacer, con la ayuda de leaflet

```{r}
leaflet() %>% 
  addTiles %>% 
  addPolygons(data=parqueChacabuco %>% st_transform(4326)) %>% 
  addCircleMarkers(data=puntosChacabuco %>% st_transform(4326), color='red') %>% 
  addCircleMarkers(data=centroideChacabuco %>% st_transform(4326),color ='yellow') 
  
```

Ahora midamos la isolinea de 15 minutos para esta forma de identificar al parque. Antes de so, tenemos que hacer algo con puntosChacabuco, porque no es un objeto sf ! miren

```{r}
class(puntosChacabuco)
```

Lo que tenemos que usar es **st_as_sf()**, que es la forma de decirle a R que queremos que ese objeto sea uno sf:

```{r}
puntosChacabucoSF <- st_as_sf(puntosChacabuco)
class(puntosChacabucoSF)
```

Ahora sí, ya estamos en condiciones de hacer lo que queríamos. Para el centroide es muy simple, usamos de nuevo **isoline()** y no tenemos mayores inconvenientes

```{r echo= FALSE}
load("data/isoCronaCentroide.RData")
```

```{r eval= FALSE}
isoCronaCentroide <- isoline(centroideChacabuco,mode = "pedestrian",range = 60*15)
```

Ahora bien, lamentablemente la función **isoline()** no hace automáticamente la operación para todas las filas. Eso ciertamente nos haría el trabajo más simple, pero no vamos a impedir que eso nos deje terminar el trabajo. Lo que vamos a usar es la función **map()** del paquete **purrr**. Lo que hace es hacer una función, la que queramos, para un conjunto de objetos que le digamos. Es muy general, por lo cual veamosla en funcionamiento:

```{r}
map(c(1:5),function(x) x+5)
```

Todas las funciones de **map()** tienen las dos cosas que les dije. En primer lugar, un conjunto de elementos a los cuales queremos hacerles una función en particular. En este caso, le pasamos los números que van desde el 1 al 5. Después, creamos una función que toma el el valor x y le suma 5. **x** en ese contexto significa cada uno de los valores que decíamos antes. Lo que devuelve es una lista con todos los valores que queríamos. Si no quedó del todo claro, no importa: ya va a quedar más claro con la experiencia. Apliquemoslo a este ejemplo entonces:

```{r echo = FALSE}
load("data/isoCronaPuntos.RData")
```
```{r eval = FALSE}
isocronaPuntos <- map(1:nrow(puntosChacabucoSF),
                         function(x) { isoline(puntosChacabucoSF[x,],mode = "pedestrian",range = 60*15) })
```

Fijense que lo que nos devolvió es una lista con 4 isolineas basadas en cada uno de los puntos que tomamos aleatoriamente anteriormente. Ahora nos queda juntarlos con rbind. Podemos hacerlo uno por uno, pero también podemos apoyarnos en **do.call()** que es muy similar a **map()**. La principal diferencia en este caso es que tenemos que pasarle la función que queremos que haga entre comillas, y luego la lista sobre la que queremos que lo haga. Nosotros queremos qu use **rbind** para todos los elementos de **isocronaPuntos**.

```{r}
isocronaPuntos <- do.call("rbind",isocronaPuntos)
```

Vemos que es lo que tenemos hasta ahora:

```{r}
leaflet(isocronaPuntos) %>% 
  addTiles() %>% 
  addPolygons()
```

Podemos ver que las curvas se solapan quizás demasiado, y eso no es lo que buscamos. Entonces directamente lo que vamos a hacer es unir todo en un solo gran polígono, para hacer las cosas más fáciles. Esto se hace con **st_union()**

```{r}
isocronaPuntos <- st_union(isocronaPuntos)
```

Finalmente podemos comparar la cobertura con ambas metodologías: centroides y puntos. Veamos la diferencia:

```{r}
ggplot() +
  geom_sf(data=parqueChacabuco) +
  geom_sf(data=isoCronaCentroide, fill="red",alpha=0.1) +
  geom_sf(data=isocronaPuntos, fill="orange", alpha=0.2)
```

Otra forma de verlo es sumar el área de cobertura de ambas alternativas:

```{r}
st_area(isoCronaCentroide)
st_area(isocronaPuntos)
```

Una diferencia más que importante en cobertura ! Ahora ya estamos en condiciones de hacer el ejercicio por el que veníamos: caracterizar la cobertura de los espacios verdes en la Ciudad de Buenos Aires

## Redondeando: caracterizando la oferta de los espacios verdes en CABA

Toda la larga discusión que tiene este capítulo puede resumirse en tan pocas líneas como las que siguen:

```{r echo=FALSE}
load("data/isocronosEspaciosVerdesJuntasUnion.RData")
```

```{r eval=FALSE}
# Para cada uno de los espacios verdes agarramos 4 puntos al azar
puntosEspaciosVerdes <- map(1:nrow(espaciosVerdes),function(x){
  st_sample(espaciosVerdes[x,],size=4)
})
# Juntamos todos los puntos en un objeto
puntosEspaciosVerdes <- do.call("c",puntosEspaciosVerdes)
# Lo convertimos a un objeto SF
puntosEspaciosVerdesSF <- st_as_sf(puntosEspaciosVerdes)
# Hacemos una transformación para que esté en el WSG84, que es lo que puede procesar isoline()
puntosEspaciosVerdesSF <- st_transform(puntosEspaciosVerdesSF,crs=4326)
# Efectivamente calculamos las isocronas para cada uno de los puntos
isocronosEspaciosVerdes <- map(1:nrow(puntosEspaciosVerdesSF),function(x) {
  # Esta línea es solo para que nos vaya avisando qué está haciendo
  cat("Procesando: ",x,"\r")
  isoline(puntosEspaciosVerdesSF[x,], mode = "pedestrian",range_type = "time",range = 60*15)
})
# Los juntamos en el mismo data frame
isocronosEspaciosVerdesJuntas <- do.call(rbind,isocronosEspaciosVerdes)
# Hacemos un gran poligono
isocronosEspaciosVerdesJuntasUnion <-  st_union(isocronosEspaciosVerdesJuntas)
```

Va a tardar un poco porque son 588 puntos para los cuales tiene que encontrar las icoronas... Pero si lo dejan correr, va a terminar de procesarlo. Podemos ver la cobertura que estimamos en un mapa de leaflet

```{r}
leaflet(isocronosEspaciosVerdesJuntasUnion %>% st_transform(4326)) %>% 
  addTiles() %>% 
  addPolygons()
```

Ahora simplemente tenemos que hacer un spatial join con **st_join()**. Recuerden que lo que hace esta función es unir a dos datasets según algún criterio de unión, siendo por default si se intersectan o no. En los casos que se intersecten, entonces va a agregar la información que se encuentra en el segundo dataset al primero. En caso de que no existe unión, ese valor para ese punto/polígono en particular será **NA**. Para hacer el spatial join vamos a proyectar a los dos datasets, convertimos como objeto sf a todo el espacio que identifica la cobertura de espacios verdes, y creamos una variable que identifique eso, y generamos efectivamente el spatial join.

```{r}
# Transformamos la proyección de las manzanas
manzanas <- manzanas %>%
            st_transform("+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
# Transformamos la proyección de la cobertura de espacios verdes
isocronosEspaciosVerdesJuntasUnion <- isocronosEspaciosVerdesJuntasUnion %>%
  st_transform("+proj=tmerc +lat_0=-34.6297166 +lon_0=-58.4627 +k=1 +x_0=100000 +y_0=100000 +ellps=intl +units=m +no_defs")
# Lo convertimos en un objeto sf y creamos una columna, cobertura, que tendrá valor TRUE siempre
isocronosEspaciosVerdesJuntasUnion <- st_as_sf(isocronosEspaciosVerdesJuntasUnion) %>% 
                                      mutate(cobertura=TRUE)
# Spatial join
manzanas <- st_join(manzanas,st_as_sf(isocronosEspaciosVerdesJuntasUnion))
# Completamos los datos para los casos en los cuales no hubo ningún resultadoe en el match
manzanas <- manzanas %>% mutate(cobertura=ifelse(is.na(cobertura),FALSE,TRUE))
```


Hacemos el gráfico con ggplot, listo para exportar en caso que sea necesario con **ggsave()**

```{r}
ggplot(manzanas) +
  geom_sf(aes(fill=cobertura), color=NA) +
  theme_minimal() +
  coord_sf(datum=NA) +
  scale_fill_manual(values = c("#377eb8","#e41a1c"),
                    breaks = c(TRUE,FALSE),
                    labels=c("Menos de 15 minutos","Más de 15 minutos"))
```

Se puede mejorar y hacer el siguiente gráfico si usan **ggmap()** para agregar un mapa de base. Usamos **getbb** para que nos de la Bounding Box, es decir cuatro puntos, que definen todo un rectángulo dónde se puede ver la Ciudad de Buenos Aires 

```{r message=FALSE, warning=FALSE}
library(osmdata)
library(ggmap)
bbCABA <-getbb("Ciudad de Buenos Aires, Argentina")
cabaBaseMap <- get_stamenmap(bbCABA,maptype = "toner-lite",zoom=12)
ggmap(cabaBaseMap,extent = "device") +
  geom_sf(data=manzanas %>% st_transform(4326),aes(fill=cobertura), color=NA, inherit.aes=FALSE, alpha=0.7) +
    theme_minimal() +
  coord_sf(datum=NA) +
  labs(x="",y="") +
  scale_fill_manual(values = c("#377eb8","#e41a1c"),
                    breaks = c(TRUE,FALSE),
                    labels=c("Menos de 15 minutos","Más de 15 minutos"),
                    name="")+
  theme(legend.position = "bottom")
```


## Ejercicios

1. ¿Cuánto tiempo tardás en llegar desde tu casa hasta la oficina de trabajo en auto? Usá hereR en R para poder sacar esa conclusión ¿Qué camino te sugirió? Podés descubrirlo haciendo un gráfico del objeto que devuelve la función **route()**

2. Estimar la cobertura de espacios verdes en la ciudad de buenos aires, pero en lugar de usar 15 minutos como en el capítulo, usar 30 minutos ¿Qué zonas no están cubiertas con un parque a 30 minutos de caminata?

3. El GCBA ofrece información sobre la ubicación geográfica de las (comisarias en la Ciudad)[!https://data.buenosaires.gob.ar/dataset/comisarias-policia-ciudad] ¿Hay zonas de la Ciudad en las cuales no hay acceso a una comisaria en menos de 10 minutos en auto, según HERE Maps?  

<!--chapter:end:Geowrangling.Rmd-->

# El ritual del aprendizaje automático (Parte 1)

En las notas de clases de [Ciencia de Datos para Curiosos](https://martinmontane.github.io/CienciaDeDatosBook/) van a poder encontrar un capítulo dedicado a la introducción a las herramientas de Machine Learning, en particular los **árboles de decisión**, los **árboles de regresión** y sus implementciones en R. 

El objetivo de este capítulo es mostrar cómo pueden aplicarse estos conocimientos para una tarea de **predicción** asociada a un problema clásico: el precio de los inmuebles. Pero lo haremos de una manera secuencial, con el propósito de mostrar cuáles son los pasos más comunes y necesarios al momento de elegir el modelo, optimizar sus parámetros y comprender su capacidad predictiva.

## Paso 1: Carga de los datos

Puede sonar obvio y repetitivo, pero todo lo que hagan ustedes en análisis cuantitativos comienza con la carga de datos. Es importante que se sientan cómodos y cómodas como para poder leer los datos en R. En esta oportunidad vamos a usar los datos que provee la gente de **Properati**, disponible mediante consultas al servicio de **Big Query** de Google. Vamos a leer un archivo **.RData** que tiene 94.257 anuncios de inmuebles para la provincia de Córdoba entre el 2015 y 2020.

Los archivos .RData son objetos de R que ya han sido cargados y exportados desde R como este formato específico. Estos archivos pueden ser útiles cuando queremos compartir algo con gente que sabemos que va a programar en R, ya que los pueden cargar directamente con **load()** y pesan realmente poco. Sin embargo, es importante aclarar que estos .RData "rompen" con la reproducibilidad: no podemos replicar exactamente cómo se llegó a esos datos, ni si hubo algún error en el preprocesamiento. En este caso, lo guardé de esta manera porque es una forma simple de compartir estos datos con ustedes, pero en la [página de Properati](https://www.properati.com.ar/data/) van a encontrar incluso ejemplos sobre consultas en **Big Query** para consultar información en la que ustedes estén interesados y luego la pueden exportar como CSV o json, archivos que pueden leer sin mayores problemas a R.

Carguemos los datos de la Provincia de Córdoba:

```{r}
load("data/datosCordoba.RData")
```

## Paso 2: análisis exploratorio y corrección de errores

Una vez que tenemos nuestro dataset cargado - deberían tener un objeto que se llama **datosCordoba** en la pestaña de "Environment" arriba a la derecha - lo que siempre debemos preguntarnos es qué es lo que realmente tenemos y detectar algunos patrones, que puede servirnos también para eliminar algunos outliers o errores en los datos. Carguemos nuestro aliado para - entre otras cosas - transformaciones de datos: **tidyverse**

```{r}
library(tidyverse)
glimpse(datosCordoba)
```

Nuestro dataset tiene 29 columnas. Quizás piensen que es una buena idea usar **View()** para tener una primera aproximación a esto, puede que no sea la mejor opción cuando trabajamos con muchos datos. Si quieren ir por ese camino, les recomiendo que usen **View(head(100))** para ver las primeras 100 filas, pudiendo cambiar este valor para ajustarlo a sus necesidades.

Otra opción consiste en apoyarnos en la descripción que vemos en enviroment. Podemos tener alguna idea de cuáles son las variables para las cuales querríamos ver los valores que toma, particularmente las variables categóricas. Las variables numéricas merecen otro tratamiento, ya que por su naturaleza pueden tomar distintos valores y es muy importante ver su distribución.

Una opción con las variables categóricas es usar la función **map()**. Ya la hemos usado en capítulos anteriores: lo que hace es repetir una tarea que nosotros le decimos para cada uno de objetos que queremos. Si lo usamos con data.frame(), por default lo que hará **map()** es hacer algo para cada una de las columnas del data.frame. Entonces podemos seleccionar primero las columnas que queremos investigar, y luego pedirle que nos devuelva una tabla, que incluya la cantidad de casos falantes:

```{r}
datosCordoba %>%
  select(type,type_i18n,country,place.l2,property.operation,property.operation_i18n,property.type,property.operation_i18n, property.currency, property.price_period) %>% 
  map(function(x) table(x,useNA = "always")) 
```

Esto nos da una buena idea - y bastante visual - de qué datos son los que tenemos. Para este ejercicio vamos a hacer un modelo predicitivo sobre el valor de venta de los inmuebles para la Ciudad de Córdoba. Además vamos a pedir que los prediga en dólares, por lo que vamos a quedarnos con aquellos que están anunciados en dólares, aunque haya una minoría que se propiedades para la vente que se anuncian en pesos. En rigor, todavía no sabemos exactamente cuál es Córdoba capital. Por el momento, queremos quedarnos con anuncios para venta y que sea un departamento o casa y que estén denominados en USD

```{r}
datosCordoba <- datosCordoba %>% 
                filter(property.currency == "USD" & property.operation=="Venta" & property.type %in% c("Casa","Departamento"))
```

Ahora ya tenemos 33.148 observaciones pertenecientes a la provincia de Córdoba, que fueron publicados en dólares y que tenían como destino de operación una venta.

Otra de la manipulación de datos relevante es el formato de las columnas (vectores). Cada uno de los vectores de nuestro data frame tiene un tipo de datos en particular. Algo que suele ser importante es estar seguros de que los datos que son numéricos están representados de tal manera, caso contrario no podremos hacer ninguna operación matemática, igual que, por ejemplo, lo que sucede en Excel cuando no tienen correctamente definido el tipo de datos.

Para esto podemos aplicar el mismo concepto que usamos anteriorente, pero en este caso primero vamos a preguntar que tipo de vector es a aquellos que consideramos que deberían ser numéricos:

```{r}
datosCordoba %>%
  select(property.rooms,
         property.surface_covered,
         property.price,
         property.surface_total,
         property.bathrooms,
         property.bedrooms) %>% 
  map(function(x) class(x))
```

Las seis columnas que deberían ser numéricas, en realidad son de tipo caracter ¿Como podemos hacer para convertir todas a tipo numérico? Usemos **mutate_at()** para evitar tener que hacer un mutate para cada una de ellas. Lo que tenemos que pasarse es un vector con el nombre de las columnas a transformar, seguido por una función a realizar en cada una de ellas. En nuestro caso en particular, queremos que todas sean tipo numérico, así que usamos la función **as.numeric()**. Tengan en cuenta que nos pide el nombre de la función, no ejecutar la función, así que no hace falta pasarle los paréntesis: **mutate_at()** lo hará automáticamente de manera interna.

```{r}
# Primero convertimos nuestro data.frame en tibble(), solo para aprovechar que se ve mejor en la consola
datosCordoba <- datosCordoba %>% as_tibble()
datosCordoba <- datosCordoba %>% 
  mutate_at(.vars = c("property.rooms",
                      "property.surface_covered",
                      "property.price",
                      "property.surface_total",
                      "property.bedrooms",
                      "property.bathrooms"),as.numeric)
```

Podemos chequear nuevamente la clase de las columnas de la misma manera que lo hicimos anteriormente, deberían ser numéricas:

```{r}
datosCordoba %>%
    select(property.rooms,
         property.surface_covered,
         property.price,
         property.surface_total,
         property.bathrooms,
         property.bedrooms) %>% 
  map(function(x) class(x))
```

Perfecto, ahora nos quedan dos tareas adicionales: la primera, es identificar el año de publicación del anuncio y la segunda es seleccionar correctamente cuáles son los inmuebles geolocaliazdos en la Ciudad de Córdoba. La primera parte es muy simple con la ayuda de la función **substr()**. Lo que hace es recortar el texto según las posiciones que nosotros le digamos. En este caso, queremos quedarnos con los primeros 4 caracteres de la columna **created_on**, que es la que tiene la fecha.

```{r}
datosCordoba <- datosCordoba %>% 
                mutate(year=substr(created_on,start = 1,stop = 4))
```

Ahora para elegir aquellos inmuebles que están en Córdoba Capital tenemos dos alternativas. La primera es filtrar los casos que tienen el valor *Córdoba* en**place.l3**, pero no estamos 100% de que ese sea el caso. Aprovechemos de que una gran cantidad de anuncios están geolocalizados y usemos esa información para identificarlos correctamente. El primer paso consiste en eliminar aquellos casos para los cuales no hay información sobre las coordenadas:

```{r}
datosCordoba <- datosCordoba %>% 
                filter(!is.na(place.lat) & !is.na(place.lon))
```

Luego,podemos convertir este objeto no espacial a uno espacial, usando **st_as_sf()**. No se olviden que antes debemos cargar el paquete sf

```{r}
library(sf)
cordobaSF <- st_as_sf(datosCordoba,coords = c("place.lon","place.lat"),crs=4326)
```

Con leaflet veamos si todo va bien, es decir si estos puntos parecen estar en la provincia de Córdoba

```{r}
library(leaflet)
leaflet(cordobaSF) %>% 
  addTiles() %>% 
  addCircles()
```

Parece que hay algunos problemas con algunos puntos localizados fuera de la Provincia de Córdoba. No se preocupen, esto suele ser algo normal. Lo que vamos a hacer es buscar el polígono de la Ciudad de Córdoba y buscar la intersección con los anuncios y quedarnos solo con los que estén dentro de ese polígono. Para esto vamos a usar al paquete **osmdata** y la función **getbb()**

```{r}
library(osmdata)
polyCordoba <- getbb(place_name = "Córdoba Capital, Argentina",format_out = "sf_polygon")
```

Veamos qué es lo que levantó:

```{r}
leaflet(polyCordoba) %>% 
  addTiles() %>% 
  addPolygons()
```

Este límite que nos muestra es el límite administrativo de la Ciudad de Córdoba, vamos a trabajar con él. Para encontrar cuáles puntos tienen una intersección vamos a proyectar a nuestros datos al EPSG 5343, una proyección oficial de Argentina (pueden usar **st_crs()** para encontrar más información sobre esta proyección)

```{r}
polyCordoba <- st_transform(polyCordoba,5343)
cordobaSF <- st_transform(cordobaSF,5343)
```

Y ahora ya podemos crear una columna en nuetro dataset espacial en base a si hay o no intersección. Tengan en cuenta que **st_intersects()** devuelve por default una matriz **sparse**, por eso ponemos **sparse=FALSE** para que nos devuelva una matriz lógica. Sin embargo, esta función esta preparada para que busquemos intersecciones entre más de un polígono, por lo que devuelve es una matriz de una columna, algo muy incómodo para nuestro dataset. Por eso le pedimos que lo convierta a un vector lógico con **as.logical()**, mucho más fácil para trabajar

```{r}
cordobaSF <-  cordobaSF %>% 
              mutate(cordobaCapital=as.logical(st_intersects(cordobaSF,polyCordoba,sparse=FALSE)))
```

Inspeccionemos visualmente que todo ande bien usando leaflet. Recuerden que para usar leaflet debemos tener proyectados nuestros datos en el EPSG 4326, lo vamos a hacer dentro de la función de leaflet, en lugar de modificar nuestros objetos. También vamos a poner dos colores, green y red, en una nueva variable para mostrar correctamente cuál fue el resultado de la intersección.

```{r}
cordobaSF <-  cordobaSF %>% 
              mutate(colorCordoba = ifelse(cordobaCapital==TRUE,"green","red"))
leaflet(cordobaSF %>% st_transform(4326)) %>% 
  addTiles() %>% 
  addCircles(color = ~colorCordoba)
```

Todo parece estar en orden, ahora vamos a hacer una última limpieza de nuestros datos. Para empezar, vamos a quedarnos con aquellos puntos que están dentro de la Ciudad de Córdoba y las que tengan un valor de propiedad total mayor a cero. También vamos a quedarnos con los casos en los cuales la superficie total y la cubierta es mayor a cero, ya que se trata de algún error en los datos

```{r}
cordobaSF <- cordobaSF %>%
             filter(cordobaCapital==TRUE & property.price>0 & property.surface_covered>0 & property.surface_total> 0)
```

Ahora vayamos a un tema muy importante: los **datos faltantes**. Los datos faltantes son siempre un tema que requiere una particular atención. Algunos modelos de aprendizaje automático/estadístico no tienen ningún problema para trabajar con datos faltantes en las variables explicativas, pero otros sí. Más allá de esto, precisamos saber cuáles variables tienen datos faltantes, particularmente aquellas variables que son cuantitativas.

Podemos aprovechar esta oportunidad para aprander algo nuevo. Si **map()** nos devuelve una lista, **map_dfr()** nos devuelve un data.frame, lo cual es muy bueno para poder aplicar otras funciones como **pivot_longer()** o **arrange()**. De hecho, lo que hacemos en este ejemplo es justamente esto: vamos columna por columna calculando la cantidad de casos que tienen casos faltantes, después pasamos todo a formato largo y ordenamos de manera descendiente por "n_faltantes", una columna que nos dice la cantidad de faltantes para cada una de las variables.

```{r}
cordobaSF %>% 
  map_dfr(function(x) sum(is.na(x))) %>%
  pivot_longer(cols = 1:ncol(.),values_to="n_faltantes") %>%
  arrange(desc(n_faltantes))
```

Tenemos faltantes en cuatro variables, siendo las más importantes la cantidad de dormitorios, la cantidad de ambientes y de baños. También debería aparecerles **price_period()** entre las variables con datos faltantes, pero no se preocupen, esa variable vamos a descartarla y no vamos a usarla para predecir el precio de los inmuebles

Una opción con los datos faltantes es elegir modelos de aprendizaje automático que sepan trabajar con ellos, como por ejemplo el árbol implementado por el paquete **rpart** en R o quizás **xgboost**, otro modelo derivado de árboles. También pueden imputarse mediante distintos procedimientos, y de hecho random forest, el modelo que vamos a usar finalmente, brinda una función para imputar estos datos. Dejemos esto un poco es suspenso y continuemos por el paso 3. Ya vamos a levantarlo más adelante.

## Paso 3: crear nuevas variables (o *feature engineering*)

Una parte relevante para lograr entrenar modelos que predigan con alta precisión es generar nuevas variables en base a las que ya existen en nuestro dataset. En particular, el conjunto de datos con el que trabajamos nos deja crear distintos tipos de variables. En primer lugar, veamos el tema de los datos faltantes en las variables de ambientes, dormitorios y baños.

Hasta ahora no nos fijamos en dos variables potencialmente muy útiles: **propety.title** y **property.description**. Se trata de dos variables que almacenan tanto el título como la descripción de los anuncios de los inmuebles. Veamos el título y la descripción del primer anuncio:

```{r}
cordobaSF %>% slice(1) %>% pull(property.description)
```

```{r}
cordobaSF %>% slice(1) %>% pull(property.title)
```

Si sabemos cómo encontrar patrones en estos textos quizás podemos recuperar algo de información. Agreguemos información sobre la existencia - o no - de un gimnasio, cochera o pileta. Para esto, vamos a usar **str_detect()**, una función que busca un patrón dentro de un texto y nos responde TRUE si aparece y FALSE si no aparece. Presten atención a la función **regex()** que está dentro del argumento *pattern*. Se trata de una función que nos ayuda a crear **REG**ular **EX**pressions de una manera simple. En la primera parte escribimos específicamente lo que queremos buscar, y con el parámetro *ignore_case* le específicamos que ignore si es mayúscula o minúscula.

```{r}
cordobaSF <- cordobaSF %>%
                mutate(gimnasio = ifelse(str_detect(pattern = regex("gym|gimn", ignore_case = TRUE),
                                                    string = property.description) |
                                         str_detect(pattern = regex("gym|gimn", ignore_case = TRUE),
                                                    string = property.title), 1, 0),
                       cochera = ifelse(str_detect(pattern = regex("coch|garage", ignore_case = TRUE),
                                                    string = property.description) |
                                         str_detect(pattern = regex("coch|garage", ignore_case = TRUE),
                                                    string = property.title), 1, 0),
                       pileta =ifelse(str_detect(pattern = regex("pileta|piscina", ignore_case = TRUE),
                                                    string = property.description) |
                                         str_detect(pattern = regex("pileta|piscina", ignore_case = TRUE),
                                                    string = property.title), 1, 0))
```


En segundo lugar, vamos a intentar recuperar la información sobre la cantidad de dormitorios con la función **str_extract()**. La regex que vamos a usar es un poco más compleja: **(\\d)(?= dorm)**. Lo que está en el primer paréntesis (\\d) quiere decir "dígitos" y lo que está entre los otros paréntesis (?= dorm) quiere decir "seguido por dorm". Por ejemplo, si dentro de la descripción se encontrara " 4 dormitorios", **str_extract()** nos devolvería "4". Pero antes de hacer esto, vamos a reemplazar los valores uno, dos y tres por los números correspondientes, para que pueda encontrarlo. Esto lo hacemos con **str_replace_all()**

```{r}
cordobaSF <- cordobaSF %>%
             mutate(property.description=str_replace_all(string = property.description,
                                                         pattern = regex("un|uno",ignore_case = TRUE),
                                                         "1"),
                    property.description=str_replace_all(string = property.description,
                                              pattern = regex("dos",ignore_case = TRUE),
                                              "2"),
                    property.description=str_replace_all(string = property.description,
                                                         pattern = regex("tres",ignore_case = TRUE),
                                                         "1")) %>% 
  mutate(ambientes=str_extract(pattern=regex("(\\d)(?= dorm)",ignore_case = TRUE),string = property.description))
```

Para conocer más sobre cómo trabajar con texto en R pueden revisar la [cheat sheet de stringr](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf).

Ya nos quedan los últimos pasos antes de poder entrenar nuestro modelo. Vamos a quedarnos con las variables que vamos a usar:

```{r}
cordobaSF <- cordobaSF %>%
             select(place.l4,ambientes,property.surface_covered,property.price,property.surface_total,year,gimnasio,cochera,pileta)
```

Por otro lado, vamos a agregar la información sobre la latitud y longitud de la ubicación de los inmuebles. Si es importante, nuestro modelo debería usarla para hacer mejores predicciones. Tendremos más para decir sobre esto más adelante. Para recuperar las coordenadas podemos usar **st_coordinates**

```{r}
anunciosCoords <- st_coordinates(cordobaSF) %>% as.data.frame()
```

Esta es una forma de incoporar el factor espacial en nuestros modelos predictivos. Otra forma de agregar información espacial es calcular el valor de un conjunto de vecinos e imputar su precio. Esta es también una idea importante para el análisis estadístico de los datos espaciales, algo que se trata más adelante en este libro. Para crear una variable que capture el valor promedio de los vecinos, primero tenemos que definir a los vecinos. En este caso, vamos a identificar como vecinos a todos aquellos puntos que estén a menos de 500 metros de cada uno de los anuncios, usando la función **st_is_within_distance()**, que nos devuelve una lista con los puntos de que cumplen esa condición, para cada uno de nuestros anuncios.

```{r}
listaVecinos <- st_is_within_distance(cordobaSF,cordobaSF,dist = 500)
# Vemos el primer elemento
listaVecinos[[1]]
```

Esta lista, aunque aparezca como un objeto **sgbp** podemos trabajarla como si fuera una lista normal con la función **map()**. Fijénse que el primer elemento tiene un vector númerico, que no es otra cosa que el número de fila, del data.frame **cordobaSF**, que son vecinos de la fila 1. Con un pequeño problema: **st_is_within_distance()** incluye al mismo punto como vecino, ya que efectivamente está a menos de 500 metros de sí mismo. Sería incorrecto incluirlo para calcular el precio por metro cuadrado de los vecinos, ya que está justamente relacionado con el precio que queremos calcular.

```{r}
# Creamos datosCordoba, un dataset que ya no es más espacial
datosCordoba <- cordobaSF %>% st_set_geometry(NULL)
# Agregamos una columna que tenga el precio metro cuadrado
datosCordoba <- datosCordoba %>% 
                mutate(precioM2=property.price/property.surface_covered)
# Recorremos cada uno de los valores y calculamos el valor mediano por metro cuadrado de los vecinos
valorVecinos <- map(1:length(listaVecinos),function(x) {
  # Esta linea elimina como vecino a la misma fila para la cual estamos calculando el precio promedio
  vecinos <- listaVecinos[[x]][!listaVecinos[[x]] %in% x]
  datosCordoba %>% slice(vecinos) %>% summarise(promedio=quantile(precioM2, 0.5))
}) %>% bind_rows()

# Ya podemos unir estos datos a los anteriores
datosCordoba <- datosCordoba %>% 
                mutate(precioM2Vecino = valorVecinos %>% pull(promedio))

```

También podemos agregar las coordenadas que guardamos anteriormente en **anunciosCoords**

```{r}
datosCordoba <- cbind(datosCordoba,anunciosCoords) 
```

Finalmente, vamos a necesitar que las variables que tenemos como character sean factores para que nuestro modelo más adelante pueda entrenarse sin problemas. Usamos la misma lógica que hicimos antes con los datos numéricos. Además, vamos a elimnar los casos en los cuales tenemos NA en la variable **ambientes** y en el valor del precio cuadrado de los vecinos

```{r}
datosCordoba <- datosCordoba %>%
  filter(!is.na(property.surface_covered)) %>% 
  mutate_at(.vars = c("place.l4","year"),as.factor) 
datosCordoba <- datosCordoba %>% 
                filter(!is.na(ambientes) & !is.na(precioM2Vecino))
```


## Paso 4: criterio de selección y optimización de los parámetros

Todos los pasos anteriores sirvieron para tener en nuestros datos un conjunto de variables que pensamos que pueden ser útiles para predecir el precio de venta de los inmuebles. En el capítulo introductorio al aprendizaje automático de [Ciencia de Datos Para Curiosos](https://martinmontane.github.io/CienciaDeDatosBook/) vimos cómo debemos elegir un modelo y, luego, optimizar los **parámetros**.

Vamos a usar un modelo de árboles muy conocido: **random forests**. Se basan en la idea de **bagging**, en la cual se entrenan muchos árboles que aprenden "demasiado", pero que son muy especializados en una sección de nuestros datos. Luego, para predecir un caso nuevo se promedian las predicciones de todos los árboles y ese es el valor final. Podríamos hacer una analogía y decir que en este modelo necesitamos tener un conjunto elevado de personas con un conocimeinto muy específico a las cuales les presentamos un caso que tienen que clasificar. Todas las personas harán en base al conocimiento que tienen, nosotros promediamos la opinión y ese será el resultado final.

Recuerden que lo que siempre nos importa en los modelos de aprendizaje automático es **predecir a casos nuevos sobre los que no hayamos entrenado nuestro modelo**. El objetivo al final del día es aproximar la relación que existe entre el conjunto de variables explicativas y aquella que queremos predecir para todos los casos posibles, y no solo para la muestra que tenemos. En el siguiente capítulo trabajaremos con **tidymodels()** para este último paso, que tiene distintos componentes y requiere una explicación más pormenroizada.

Cerremos este capítulo midiendo de alguna manera el impacto de todo nuestro trabajo en la creación de nuevas variables. Estimemos un modelo de regresión lineal usando solo los datos originales, y luego agreguemos los nuestros y comparemos el cambio en una medida de ajuste, el R2 y también el RMSE de estos modelos usando el paquete **yardstick**

```{r}
library(yardstick)
# Regresión con los datos iniciales
regresionOriginal <- lm(formula = property.price ~ place.l4 + property.surface_covered + property.surface_total + year,
   data = datosCordoba)
# rsq_vec calcula el R2 con un vector de predicciones y otro de valores reales
rsq_vec(truth = predict(regresionOriginal),
         estimate = datosCordoba %>% pull(property.price))
# rmse_vec calcula el RMSE con un vector de predicciones y otro de valores reales
rmse_vec(truth = predict(regresionOriginal),
         estimate = datosCordoba %>% pull(property.price))
# Regresión con los datos nuevos
regresionVariablesNuevas <- lm(formula = property.price ~ place.l4 + ambientes + property.surface_covered + property.surface_total + year + gimnasio + cochera + pileta + precioM2Vecino + X + Y,
   data = datosCordoba)
# R2
rsq_vec(truth = predict(regresionVariablesNuevas),
         estimate = datosCordoba %>% pull(property.price))
#RMSE
rmse_vec(truth = predict(regresionVariablesNuevas),
         estimate = datosCordoba %>% pull(property.price))
```

Como podemos ver en este simple modelo, solo agregar nuevas variables aumentó el R2 de nuestro modelo del 5,3% al 15.6% y redujo el RMSE un 5.6%. Veremos más sobre qué tan relevantes son estas variables en la segunda parte de esta sección. Quizás quieran ver, también, cual es la distribución de los errores absolutos de predicción de ambos modelos

```{r}
# Modelo original
errorAbsolutoOriginal <- round(abs(predict(regresionOriginal)-datosCordoba %>% pull(property.price) ),0)
quantile(errorAbsolutoOriginal)
```

```{r}
# Modelo con variables agregadas
errorAbsolutoOriginal <- round(abs(predict(regresionVariablesNuevas)-datosCordoba %>% pull(property.price) ),0)
quantile(errorAbsolutoOriginal)
```


<!--chapter:end:MachineLearningEspacial.Rmd-->

# El ritual del aprendizaje automático (Parte 2)

En el capítulo anterior dejamos pendiente el último tramo en un típico proyecto para predecir o clasificar usando modelos de aprendizaje automático: la **optimización de parámetros y selección de modelo**. Esta parte suele consistir en establecer uno o más **criterios de selección** y probar distintas combinaciones de parámetros de nuestros modelos para ver cuál es el que mejor predice **nuevos datos**. Adicionalmente, vamos a investigar un poco cómo podemos obtener información sobre las variables más importante para los modelos.

Para este capítulo vamos a usar las funciones de los paquetes que componen **tidymodels**, que buscan simplificar muchas etapas de este último paso de optimización de parámetros y selección de modelo, aunque también brinda funciones para incoporporar el resto de las etapas que estuvimos haciendo en el capítulo anterior. Ya quedará todo más claro hacia el final de este capítulo.

La figura que se muestra debajo de este párrafo muestra, en naranja, las tareas que haremos en este capítulo y que completan una parte muy importante de un proyecto de ciencia de datos en el cual se usan modelos de aprendizaje automático para tareas de predicción o clasificación. Noten que lo que tenemos que hacer es optimizar los parámetros y, en base a alguna medida de *performance*, seleccionar el mejor modelo e intentar explorar la importancia de las variables y resumir su capacidad predictiva ¿Ven esas flechas de ida y vuelta entre la creación de nuevas variables y la optimización de los parámetros? Muchas veces creamos nuevas variables o modificamos las existentes para mejorar alguno de los resultados que nos arroja la optimización de los parámetros. En este caso no vamos a hacerlo, pero vale la pena aclararlo.

```{r echo=FALSE}
knitr::include_graphics("data/DiagramaMachineLearningParte2.png")
```

Finalmente, para lo que sigue de este capítulo vamos a tener que tener instalado el paquete **tidymodels** y cargados los datos del capítulo anterior:

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
# Carga de datos
datosCordoba <- read_delim("https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/datosMachineLearning2.csv", delim=";") %>% select(-precioM2)
```


## Separando dataset de entrenamiento y de testing

En **tidymodels** podemos separar a los datos en un conjunto de entrenamiento y de testing de una manera muy simple usando la función **initial_split()**. Recuerden que esto es necesario porque nos interesa conocer la capacidad predictiva de nuestros modelos cuando le presentamos datos con los que NO se entrenaron, no sobre aquellos con los que se entrenó. Esto es así porque si seleccionamos un modelo en base a qué tan bien ajusta a los datos con los que entrena podemos caer en el territorio del **overfitting**, es decir, cuando nuestro modelo se aprende "de memoria" los datos y extrae una mala aproximación de la relación que existe entre las variables, haciendo muy malas predicciones **out-of-sample**, es decir en nuevos datos.

```{r}
# Esta primer línea es solo para que separemos los mismos casos en training y testing
set.seed(10)
# Guardamos un 10% de los datos para después de haber entrenado el modelo
cordobaSplit <- initial_split(datosCordoba,prop = 0.7)
```

Con el parámetro **prop** indicamos cuál es la proporción de los datos que queremos que nos quede en el dataset de entrenamiento. En este caso, elegimos que el 70% de todos nuestros datos sean utilizados para entrenar a nuestro modelo de aprendizaje automático. Pueden verlo usando el método de print() de cordobaSplit:

```{r}
cordobaSplit
```

Aunque los nombra un poco distinto, lo que nos dice es que hay 10.653 observaciones para entrenar a nuestro modelo y 4.565 para evaluar su capacidad predictiva sobre nuevos datos. cordobaSplit es un objeto de rsplit, para efectivamente tener a los data.frames podemos usar las funciones training() y testing() que nos generan estos datasets a partir del split. Presten atención al largo de cada uno de estos objetos.

```{r}
cordobaTrain <- training(cordobaSplit)
cordobaTesting <- testing(cordobaSplit)
```


## Nuestra receta y su preparación

En el framework de tidymodels, existen **recetas** (recipes) que escribimos para nuestros modelos. Pueden incluir transformaciones a los datos y formulas de predicción, entre otras cosas. En este caso, ya hicimos toda la transformación en el capítulo anterior, así que lo que vamos a marcar simplemente la fórmula de lo que queremos predecir: el precio de las propiedades. Como siempre, pueden usar el menú de ayuda de R , **?recipe** en este caso, para conocer específicamente lo que podemos incluir en esta función.

```{r}
cordobaRecipe <- recipe(formula = property.price ~.,
                        data=cordobaTrain)
cordobaRecipe
```

Como podemos ver, creamos una "Data Recipe", y lo que nos indica es que existe 1 variable que queremos predecir y 10 predictores. Ahora, debemos indicar que queremos "preparar" esta receta, es decir evaluar si esta receta puede ser aplicada a nuestros datos y dejarla lista para los siguientes pasos.

```{r}
treePrep <- prep(cordobaRecipe)
treePrep
```

## Selección de modelo y espacio de búsqueda de los parámetros

Ahora lo que nos queda para definir es el modelo a estimar, y esto requiere el uso de algunas funciones debido a que existen diversas decisiones que tenemos que hacer. La primera de ellas es básicamente elegir el modelo con el queremos trabajar. Ya hemos aclarado anteriormente que vamos a usar **random forest**, un modelo basado en árboles y **bagging**, es decir la creación de muchos árboles sumamente especializados que luego hacen una predicción conjunta. Podemos elegir este tipo de modelos con **rand_forest()**.

Dentro de esta función podemos especificar los valores de los parámetros y, más importante, aclarar cuales son los que queremos "tunear", es decir, optimizar. Esto lo declaramos con la función **tune()**. En el caso de los random forests, dos de los parámetros que suelen optimizarse son **mtry** y **min_n**. El primero determina la cantidad de variables que, al azar, pueden usarse en cada uno de los nodos de los árboles. Recuerden que random forest no elige a todos los predictores en cada nodo, sino que solo candidatea al azar a una cantidad fija de columnas, y esto es lo que marcamos con **mtry**. Por otro lado, **min_n** establece una cantidad mínima de valores que tiene que haber en un nodo como para poder seguir profundizando el árbol.

Por otro lado, con la función **set_mode()** debemos identificar si queremos hacer una regressión (para predecir valores númericos) o una clasificación (para predecir valores categóricos). Finalmente, en **set_engine** indicamos el paquete que tiene la implementación de random forest. Existen muchas librerías en R para hacerlo, vamos a usar la que brinda el paquete *ranger*

```{r}
tuneSpec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")
tuneSpec
```

Ya tenemos la especificación de nuestro modelo. Ahora nos queda establecer un **workflow**, que no es otra cosa que un conjunto de instrucciones secuenciales que hay que hacer para entrenar los modelos. En este caso, le decimos que tome la receta ya preparada que declaramos anteriormente, y luego entrene el modelo que especificamos recién.

```{r}
tuneWf <-workflow() %>% 
  add_recipe(treePrep) %>% 
  add_model(tuneSpec)
tuneWf
```

Ahora lo que vamos a hacer es crear conjuntos de entrenamiento para realizar **cross validation** o validación cruzada. Este concepto que puede sonar complejo es realmente simple. Lo que hacemos es cortar nuestros datos al azar grupos de igual tamaño. Para cada combinación de parámetros, entrenamos el modelo en todos los grupos menos uno, y predecimos sobre el que no entrenamos. Promediamos el valor de todos los errores y ese es el valor por el cual vamos a juzgar la *performance* de nuestro modelo. El gráfico que sigue es muy ilustrativo sobre cómo funciona

```{r echo=FALSE}
knitr::include_graphics("data/kfolds.png")
```

Podemos crear estos grupos, en base a nuestro conjunto de entrenamiento, con la función **vfold_cv()**

```{r}
# Hacemos esto para que tengamos los mismos grupos
set.seed(234)
# Creamos los grupos
trees_folds <- vfold_cv(cordobaTrain, v = 5)
```

Ya casi estamos listos para entrenar los modelos. Lo que nos falta es fundamental: específicar cual es el espacio de búsqueda de nuestros parámetros. Hay muchas formas de hacer esto en **tidymodels**, pero lo que vamos a usar en esta clase es **grid_regular()**. Le pasamos un valor mínimo y máximo de valores posibles para cada uno de los parámetros que queremos optimizar, y luego una cantidad de valores únicos para cada uno de los parámetros. Obviamente, cómo mtry va entre 2 y 8, vamos a tener solo 7 valores, pero en el caso de **min_n** tomára 10 valores espaciados entre 5 y 200. Pueden ver por ustedes mismos la grilla, ya que es simplemente un tibble.

```{r}
rf_grid <- grid_regular(
  mtry(range = c(2,8)),
  min_n(range = c(5,200)),
       levels=10
)
```

## Entrenando los modelos en nuestro espacio de búsqueda

Entrenar 70 modelos con cross validation de 5 grupos implica entrenar 5x70 = 350 modelos... esto puede llevar un buen rato. Usemos al paquete **doParallel** para aprovechar el procesamiento en paralelo en R. Pueden hacerlo simplemente en una línea:

```{r}
doParallel::registerDoParallel()
```

Ahora ya podemos entrenar todos los parámetros de nuestra grilla con la función **tune_grid()**. Lo único que hacemos es pasarle objetos que ya creamos anteriormente. Tengan paciencia cuando corran este código, que puede tardar un rato. 

```{r eval=FALSE}
set.seed(345)
resultadoBusqueda <- tune_grid(
  tuneWf,
  resamples=trees_folds,
  grid=rf_grid
)
```

Como siempre, dejo a disposición el resultado de este proceso para que no tengan que esperar a correrlo para entender qué es lo que hicimos.

```{r}
load("https://github.com/martinmontane/AnalisisEspacialEnR/raw/master/data/resultadoBusqueda.RData")
resultadoBusqueda
```

**resultadoBusqueda** tiene para cada uno de nuestros grupos de cross validation el resultado de RMSE y R2, dos indicadores que pueden ser utilizados para medir la capacidad predicitiva de nuetro modelo. Nosotros necesitamos el promedio de estos grupos, y para eso usaremos **collect_metrics()**

```{r}
metricasPerformance <- collect_metrics(resultadoBusqueda)
glimpse(metricasPerformance)
```

Se trata de un dataset con 140 filas, dado que entrenamos 70 modelos y tenemos 2 indicadores de performance para cada uno de ellos. Refrescemos un poco la memoría sobre qué es lo que hace cada uno de ellos.

Por un lado, la raíz del error cuadrático medio (RMSE, por su sigla en inglés) tiene la siguiente fórmula

$$ \color{#E7298A}{RMSE} = \sqrt{(1/\color{olive}{n})\sum_{i=1}^{\color{olive}{n}}(\color{purple}{y_i} - \color{orange}{\hat{f}(x_i)})^2}=$$

Donde $\color{olive}{n}$ es la cantidad de observaciones, $\color{purple}{y_i}$ es el valor observado para la observación i; y $\color{orange}{\hat{f}(x_i)}$ es el valor predicho dada la aproximación y los valores de las variables para la observación i. En otras palabras, el RMSE es cuanto nos confundimos en promedio al predecir los datos. 

Por otro lado, el R2 o "Rsquared" puede explicitarse de la siguiente manera:

$$ \color{#e41a1c}{R^2} = 1 - \frac{\color{#377eb8}{SS_{res}}} {\color{#4daf4a}{SS_{tot}}} $$,

donde $\color{#377eb8}{SS_{res}}$ es la suma al cuadrado de los residuos, es decir $\color{#377eb8}{SS_{res}} = \sum_{i}^{N}(y_i-\hat{y})^2$: cuanto de la variabilidad queda plara explicar luego de que hacemos la predicción, mientras que $\color{#4daf4a}{SS_{tot}} = \sum_{i}^{N}(y_i-\bar{y})^2$, es decir la diferencia entre el valor observado y el promedio.

Estas son dos de las métricas más utilizadas para seleccionar los modelos. Veamos cómo dan los resultados para la combinación de los parámetros que hemos usado:

```{r}
ggplot(metricasPerformance) +
  geom_point(aes(x=mtry,color=factor(min_n),y=mean)) +
  facet_wrap(~ .metric,scales = "free")
```

En ambos casos podemos ver que el RMSE mínimo y el R2 máximo se alcanzan con los parámetros **min_n = 5** y **mtry = 2**. Además podemos ver que cuanto más aumenta mtry, el modelo empeora, señalando una de las ventajas de random forest. Parece contrar un mínimo cercano al valor 2 o 3.

## Seleccionando el mejor modelo y analizando los resultados

Ya estamos en condiciones de elegir el mejor modelo y medir su capacidad predictiva en el conjunto de testing - o validación, en rigor - que separamos al principio de la clase. Para elegir el mejor modelo tenemos que usar **select_best()**, función a la cual solo tenemos que pasarle el resultado de la búsqueda de parámetros y una métrica para elegir al modelo:

```{r}
select_best(resultadoBusqueda, metric="rmse")
```

Nos dice algo que ya podíamos ver anteriormente, que el mejor modelo dentro de los que buscamos es el que tiene mtry 2 y min_n 5. Veamos si r2 elige al mismo modelo:

```{r}
select_best(resultadoBusqueda, metric="rsq")
```

Ambos criterios eligen exactamente al mismo modelo. Ahora estimamos ese modelo para esos parámetros, será nuestro modelo final con el que probaremos la capacidad predictiva sobre los datos nuevos. Para seleccionar al modelo ganador solo tenemos que usar **finalize_model()**

```{r}
rfWinner <- finalize_model(
  tuneSpec,
  select_best(resultadoBusqueda, metric="rmse")
)
```

Y ya podemos entrenar al modelo con esos parámetros sobre todos los datos de training

```{r}
set.seed(456)
modeloGanador <-rfWinner %>% 
  set_engine("ranger") %>% 
  fit(property.price ~., data=cordobaTrain)
```

Y predecir sobre nuevos datos

```{r}
# R2
rsq_vec(truth =  predict(modeloGanador,new_data = cordobaTesting) %>% pull(.pred),
         estimate = cordobaTesting %>% pull(property.price))
#RMSE
rmse_vec(truth = predict(modeloGanador,new_data = cordobaTesting) %>% pull(.pred),
         estimate = cordobaTesting %>% pull(property.price))
```

## Importancia de las variables

Cerremos este ejercicio teniendo alguna idea de la importancia que tienen cada una de las variables en nuestro modelo ganador. Usemos el paquete **vip** para esta tarea. Fijense que tenemos que elegir el método para elegir la importancia de las variables, algo que está fuera del objetivo de este libro, aunque merece una breve introducción. "Permutation" lo que hace es calcular la contribución promedio de una variable a la capacidad predictiva del modelo comparando el MSE (Mean squared error) obtenido cuando se "mezcla" la variable con respecto al valor cuando se usa como se observa en los datos. La idea es que si una variable no es muy importante, entonces la diferencia en la capacidad predictiva de la variable observada y de la "mezcla" debería ser baja y viceversa.

```{r}
library(vip)
set.seed(456)
modeloGanador <-rfWinner %>% 
  set_engine("ranger", importance="permutation") %>% 
  fit(property.price ~., data=cordobaTrain)
vip(modeloGanador,geom="point")
```

¿Qué observamos? Que la información espacial importa para este modelo. Más allá de que las primeras dos variables más importantes para explicar este modelo son **property.surface_covered** y **property.surface_total**, las variables Y, precioM2Vecino y X son la tercera, cuarta y quinta variable más importantes según esta medida de importancia.


<!--chapter:end:MachineLearningEspacial2.Rmd-->

